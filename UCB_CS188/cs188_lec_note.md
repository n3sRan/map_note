# Introduction to Artificial Intelligence

> CS188 25 Spring 教材/课堂笔记, 由 DS V3 翻译.

## 1. 搜索

### 1.1 智能体

在人工智能中, 核心问题是创建理性智能体 (rational agent)--这种实体具有目标或偏好, 并试图执行一系列行动以在这些目标下产生最佳/最优的预期结果. 理性智能体存在于特定环境中, 环境的具体形式取决于智能体的实例化. 智能体通过传感器与环境交互, 并通过执行器对环境施加行动. 举个简单的例子, 一个跳棋智能体的环境是虚拟的跳棋棋盘, 它在此与对手对弈, 移动棋子就是其行动. 环境和其中的智能体共同构成了一个世界.

**反射型智能体** (reflex agent) 不会考虑行动的后果, 而是仅根据当前世界状态选择行动. 这类智能体通常不如**规划型智能体** (planning agent) 表现优异, 后者会维护一个世界模型, 并利用该模型模拟执行各种行动. 随后, 智能体可以推测行动的假设性后果, 并选择最佳行动. 这种模拟的“智能”类似于人类在判断任何情境下的最佳行动时的思考方式--即前瞻性思考.

为了定义任务环境, 我们使用**PEAS描述** (Performance Measure, Environment, Actuators, Sensors):

- **性能指标** (Performance Measure): 描述智能体试图提升的效用.
- **环境** (Environment): 总结智能体行动的范围及其影响因素.
- **执行器与传感器** (Actuators and Sensors): 智能体作用于环境和从中获取信息的方法.

智能体的设计很大程度上取决于其所处环境的类型. 我们可以通过以下方式对环境进行分类:

- **部分可观测环境** (partially observable environments): 智能体无法获取完整的状态信息, 因此必须对世界状态进行内部估计. 与之相对的是**完全可观测环境** (fully observable environments), 智能体可以获取其状态的完整信息.
- **随机环境** (stochastic environments): 转移模型存在不确定性, 即在特定状态下执行某个行动可能产生多种不同概率的结果. 与之相对的是**确定性环境** (deterministic environments), 行动在特定状态下只会产生一个确定的结果.
- **多智能体环境** (multi-agent environments): 智能体与其他智能体共同行动. 因此, 智能体可能需要随机化其行动以避免被其他智能体“预测”.
- **静态环境** (static environments): 智能体行动时环境不会发生变化. 与之相对的是**动态环境** (dynamic environments), 环境会随着智能体的交互而改变.
- **已知物理规则的环境**: 转移模型 (即使是随机的) 对智能体已知, 智能体可以在规划路径时利用该模型. 若物理规则未知, 智能体需通过行动主动学习未知的动态变化.

### 1.2 状态空间与搜索问题

为了创建理性的规划型智能体, 我们需要一种数学方法来表示智能体所处的环境. 为此, 必须形式化地定义**搜索问题**: 给定智能体的当前状态 (其在环境中的配置), 如何以最佳方式达到满足其目标的新状态? 一个搜索问题包含以下要素:

- **状态空间** (state space): 世界中所有可能状态的集合.
- **每个状态下可用的行动集合**.
- **转移模型** (transition model): 在当前状态下执行特定行动后输出下一个状态.
- **行动成本** (action cost): 执行行动后从一个状态转移到另一个状态所消耗的成本.
- **初始状态** (start state): 智能体最初存在的状态.
- **目标测试** (goal test): 以状态为输入的函数, 判断该状态是否为目标状态.

从根本上说, 解决搜索问题的方法是: 首先考虑初始状态, 然后利用行动, 转移和成本方法探索状态空间, 迭代计算各状态的子状态, 直到到达目标状态. 此时, 我们就确定了从初始状态到目标状态的路径 (通常称为**计划**). 状态的考虑顺序由预定义的策略决定. 稍后我们将介绍不同类型的策略及其用途.

在继续讨论如何解决搜索问题之前, 必须区分**世界状态** (world state) 和**搜索状态** (search state). 世界状态包含给定状态的所有信息, 而搜索状态仅包含规划所需的世界信息 (主要是出于空间效率的考虑). 为了说明这些概念, 我们将引入本课程的经典示例--**吃豆人** (Pacman). 吃豆人游戏的规则很简单: 吃豆人必须在一个迷宫中导航, 吃掉所有 (小) 食物豆, 同时不被巡逻的幽灵吃掉. 如果吃豆人吃掉一个 (大) 能量豆, 他会在一定时间内免疫幽灵的攻击, 并获得吃掉幽灵以得分的能力.

![](<../0_Attachment/Pasted image 20250511174709.png>)

考虑一个简化版的游戏, 迷宫中只有吃豆人和食物豆. 我们可以提出两种不同的搜索问题: **路径规划** (pathing) 和**吃掉所有豆子** (eat-all-dots). 路径规划试图解决从位置 $(x_1, y_1)$ 到位置 $(x_2, y_2)$ 的最优路径问题, 而吃掉所有豆子则试图解决在最短时间内吃掉迷宫中所有食物豆的问题. 以下是两种问题的状态, 行动, 转移模型和目标测试:

**路径规划**

- 状态: $(x, y)$ 位置
- 行动: 北, 南, 东, 西
- 转移模型 (获取下一状态): 仅更新位置
- 目标测试: 是否 $(x,y) = \text{END}$? 

**吃掉所有豆子**

- 状态: $\{(x,y) \text{位置}, \text{豆子的布尔值}\}$
- 行动: 北, 南, 东, 西
- 转移模型 (获取下一状态): 更新位置和布尔值
- 目标测试: 所有豆子的布尔值是否为假?

注意, 路径规划的状态包含的信息少于吃掉所有豆子的状态, 因为后者必须维护一个布尔数组, 记录每个食物豆是否被吃掉. 世界状态可能包含更多信息, 例如吃豆人移动的总距离或所有访问过的位置, 以及当前的 $(x, y)$ 位置和豆子的布尔值.

#### 1.2.1 状态空间大小

在估计解决搜索问题的计算运行时间时, 一个重要的问题是状态空间的大小. 这通常通过**基本计数原理** (fundamental counting principle) 来完成, 该原理指出: 如果一个世界中有 $n$ 个可变对象, 每个对象可以分别取 $x_1, x_2, \ldots, x_n$ 个不同的值, 那么状态的总数为 $x_1 \cdot x_2 \cdot \ldots \cdot x_n$. 我们以吃豆人为例说明这一概念:

![](<../0_Attachment/Pasted image 20250511201419.png>)

假设可变对象及其对应的可能性数量如下:

- 吃豆人位置: 吃豆人可以在 120 个不同的 $(x, y)$ 位置, 且只有一个吃豆人.
- 吃豆人方向: 可以是北, 南, 东, 西, 共 4 种可能.
- 幽灵位置: 有两个幽灵, 每个幽灵可以在 12 个不同的 $(x, y)$ 位置.
- 食物豆配置: 有 30 个食物豆, 每个食物豆可以被吃掉或未被吃掉.

根据基本计数原理, 吃豆人有 120 个位置, 4 种方向, 幽灵的配置为 $12 \cdot 12$ (每个幽灵有 12 种位置), 食物豆的配置为 $2 \cdot 2 \cdot \ldots \cdot 2 = 2^{30}$ (每个食物豆有两种可能值--被吃掉或未被吃掉). 因此, 总状态空间大小为: $120 \cdot 4 \cdot 12^2 \cdot 2^{30}.$

#### 1.2.2 状态空间图与搜索树

现在我们已经建立了状态空间的概念以及完整定义状态空间所需的四个组成部分, 接下来可以开始解决搜索问题了. 最后一块拼图是**状态空间图** (state space graph) 和**搜索树** (search tree).

回想一下, 图由一组节点和一组连接节点对的边定义, 这些边可能还带有权重. 状态空间图的节点表示状态, 从一个状态到其子状态的有向边表示行动, 边的权重表示执行相应行动的成本. 通常, 状态空间图太大而无法存储在内存中 (即使是我们简单的吃豆人示例也有约 $10^{13}$ 个可能状态), 但它们在概念上对解决问题很有帮助. 还需注意, 在状态空间图中, 每个状态只表示一次--没有必要多次表示一个状态, 这一点在推理搜索问题时非常有用.

与状态空间图不同, 搜索树没有限制状态出现的次数. 这是因为搜索树虽然也是一种图, 其节点表示状态, 边表示状态之间的行动, 但每个节点不仅编码状态本身, 还编码从初始状态到该状态的完整路径 (或计划). 观察以下状态空间图和对应的搜索树:

![](<../0_Attachment/Pasted image 20250511201616.png>)

在状态空间图中, 高亮路径 $(S \rightarrow d \rightarrow e \rightarrow r \rightarrow f \rightarrow G)$ 在对应的搜索树中表现为从初始状态 $S$ 到目标状态 $G$ 的路径. 类似地, 从初始节点到任何其他节点的每条路径在搜索树中都表现为从根 $S$ 到某个后代节点的路径. 由于从一个状态到另一个状态通常存在多种方式, 状态往往会在搜索树中多次出现. 因此, 搜索树的大小通常大于或等于对应的状态空间图.

我们已经确定, 即使是简单问题的状态空间图也可能非常庞大, 因此问题来了: 如果这些结构太大而无法存储在内存中, 我们如何进行有用的计算? 答案在于如何计算当前状态的子状态--我们只存储当前正在处理的状态, 并根据需要使用相应的 `getNextState`, `getAction` 和 `getActionCost` 方法动态生成新状态. 通常, 搜索问题通过搜索树解决, 我们谨慎地存储少量节点进行观察, 迭代地用子节点替换节点, 直到到达目标状态. 有多种方法可以决定这种迭代替换的顺序, 接下来我们将介绍这些方法.

### 1.3 无信息搜索

从起始状态到目标状态寻找规划方案的标准流程是: 维护一个由搜索树衍生的部分规划方案的外部边界. 我们通过从边界中移除一个节点 (根据给定策略选择) 对应的部分规划, 并用其所有子节点替换该边界节点, 从而持续扩展边界. 移除并用子节点替换边界元素相当于丢弃一个长度为$n$的规划, 同时考虑所有由其衍生的长度为$(n+1)$的规划. 这一过程持续进行, 直到最终从边界中移除一个目标状态, 此时我们确认该目标状态对应的部分规划即为从起始状态到目标状态的路径.

实际实现中, 此类算法通常会在节点对象中编码父节点信息, 节点距离以及状态信息. 上述流程称为**树搜索**, 其伪代码如下:

```pseudocode
function TREE-SEARCH(problem, frontier) return a solution or failure
    frontier ← INSERT(MAKE-NODE(INITIAL-STATE[problem]), frontier)
    while not IS-EMPTY(frontier) do
        node ← POP(frontier)
        if problem.IS-GOAL(node.STATE) then return node
        for each child-node in EXPAND(problem, node) do
            add child-node to frontier
    return failure
```

伪代码中的`EXPAND`函数通过考虑所有可用动作, 返回从给定节点可到达的所有可能节点. 其伪代码如下:

```pseudocode
function EXPAND(problem, node) yields nodes
    s ← node.STATE
    for each action in problem.ACTIONS(s) do
        s' ← problem.RESULT(s, action)
        yield NODE(STATE=s', PARENT=node, ACTION=action)
```

当搜索树中目标状态的位置未知时, 我们只能从**无信息搜索**技术中选择树搜索策略. 下面将依次介绍三种策略: 深度优先搜索, 广度优先搜索和一致代价搜索. 针对每种策略, 我们将从以下维度分析其基本特性:

- **完备性**: 若存在解, 给定无限计算资源时, 该策略是否能保证找到解?
- **最优性**: 该策略是否能保证找到最低成本路径?
- **分支因子b**: 每次从边界取出节点并用其子节点替换时, 边界节点数量的增长为$O(b)$. 在搜索树的第k层, 存在$O(b^{k})$个节点.
- **最大深度m**
- **最浅解深度s**

#### 1.3.1 深度优先搜索 (DFS)

- **描述**: 总是选择从起始节点出发最深的边界节点进行扩展.
- **边界表示**: 需使用后进先出 (LIFO) 栈结构, 确保最新添加的对象具有最高优先级.
  ![](../0_Attachment/Pasted%20image%2020250511204140.png)
- **完备性**: 不完备. 若状态空间图中存在环路, 搜索树将无限深, DFS可能永远无法找到解.
- **最优性**: 非最优. DFS仅找到搜索树中"最左侧"的解, 不考虑路径成本.

- **时间复杂度**: 最坏情况下需探索整个搜索树, 为$O(b^{m})$.
- **空间复杂度**: 最坏情况下边界需维护每层$b$个节点, 共$m$层, 为$O(bm)$.

#### 1.3.2 广度优先搜索 (BFS) 

- **描述**: 总是选择从起始节点出发最浅的边界节点进行扩展.
- **边界表示**: 使用先进先出 (FIFO) 队列, 确保按插入顺序访问节点.
- **完备性**: 完备. 若解存在, 其最浅深度$s$有限, BFS终将搜索到该深度.
- **最优性**: 通常非最优 (除非所有边成本相同, 此时退化为一致代价搜索的特例).
  ![](../0_Attachment/Pasted%20image%2020250511204202.png)
- **时间复杂度**: 需搜索$1 + b + b^{2} + \ldots + b^{s}$个节点, 为$O(b^{s})$.
- **空间复杂度**: 边界在最坏情况下包含最浅解深度的所有节点, 为$O(b^{s})$.

#### 1.3.3 一致代价搜索 (UCS) 

- **描述**: 总是选择从起始节点出发路径成本最低的边界节点进行扩展.
- **边界表示**: 采用基于堆的优先队列, 优先级为从起始节点到该节点的路径成本 (后向成本).
  ![](../0_Attachment/Pasted%20image%2020250511204229.png)
- **完备性**: 完备. 若目标状态存在, 其最短路径长度必然有限.
- **最优性**: 在边成本非负时最优. 按路径成本递增顺序探索, 保证找到最低成本解 (本质是Dijkstra算法的变体). 负边成本会破坏最优性保证.
- **时间复杂度**: 设最优路径成本为$C^*$, 状态空间图中两节点间最小成本为$\varepsilon$, 则需探索深度$1$到$\frac{C^*}{\varepsilon}$的节点, 为$O(b^{\frac{C^*}{\varepsilon}})$.
- **空间复杂度**: 边界包含最优解深度的所有节点, 为$O(b^{\frac{C^*}{\varepsilon}})$.

需要强调的是, 上述三种无信息搜索策略本质相同, 仅扩展策略不同, 其共性已体现在前述树搜索伪代码中.

### 1.4 启发式搜索

一致代价搜索 (UCS) 的优点在于其完备性和最优性, 但由于它从起始状态向各个方向均匀扩展, 搜索速度可能较慢. 如果我们能获得关于搜索方向的某种指引, 就能显著提升性能并更快地"锁定"目标状态. 这正是启发式搜索的核心思想.

#### 1.4.1 启发函数

启发函数是估算到目标状态距离的驱动力--它们是接收状态输入并输出相应估计值的函数. 该函数的具体计算方式与待解决的搜索问题相关. 出于后续将介绍的$A^{*}$搜索的原因, 我们通常希望启发函数能给出到目标剩余距离的下界, 因此启发函数通常是对原始问题松弛化 (移除部分约束条件) 后的解. 以吃豆人游戏为例, 考虑之前描述的路径规划问题. 解决该问题的常用启发函数是曼哈顿距离, 对于两点 $(x_{1},y_{1})$ 和 $(x_{2},y_{2})$ 定义如下:

$Manhattan(x_{1},y_{1},x_{2},y_{2})=|x_{1}-x_{2}|+|y_{1}-y_{2}|$

![](../0_Attachment/Pasted%20image%2020250513101121.png)

上述可视化展示了曼哈顿距离所解决的松弛问题--假设吃豆人希望到达迷宫左下角, 该函数在忽略墙壁的情况下计算当前位置到目标位置的距离. 这个距离是松弛化问题中的精确目标距离, 对应原始问题中的目标距离估计值. 通过启发函数, 我们可以轻松实现智能体的决策逻辑, 使其在选择动作时"优先"扩展估计更接近目标的状态. 这种"优先"概念非常强大, 被以下两种实现启发函数的搜索算法所采用: 贪婪搜索和$A^{*}$算法.

#### 1.4.2 贪婪搜索

- **描述** - 贪婪搜索是一种总是选择**启发值最低**的边界节点进行扩展的探索策略, 对应其认为最接近目标的状态.

- **边界表示** - 贪婪搜索与UCS操作相同, 使用优先队列表示边界. 区别在于贪婪搜索使用启发值 (**估计的前向成本**) 而非**计算的后向成本** (到该状态的路径边权总和) 来确定优先级.

- **完备性与最优性** - 贪婪搜索不能保证找到存在的目标状态, 也不具有最优性, 特别是在选择较差启发函数时. 其行为在不同场景下表现不稳定, 可能直接抵达目标, 也可能像受错误引导的DFS那样探索错误区域.

#### 1.4.3 A\*搜索

- **描述** - $A^{*}$搜索是一种总是选择**总成本估计值最低**的边界节点进行扩展的探索策略, 其中总成本是从起始节点到目标节点的完整成本.

- **边界表示** - 与贪婪搜索和UCS类似, $A^{*}$也使用优先队列表示边界. 唯一区别在于优先级选择方法: $A^{*}$将UCS使用的后向总成本 (路径边权总和) 与贪婪搜索使用的启发值 (前向成本估计) 相加, 得到从起点到目标的**估计总成本**. 由于我们需要最小化从起点到目标的总成本, 这是绝佳的选择.

- **完备性与最优性** - 在适当启发函数下 (稍后讨论) , $A^{*}$搜索兼具完备性和最优性. 它综合了我们目前讨论的所有搜索策略的优点, 融合了贪婪搜索的高速性和UCS的最优性与完备性!

#### 1.4.4 可采纳性

在讨论了启发函数及其在贪婪搜索和$A^{*}$搜索中的应用后, 现在我们来探讨构成"良好"启发函数的条件. 首先用以下定义重新表述UCS, 贪婪搜索和$A^{*}$中优先级队列排序的方法:

- $g(n)$ - UCS计算的后向总成本函数
- $h(n)$ - 贪婪搜索使用的启发函数 (前向成本估计) 
- $f(n)$ - $A^{*}$搜索使用的估计总成本函数, 满足$f(n)=g(n)+h(n)$

在探讨"良好"启发函数之前, 需先回答: 是否任意启发函数都能保持$A^{*}$的完备性和最优性? 显然很容易找到破坏这两个宝贵性质的启发函数. 例如考虑启发函数$h(n)=1-g(n)$, 在任何搜索问题中使用该函数都会得到:

$f(n)=g(n)+h(n)=g(n)+(1-g(n))=1$

此时 $A^{*}$ 退化为所有边权相同的BFS. 如前所述, 在边权非常数的一般情况下, BFS不能保证最优性.

$A^{*}$ 树搜索保持最优性的条件称为**可采纳性**. 可采纳性约束要求可采纳启发函数的估计值既非负也不高估.

定义$h^{*}(n)$为从节点n到目标状态的真实最优前向成本, 可采纳性约束的数学表述为:

$\forall n, 0\leq h(n)\leq h^{*}(n)$

**定理**: 对于给定搜索问题, 若启发函数h满足可采纳性约束, 则在该问题上使用$A^{*}$树搜索将得到最优解.

**证明**: 假设搜索树中存在两个可达目标状态: 最优目标A和次优目标B. 由于A可从起始状态到达, A的某个祖先节点n (可能包含A自身) 必在当前边界上. 我们断言n将在B之前被扩展, 依据以下三点:

1. $g(A) < g(B)$. 因为A最优而B次优, A到起点的后向成本更低
2. $h(A)=h(B)=0$. 根据可采纳性约束, 对于目标状态有$h^{*}(n)=0$, 故$0\leq h(n)\leq 0$
3. $f(n)\leq f(A)$. 根据h的可采纳性, $f(n)=g(n)+h(n)\leq g(n)+h^{*}(n)=g(A)=f(A)$

结合1和2可得$f(A) < f(B)$:

$f(A)=g(A)+h(A)=g(A)<g(B)=g(B)+h(B)=f(B)$

再结合3可得:

$f(n)\leq f(A)\wedge f(A) < f(B) → f(n) < f(B)$

因此n先于B被扩展. 由于n具有任意性, 可推出A的**所有**祖先节点 (含A自身) 都在B之前扩展.

树搜索存在的一个问题是可能陷入状态空间图中的循环无限搜索. 即使不考虑无限循环, 由于到达同一节点可能存在多条路径, 我们往往会多次访问同一节点, 导致指数级工作量. 解决方案是记录已扩展节点集合, 避免重复扩展. 这种优化后的树搜索称为**图搜索**. 此外, 保持最优性还需考虑另一个关键因素. 参考以下状态空间图及对应的搜索树 (带权值和启发值) :

![](../0_Attachment/Pasted%20image%2020250513102135.png)

显然最优路径是$SACG$, 总路径成本$1+1+3=5$. 另一条路径$SBCG$成本为$1+2+3=6$. 但由于节点A的启发值远大于B, 节点C首先作为B的子节点沿次优路径被扩展并加入"已到达"集合. 当$A^{*}$通过A访问C时, 由于已存在更差路径的C节点, 不会重新扩展, 导致错过最优解. 因此, 在$A^{*}$图搜索中保持最优性不仅需要检查节点是否已访问, 还需判断是否找到更优路径.

```pseudocode
function A*-GRAPH-SEARCH(problem, frontier) return a solution or failure
    reached ← an empty dict mapping nodes to the cost to each one
    frontier ← INSERT((MAKE-NODE(INITIAL-STATE[problem]), 0), frontier)
    while not IS-EMPTY(frontier) do
        node, node.CostToNode ← POP(frontier)
        if problem.IS-GOAL(node.STATE) then return node
        if node.STATE is not in reached or reached[node.STATE] > node.CostToNode then
            reached[node.STATE] = node.CostToNode
            for each child-node in EXPAND(problem, node) do
                frontier ← INSERT((child-node, child-node.COST + CostToNode), frontier)
    return failure
```

实现时需注意: 将reached集合存储为哈希集合而非列表至关重要, 因为列表查询成员需要$O(n)$时间复杂度, 这会抵消图搜索的性能优势.

重要提示: 根据定义, 可采纳启发函数必须满足对于任何目标状态G有$h(G)=0$.

#### 1.4.5 支配性

在建立可采纳性及其对保持$A^{*}$搜索最优性的作用后, 我们回到最初的问题: 如何创建"良好"启发函数, 以及如何比较启发函数的优劣. 标准度量是**支配性**. 若启发函数a支配b, 则对于状态空间图中所有节点, a的估计目标距离都不小于b. 数学表述为:

$\forall n: h_{a}(n)\geq h_{b}(n)$

支配性直观体现了启发函数的优劣--如果一个可采纳启发函数支配另一个, 则它必然更优, 因为它总能给出更接近真实目标距离的估计. 此外, **平凡启发函数**定义为$h(n)=0$, 使用它会将$A^{*}$退化为UCS. 所有可采纳启发函数都支配平凡启发函数. 在搜索问题的半格结构中, 平凡启发函数通常位于底层. 下图展示了一个包含多个启发函数$h_{a}, h_{b}, h_{c}$的半格结构示例, 从底层的平凡启发函数到顶层的真实目标距离:

![](../0_Attachment/Pasted%20image%2020250513102302.png)

通常, 对多个可采纳启发函数取最大值得到的函数仍保持可采纳性. 这是因为所有启发函数对任意状态的输出值都受限于可采纳性条件$0\leq h(n)\leq h^{*}(n)$, 该范围内的最大值仍在此范围内. 实践中通常为搜索问题生成多个可采纳启发函数, 并取其输出的最大值构成支配 (即优于) 所有单个启发函数的新启发函数.

### 1.5 局部搜索

在前文中, 我们关注的是找到目标状态以及抵达该状态的最优路径. 但在某些问题中, 我们只关心如何找到目标状态--重建路径可能是微不足道的. 例如在数独问题中, 最优配置就是目标状态. 一旦找到这个配置, 只需逐个填入数字即可知道如何达成目标.

局部搜索算法让我们无需关注抵达路径即可找到目标状态. 在局部搜索问题中, 状态空间由"完整"解的集合构成. 我们通过这些算法寻找满足某些约束或优化目标函数的配置.

![](../0_Attachment/Pasted%20image%2020250513102838.png)

上图展示了状态空间上目标函数的一维曲线图. 对于该函数, 我们希望找到对应最高目标值的状态. 局部搜索算法的核心思想是: 从每个状态出发, 算法会向目标值更高的邻近状态移动, 直到达到 (期望中的全局) 最大值. 我们将介绍四种此类算法: **爬山法**, **模拟退火**, **局部束搜索**和**遗传算法**. 这些算法也常用于优化任务中以最大化或最小化目标函数.

#### 1.5.1 爬山搜索

爬山搜索算法 (又称**最陡上升法**) 从当前状态移动到能最大程度提升目标值的邻近状态. 该算法不维护搜索树, 仅跟踪状态及其对应的目标值. 爬山法的"贪婪性"使其容易陷入**局部最大值** (见图4.1) , 因为这些点在局部看来就是全局最大值; 同时也容易受困于**高原区域** (见图4.1). 高原可分为两类: 无法通过任何方向获得改进的"平坦区域" ("平坦局部最大值") , 以及进展缓慢的平坦区域 ("肩部区域").

爬山法的变体包括**随机爬山法** (从可能的上升移动中随机选择动作) , 实践证明这种变体能以更多迭代次数为代价收敛到更高极值. 另一变体是允许**随机横向移动**, 即接受不严格提升目标值的移动, 帮助算法逃离"肩部区域".

```pseudocode
function HILL-CLIMBING(problem) returns a state
	current ← make-node(problem.initial-state)
	loop do
		neighbor ← a highest-valued successor of current
		if neighbor.value ≤ current.value then
			return current.state
        current ← neighbor
```

如上所示, 爬山算法会迭代移动到目标值更高的状态, 直到无法继续优化. 该算法是不完备的. 而**随机重启爬山法**通过从随机选择的初始状态进行多次爬山搜索, 最终必然能收敛到全局最大值, 因此是完备的.

需要注意的是, 后续课程中会出现的"梯度下降"概念与爬山法本质相同, 区别在于前者是最小化代价函数而非最大化目标函数.

#### 1.5.2 模拟退火搜索

第二个局部搜索算法是模拟退火. 该算法结合了随机游走 (随机移动到邻近状态) 和爬山法, 旨在获得完备且高效的搜索算法. 模拟退火允许移动到可能降低目标值的状态.

算法在每个时间步随机选择移动方向: 若移动导致目标值升高则总是接受; 若导致目标值降低则以一定概率接受. 这个概率由温度参数决定--初始高温允许更多"不良"移动, 随后按预定"退火计划"逐步降温. 理论上, 若降温足够缓慢, 模拟退火算法将以接近1的概率达到全局最大值.

![图3: 模拟退火算法](../0_Attachment/Pasted%20image%2020250513103419.png)

#### 1.5.3 局部束搜索

局部束搜索是爬山法的另一变体, 关键区别在于该算法每次迭代跟踪k个状态 (线程). 算法从随机初始化k个状态开始, 每轮像爬山法那样选择k个新状态. 这不是简单的k个并行爬山法--核心在于算法会从所有线程的后继状态中选出k个最优状态. 若任一线程找到最优值, 算法即终止.

k个线程间可以共享信息, 使得目标值高的"优质线程"能吸引其他线程向该区域靠拢. 与爬山法类似, 局部束搜索也容易陷入"平坦"区域. 随机束搜索 (类比随机爬山法) 可缓解此问题.

#### 1.5.4 遗传算法

最后介绍遗传算法, 这是局部束搜索的变体, 广泛应用于优化任务. 如其名所示, 该算法受进化论启发. 遗传算法开始时如同束搜索, 随机初始化k个状态 (称为种群) , 每个状态 (个体) 用有限字母表的字符串表示.

以课程中提到的8皇后问题为例: 该约束满足问题要求在8×8棋盘上放置8个互不攻击的皇后 (即不在同行, 同列或同对角线). 之前介绍的所有算法都可用于解决此问题.

在遗传算法中, 我们用1-8的数字表示每列中皇后的行位置 (图4.6(a)). 每个个体通过评估函数 (适应度函数) 进行评分, 8皇后问题中该函数值为非攻击皇后对数. 选择个体"繁殖"的概率与其适应度成正比 (图4.6(c)) , 通过交叉点随机交换父代字符串生成子代 (图4.6(d)) , 每个子代还有独立概率发生随机变异.

(a) 8皇后问题: 在棋盘放置8个互不攻击的皇后 (皇后会攻击同行, 同列或同对角线的棋子). 该布局接近解, 但第四和第七列的皇后在对角线上互相攻击. (b) 启发式代价估计h=17的8皇后状态. 棋盘显示每个可能后继状态的h值 (通过移动列内皇后获得) , 有8个最佳移动选项h=12. 爬山算法将任选其一.

![图4: 8皇后问题](../0_Attachment/Pasted%20image%2020250513103653.png)

![图5: 遗传算法例子](../0_Attachment/Pasted%20image%2020250513103658.png)

![图6: 遗传算法伪代码](../0_Attachment/Pasted%20image%2020250513103704.png)

与随机束搜索类似, 遗传算法在探索状态空间时尝试向上移动, 并在线程间交换信息. 其核心优势在于交叉操作--已进化出高评分的字母块可与其他高评分块组合, 从而产生总分更高的解.

![图7: 8皇后问题解](../0_Attachment/Pasted%20image%2020250513103713.png)

图4.6(c)中前两个父代与图4.6(d)中第一个子代对应的8皇后状态. 绿色列在交叉步骤丢失, 红色列被保留.  (图4.6中的数字说明: 第1行是最底行, 第8行是最顶行) 

### 1.6 总结

我们探讨了搜索问题及其核心要素: 状态空间, 动作集合, 转移函数, 动作成本, 初始状态与目标状态. 智能体通过传感器和执行器与环境交互, 其行为由智能体函数全面定义. 理性智能体以期望效用最大化为目标. 最后, 我们使用PEAS (性能指标, 环境, 执行器, 传感器) 框架来描述任务环境.

关于搜索问题, 可通过多种技术解决, 包括但不限于CS 188课程中研究的五种方法:
- 广度优先搜索
- 深度优先搜索
- 统一成本搜索
- 贪婪搜索
- A*搜索

前三种属于无信息搜索, 后两种则采用启发式估计目标距离的启发式搜索以优化性能. 我们还区分了上述技术中的树搜索与图搜索算法.

此外, 我们讨论了局部搜索算法及其应用场景. 当不关注路径而需满足约束或优化目标时, 这类方法能节省空间并在庞大状态空间中寻找可行解. 涉及的经典局部搜索方法包括:
- 爬山法
- 模拟退火
- 局部束搜索
- 遗传算法

函数优化思想将在后续课程中重现, 尤其在神经网络章节会再次涉及.

## 2 约束满足问题 (CSPs) 

### 2.1 约束满足问题

在之前的笔记中, 我们学习了如何找到搜索问题的最优解, 这是一种规划问题. 现在, 我们将学习解决另一类相关问题--**约束满足问题 (CSPs)**. 与搜索问题不同, CSPs 是一种**识别问题**, 即我们只需判断某个状态是否为目标状态, 而无需考虑如何到达该目标. CSPs 由以下三个要素定义:

1. **变量**: CSPs 包含一组 $N$ 个变量 $X_{1},\ldots, X_{N}$, 每个变量可以从某个定义的值集合中取一个值.
2. **定义域**: 一个集合 $\left\{x_{1},\ldots, x_{d}\right\}$, 表示 CSP 变量所有可能的取值.
3. **约束**: 约束定义了变量值的限制条件, 可能涉及其他变量.

以 **N皇后问题** 为例: 给定一个 $N \times N$ 的棋盘, 能否找到一种放置 $N$ 个皇后的配置, 使得没有两个皇后互相攻击? 

![图1: N皇后问题](../0_Attachment/Pasted%20image%2020250514094742.png)

我们可以将其建模为 CSP:

1. **变量**: $X_{ij}$, 其中 $0 \leq i, j < N$. 每个 $X_{ij}$ 表示棋盘上的一个格子, $i$ 和 $j$ 分别表示行号和列号.
2. **定义域**: $\{0,1\}$. $X_{ij}$ 取值为 0 或 1, 布尔值表示位置 $(i,j)$ 是否有皇后.
3. **约束**:
   - **行约束**: $\forall i, j, k \left(X_{ij}, X_{ik}\right) \in \{(0,0),(0,1),(1,0)\}$, 确保同一行最多一个皇后.
   - **列约束**: $\forall i, j, k \left(X_{ij}, X_{kj}\right) \in \{(0,0),(0,1),(1,0)\}$, 确保同一列最多一个皇后.
   - 对角线约束:
     - $\forall i,j,k \left(X_{ij}, X_{i+k,j+k}\right) \in \{(0,0),(0,1),(1,0)\}$ (主对角线) .
     - $\forall i,j,k \left(X_{ij}, X_{i+k,j-k}\right) \in \{(0,0),(0,1),(1,0)\}$ (副对角线) .
   - **总数约束**: $\sum_{i,j} X_{ij} = N$, 确保恰好放置 $N$ 个皇后.

约束满足问题是 **NP问题**, 这意味着目前没有已知的多项式时间算法可以求解. 对于 $N$ 个变量且每个变量定义域大小为 $O(d)$ 的问题, 可能的赋值组合数为 $O(d^N)$, 随变量数量指数增长. 为解决这一问题, 我们通常将 CSPs 转化为搜索问题:

- **状态**: 部分赋值 (部分变量已赋值, 其余未赋值) .
- **后继函数**: 为当前状态中一个未赋值的变量赋予一个值, 生成新状态.
- **目标测试**: 检查所有变量是否已赋值且满足所有约束.

CSPs 比传统搜索问题具有更多结构特征, 我们可以结合启发式方法利用这些结构, 在合理时间内找到解.

#### 2.1.1 约束图

以 **地图着色问题** 为例: 给定一组颜色, 为地图着色, 要求相邻区域颜色不同.

![图2: 图着色漫画](../0_Attachment/Pasted%20image%2020250514094837.png)

CSPs 常表示为 **约束图**, 其中:

- **节点**: 变量.
- **边**: 变量间的约束.

约束类型:

1. **一元约束**: 涉及单个变量, 不体现在约束图中, 仅用于修剪变量的定义域.
2. **二元约束**: 涉及两个变量, 对应图中的普通边.
3. **高阶约束**: 涉及三个及以上变量, 需特殊表示.

**示例**: 澳大利亚地图着色问题

![图3: 澳大利亚地图](../0_Attachment/Pasted%20image%2020250514094902.png)

- **约束**: 相邻州颜色不同.
- **约束图**: 为每对相邻州添加一条边.

约束图的价值在于揭示 CSP 的结构信息, 例如稀疏/密集连接, 树形结构等, 这些信息可指导求解策略.

![图4: 澳大利亚的约束图](../0_Attachment/Pasted%20image%2020250514094930.png)

### 2.2 约束满足问题的求解

传统上, CSPs 通过 **回溯搜索** (Backtracking Search) 求解. 回溯搜索是深度优先搜索 (DFS) 的优化版本, 改进基于以下原则:

1. **变量顺序固定**: 按固定顺序为变量赋值 (赋值具有交换性) .
2. **前瞻性赋值**: 仅为当前变量选择不与已赋值变量冲突的值. 若无合法值, 则回溯到上一个变量重新赋值.

**伪代码** (递归回溯) :

![图1: 回溯搜索的伪代码](../0_Attachment/Pasted%20image%2020250514095000.png)

**对比 DFS 与回溯搜索**:

- DFS 盲目赋值, 导致大量无效搜索.
- 回溯搜索通过约束检查减少回溯次数.

进一步优化手段包括:

- **过滤** (如前向检查, 弧相容) .
- **变量/值排序启发式**.
- **利用问题结构** (如树形 CSPs 可线性时间求解) .

![图2: DFS 对比回溯搜索](../0_Attachment/Pasted%20image%2020250514195221.png)

### 2.3 过滤技术

我们考虑的第一个CSP性能优化方法是**过滤**, 即通过提前移除已知会导致回溯的值来修剪未分配变量的值域. 最基础的过滤方法是**前向检查**: 每当给变量$X_{i}$赋值时, 会修剪与之共享约束的未分配变量的值域, 这些值若被赋值将违反约束条件. 每当新变量被赋值时, 我们可以运行前向检查, 并在约束图中修剪与新赋值变量相邻的未分配变量的值域. 以地图着色问题为例, 未分配变量及其可能值如下:

![图1: 前向过滤示例](../0_Attachment/Pasted%20image%2020250514095223.png)

当依次赋值$WA=red$和$Q=green$时, 与WA, Q或两者相邻的NT, NSW和SA等州的值域大小会因值被淘汰而减小. 前向检查的思想可以推广为**弧一致性**原则. 对于弧一致性, 我们将CSP约束图中的每条无向边视为**两个**方向相反的有向边, 每条有向边称为**弧**. 弧一致性算法流程如下:

1. **初始化**: 将CSP约束图中所有弧存入队列Q

2. **迭代处理**:
   - 从Q中移除弧$X_{i}\longrightarrow X_{j}$, 并确保对于尾部变量 $X_{i}$ 的**每个**剩余值$v$, 头部变量 $X_{j}$ **至少存在一个**剩余值 $w$ 使得赋值 $X_{i}=v,\,X_{j}=w$ 不违反任何约束. 若 $X_{i}$ 的某个值 $v$ 与 $X_{j}$ 的所有剩余值都冲突, 则从 $X_{i}$ 的值域中移除 $v$
   - 若在弧 $X_{i}\longrightarrow X_{j}$ 的一致性检查中移除了 $X_{i}$ 的至少一个值, 则将所有形如 $X_{k}\longrightarrow X_{i}$ 的弧加入Q ($X_{k}$为所有未分配变量) . 若 $X_{k}\longrightarrow X_{i}$ 已在Q中则无需重复添加

3. **终止条件**: 当Q为空或某个变量的值域为空时 (触发回溯)

弧一致性算法通常不够直观, 我们通过地图着色示例演示:

初始时将共享约束的未分配变量间所有弧加入队列Q:
 $Q=[ SA\rightarrow V, V\rightarrow SA, SA\rightarrow NSW, NSW\rightarrow SA, SA\rightarrow NT, NT\rightarrow SA, V\rightarrow NSW, NSW\rightarrow V]$
![图2: 弧一致性示例 1](../0_Attachment/Pasted%20image%2020250514095417.png)

处理第一个弧 $SA \rightarrow V$ 时, SA的值域 {blue} 与V的值域 {red,green,blue} 存在有效组合, 故无需修剪. 但处理 $V\rightarrow SA$ 时, 若 $V=blue$ 会导致 SA 无有效值, 因此从V的值域中移除 blue.

由于修剪了V的值域, 需将所有以V为头的弧重新入队 ($SA\rightarrow V$和$NSW\rightarrow V$) , 最终更新后的队列为:
 $Q=[SA\rightarrow NSW, NSW\rightarrow SA, SA\rightarrow NT, NT\rightarrow SA, V\rightarrow NSW, NSW\rightarrow V, SA\rightarrow V]$

![图3: 弧一致性示例 2](../0_Attachment/Pasted%20image%2020250514095509.png)

该过程持续进行, 直到处理弧$SA\rightarrow NT$时从SA的值域中移除blue导致其值域为空, 触发回溯. 注意在Q中$NSW\rightarrow SA$先于$SA\rightarrow NT$被处理, 该弧的一致性检查会从NSW的值域中移除blue.

![图4: 弧一致性示例 3](../0_Attachment/Pasted%20image%2020250514095543.png)

弧一致性通常通过AC-3算法实现 (Arc Consistency Algorithm \#3), 其最坏时间复杂度为 $O\left(e d^{3}\right)$ ($e$为弧数量, $d$为最大值域大小). 相比前向检查, 弧一致性是更全面的值域修剪技术, 能减少回溯次数, 但需要更多计算资源. 因此在选择CSP的过滤技术时需权衡这一利弊.

关于一致性的扩展说明: 弧一致性是更广义的**k-一致性**概念的子集. k-一致性保证对于CSP中任意k个节点, 对其中k-1个节点的一致性赋值能确保第 $k^{th}$ 个节点至少存在一个有效值. 进一步地, **强k-一致性**要求图不仅满足k-一致性, 还同时满足$k-1, k-2,\ldots, 1$ 一致性. 显然, 更高阶的一致性需要更高计算成本. 在此定义下, 弧一致性等价于**2-一致性**.

![图5: AC-3算法伪代码](../0_Attachment/Pasted%20image%2020250514095619.png)

### 2.4 排序策略

在CSP求解过程中, 我们通常需要固定变量和值的顺序. 实践中更有效的方法是动态计算下一个变量和值的选择, 主要遵循两大原则: **最小剩余值**和**最少约束值**:

1. **最小剩余值 (MRV) **: 选择剩余有效值最少的未分配变量 (即约束最严格的变量) . 直观而言, 约束最严格的变量最容易因值耗尽导致回溯, 因此应优先赋值.
2. **最少约束值 (LCV) **: 选择对未分配变量值域修剪最少 *(使用的最少的值)* 的赋值. 这需要额外计算 (如为每个值重新运行弧一致性/前向检查) , 但合理使用仍能提升速度.

#### 2.4.1 结构优化

最后一类CSP优化方法利用问题结构特性. 对于**树结构CSP** (约束图**无环**的情况) , 可将求解时间从 $O(d^N)$ 降至 $O(nd^2)$ (与变量数量线性相关), 具体步骤如下:

1. **根节点选择**: 任选约束图中一个节点作为树根 (根据图论, 树结构中任意节点均可作为根)

2. **边定向**: 将所有无向边转换为从根节点向外指向的有向边, 然后对所得有向无环图进行拓扑排序 (使所有边向右指) . 示例如下: ![图1: 树结构CSP](../0_Attachment/Pasted%20image%2020250514200756.png)

3. **反向弧一致性检查**: 从$i=n$到$i=2$依次检查所有弧$\operatorname{Parent}\left(X_{i}\right)\longrightarrow X_{i}$的一致性. 此过程会修剪部分值域. 示例如下: ![图2: 修建树](../0_Attachment/Pasted%20image%2020250514200920.png)

4. **前向赋值**: 从$X_{1}$到$X_{n}$依次为每个$X_{i}$赋值, 确保与其父节点值一致. 由于已对所有弧强制执行一致性, 每个节点的子节点至少存在一个有效值, 因此该过程能保证得到正确解.

对于**近似**树结构的CSP, 可通过**割集条件法**扩展上述算法:

1. 找到约束图中最小的变量子集 (称为**割集**), 移除后得到树结构 (例如地图着色中的SA州是最小割集)
2. 为割集中所有变量赋值, 并修剪相邻节点的值域
3. 对剩余的树结构CSP使用上述算法求解

![图3: 割集示例](../0_Attachment/Pasted%20image%2020250514201131.png)

割集大小为 $c$ 时, 可能需要进行最多 $d^{c}$ 次回溯. 由于剩余树结构CSP有 $(n-c)$ 个变量, 其求解时间为 $O\left((n-c) d^{2}\right)$ . 因此割集条件法的总时间复杂度为 $O\left(d^{c}(n-c)d^{2}\right)$ , 在$c$较小时效率极高.

### 2.5 局部搜索

作为最后一个值得关注的主题, 回溯搜索并非解决约束满足问题 (CSP) 的唯一算法. 另一种广泛使用的算法是**局部搜索**, 其核心思想看似简单却异常有效. 局部搜索通过迭代改进实现--先**随机**分配初始值, 然后迭代选择随机冲突变量, 将其值重新赋值为违反最少约束的值 (该策略称为**最小冲突启发式**) , 直到不再存在约束冲突. 在此策略下, N皇后等约束满足问题的求解变得极其高效 (时间和空间层面). 例如在下图4皇后的例子中, 仅需2次迭代即可得到解:

![图1: 4皇后问题](../0_Attachment/Pasted%20image%2020250514201241.png)

实际上, 局部搜索不仅对任意大规模N皇后问题表现出近乎常数时间的高成功率, 对任何随机生成的CSP也如此！然而尽管有这些优势, 局部搜索既不完备也不最优, 因此不一定收敛到最优解. 此外, 存在一个临界比例, 超过该比例后局部搜索的计算成本会急剧上升:

![图2: bi'l](../0_Attachment/Pasted%20image%2020250514201331.png)

上图展示了状态空间上目标函数的一维曲线. 我们的目标是找到对应最高目标值的状态. 局部搜索算法的核心思想是: 从每个状态出发, 逐步向目标值更高的邻近状态移动, 直到达到 (期望中的全局) 最大值.

我们将介绍三种此类算法: 爬山法, 模拟退火和遗传算法. 这些算法也常用于优化任务中以最大化或最小化目标函数.

#### 2.5.1 爬山搜索

爬山搜索算法 (或称最陡上升法) 从当前状态移动到能提升目标值的邻近状态. 该算法不维护搜索树, 仅记录状态及其对应目标值. 爬山法的"贪婪性"使其易陷入局部最大值 (如下图) , 因为这些点在局部看来就是全局最大值; 此外还会受困于高原区域. 高原可分为两类: 无法找到改进方向的"平坦区域" ("平坦局部最大值") 或进展缓慢的"肩部区域". 改进版本如随机爬山法 (随机选择上坡移动方向) 已被提出, 实践表明这种变体能以更多迭代次数为代价收敛到更高最大值.

![图3: 全局和局部最大值](../0_Attachment/Pasted%20image%2020250514202115.png)

爬山法的伪代码如下所示. 如其名所示, 算法迭代移动到更高目标值的状态直至无法继续改进. 爬山法是不完备的. 而**随机重启爬山法**通过从随机初始状态开始多次执行爬山搜索, 最终必然能命中全局最大值所在初始状态, 因此是完备的.

![图4: 爬山算法伪代码](../0_Attachment/Pasted%20image%2020250514202136.png)

#### 2.5.2 模拟退火搜索

第二个局部搜索算法是**模拟退火**. 该算法结合了随机游走 (随机移动到邻近状态) 和爬山法, 以得到完备且高效的搜索算法. 模拟退火允许移动到可能降低目标值的状态: 算法在每个状态随机选择移动, 若移动提升目标值则总是接受; 若降低目标值则以一定概率接受. 该概率由温度参数决定--初始温度较高 (允许更多"不良"移动), 随后按预定计划降温. 若降温足够缓慢, 模拟退火算法将以接近1的概率达到全局最大值.

![模拟退火算法伪代码](../0_Attachment/Pasted%20image%2020250514202306.png)

#### 2.5.3 遗传算法

最后介绍**遗传算法**, 这是局部束搜索的变体, 也广泛用于优化任务. 遗传算法以k个随机初始状态 (称为种群) 开始束搜索. 状态 (或称个体) 用有限字母表的字符串表示. 以课堂讨论的8皇后问题为例: 每个个体可用1-8的数字串表示, 对应每列皇后的行位置 (图6列a) . 通过评估函数 (适应度函数) 对个体进行评分 (8皇后问题中是非互攻的皇后对数) , 并按此值排序.

![图6: 遗传算法示例](../0_Attachment/Pasted%20image%2020250514202641.png)

个体被选中"繁殖"的概率与其适应度成正比. 根据这些概率选择成对个体进行繁殖 (图6列c), 通过在随机选择的交叉点交换父代字符串产生子代. 最后每个子代以独立概率发生随机变异. 遗传算法伪代码如下:

![遗传算法伪代码](../0_Attachment/Pasted%20image%2020250514202735.png)

遗传算法在探索状态空间时尝试向上移动, 并通过线程间信息交换实现优势. 其主要优点在于交叉操作--这使得进化出的高评分字母块能与其他优质块组合, 最终产生高总分解决方案.

### 2.6 总结

需谨记: 约束满足问题通常不存在关于变量数量的多项式时间高效解法. 但通过以下启发式方法, 我们常可在可接受时间内找到解:

● **过滤**: 提前修剪未分配变量的值域以避免不必要回溯. 我们学过的两种重要过滤技术是前向检查和弧一致性.

● **排序**: 选择下一步要赋值的变量或值以使回溯可能性最小化. 变量选择采用MRV (最小剩余值) 策略, 值选择采用LCV (最少约束值) 策略.

● **结构**: 若CSP呈树状或近似树状结构, 可运行树结构CSP算法在线性时间内求解. 类似地, 对近似树状CSP, 可采用割集调节将其转换为多个独立树状CSP分别求解.

## 3. 博弈

### 3.1 博弈

在第一篇笔记中, 我们讨论了搜索问题及其高效, 最优的解决方法--通过强大的通用搜索算法, 智能体可以制定最佳计划并执行以达成目标. 现在, 让我们转换思路, 考虑智能体面对一个或多个对手的场景, 这些对手会试图阻止智能体达成目标. 此时, 智能体无法再使用已学的搜索算法来制定计划, 因为我们通常无法确定对手会如何针对我们制定策略并回应我们的行动. 相反, 我们需要使用一类新的算法来解决对抗性搜索问题, 更常见的说法是博弈.

博弈有多种类型. 博弈中的行动可能具有确定性或随机性 (概率性) 的结果, 可以有任意数量的玩家, 可能是零和博弈, 也可能不是. 我们将首先讨论确定性零和博弈, 其中行动是确定性的, 且我们的收益直接等同于对手的损失, 反之亦然. 理解这类博弈的最简单方式是将其视为由单一变量值定义的博弈, 其中一方试图最大化该变量, 而另一方试图最小化它, 从而形成直接竞争. 在Pacman (吃豆人) 游戏中, 这个变量就是你的分数, 你通过快速高效地吃掉豆子来最大化分数, 而幽灵则通过先吃掉你来最小化你的分数. 许多常见的家庭游戏也属于这类博弈:

- **跳棋**: 第一个跳棋计算机程序诞生于1950年. 此后, 跳棋成为一种“已解决”的博弈, 即在双方玩家都采取最优策略的情况下, 任何局面都可以确定为某一方的胜利, 失败或平局.
- **国际象棋**: 1997年, “深蓝”成为第一个在六局比赛中击败人类国际象棋冠军加里·卡斯帕罗夫的计算机程序. 深蓝采用极其复杂的方法, 每秒评估超过2亿个局面. 当前的程序更加强大, 尽管历史意义不如深蓝.
- **围棋**: 围棋的搜索空间比国际象棋大得多, 许多人曾认为围棋程序在未来几年内都无法击败人类世界冠军. 然而, 2016年3月, 谷歌开发的AlphaGo以4比1的历史性比分击败了围棋冠军李世石.

上述所有击败世界冠军的程序都至少在一定程度上使用了我们将要介绍的对抗性搜索技术. 与普通搜索返回完整计划不同, 对抗性搜索返回一种策略或策略函数, 该函数根据智能体及其对手的当前配置推荐最佳行动. 我们将看到, 这类算法具有一种美妙的特性: 通过计算产生行为--我们运行的计算在概念上相对简单且广泛通用, 但自然地生成了同一团队智能体之间的协作以及对对手智能体的“智胜”.

标准博弈形式化包含以下定义:

- 初始状态 $s_0$
- 玩家 $Players(s)$ 表示当前回合的玩家
- 行动 $Actions(s)$ 表示当前玩家可用的行动
- 转移模型 $Result(s,a)$
- 终止测试 $Terminal\text{-}test(s)$
- 终止效用 $Utility(s,player)$

![图1: 常见游戏](../0_Attachment/Pasted%20image%2020250515164134.png)

### 3.2 极小化极大算法

我们将讨论的第一个零和博弈算法是**极小化极大算法 (minimax)** , 其核心假设是对手行为最优, 即对手总是采取对我们最不利的行动. 为了介绍这一算法, 我们首先需要形式化**终止效用**和**状态值**的概念. 状态值是指控制该状态的智能体可以获得的最高分数. 为了理解这一点, 观察以下极其简单的Pacman游戏棋盘:

![图1: Pacman](../0_Attachment/Pasted%20image%2020250515164222.png)

假设Pacman初始有10分, 每移动一步损失1分, 直到吃掉豆子, 此时游戏进入**终止状态**并结束. 我们可以为这个棋盘构建如下的**博弈树**, 其中状态的子节点与普通搜索问题中的搜索树类似:

![图2: Pacman博弈树](../0_Attachment/Pasted%20image%2020250515164249.png)

从这棵树中可以看出, 如果Pacman直接走向豆子, 游戏结束时他的分数为8分; 而如果他在任何位置回头, 最终会得到更低的分数. 现在我们已经生成了一个包含多个终止状态和中间状态的博弈树, 可以形式化这些状态值的含义.

状态值定义为智能体从该状态可以达成的最佳结果 (**效用**) . 稍后我们会更具体地形式化效用的概念, 但现在可以简单地将智能体的效用视为其分数或得分. 终止状态的值 (称为终止效用) 始终是某个确定的已知值, 是博弈的固有属性. 在Pacman的例子中, 最右侧终止状态的值就是8, 即Pacman直接走向豆子获得的分数. 此外, 非终止状态的值定义为其子节点值的最大值. 用 $V(s)$ 表示状态 $s$ 的值, 我们可以总结上述讨论:

对于所有非终止状态, $V(s)=\max_{s^{\prime}\in\text{ successors}(s)} V\left(s^{\prime}\right)$

对于所有终止状态, $V(s)=\text{已知值}$

这建立了一个简单的递归规则. 由此可以理解, 根节点的直接右子节点的值为8, 而直接左子节点的值为6, 因为这是智能体从初始状态分别向右或向左移动时可以获得的最高分数. 通过这种计算, 智能体可以确定最优行动是向右移动, 因为右子节点的值大于左子节点.

现在, 我们引入一个新的游戏棋盘, 其中有一个对抗性幽灵试图阻止Pacman吃掉豆子.

![图3 存在幽灵的Pacman](../0_Attachment/Pasted%20image%2020250515164329.png)

游戏规则规定两个智能体轮流移动, 从而生成一棵博弈树, 其中两个智能体在树的层级上交替“控制”节点. 智能体控制某个节点意味着该节点对应的状态是其回合, 因此它可以决定采取什么行动并相应地改变游戏状态. 以下是上述新双智能体游戏棋盘生成的博弈树:

![图4: Pacman博弈树](../0_Attachment/Pasted%20image%2020250515164442.png)

蓝色节点对应Pacman控制的节点, 可以决定采取什么行动; 红色节点对应幽灵控制的节点. 注意, 幽灵控制节点的所有子节点都是幽灵从其父节点状态向左或向右移动后的状态, 反之亦然. 为了简化, 我们将这棵博弈树截断为深度为2的树, 并为终止状态分配以下虚构值:

![图5: 小型博弈树](../0_Attachment/Pasted%20image%2020250515164524.png)

自然地, 添加幽灵控制的节点会改变Pacman认为最优的行动. 新的最优行动由极小化极大算法确定. 与在树的每一层对子节点效用取最大值不同, 极小化极大算法仅在Pacman控制的节点对其子节点取最大值, 而在幽灵控制的节点对其子节点取最小值. 因此, 上述两个幽灵节点的值分别为 $\min(-8,-5)=-8$ 和 $\min(-10,+8)=-10$. 相应地, Pacman控制的根节点的值为 $\max(-8,-10)=-8$. 由于Pacman希望最大化其分数, 他会选择向左移动并获得-8分, 而不是试图吃掉豆子并获得-10分. 这是“通过计算产生行为”的典型例子--尽管Pacman希望获得最右子节点状态中的+8分, 但通过极小化极大算法, 他“知道”一个表现最优的幽灵不会允许他实现这一目标. 为了采取最优行动, Pacman被迫对冲风险, 反直觉地远离豆子以最小化失败的幅度. 我们可以总结极小化极大算法为状态赋值的方式如下:

- 对于智能体控制的状态, $V(s)=\max_{s^{\prime}\in\text{ successors}(s)}V\left(s^{\prime}\right)$

- 对于对手控制的状态, $V(s)=\min_{s^{\prime}\in successors(s)}V(s^{\prime})$

- 对于终止状态, $V(s)=\text{已知值}$


在实现上, 极小化极大算法的行为类似于深度优先搜索 (DFS), 按照与DFS相同的顺序计算节点的值, 从最左侧的终止节点开始, 逐步向右迭代. 更准确地说, 它是对博弈树的**后序遍历**. 极小化极大算法的伪代码既优雅又直观简单, 如下所示. 注意, 极小化极大算法将返回一个行动, 该行动对应于根节点到其取值子节点的分支.

![图6: 极小化极大算法伪代码](../0_Attachment/Pasted%20image%2020250515164602.png)

#### 3.2.1 Alpha-Beta剪枝

极小化极大算法看起来近乎完美--简单, 最优且直观. 然而, 其执行过程与深度优先搜索非常相似, 时间复杂度也相同, 为令人沮丧的 $O(b^m)$. 回顾一下, $b$ 是分支因子, $m$ 是找到终止节点的近似树深度. 对于许多博弈来说, 这一运行时间过长. 例如, 国际象棋的分支因子 $b \approx 35$, 树深度 $m \approx 100$. 为了缓解这一问题, 极小化极大算法有一种优化方法--**Alpha-Beta剪枝**.

从概念上讲, Alpha-Beta剪枝是这样的: 如果你试图通过查看节点的后继节点来确定其值, 那么一旦你知道该节点的值最多只能等于其父节点的最优值, 就可以停止查看. 让我们通过一个例子来理解这一复杂陈述的含义. 考虑以下博弈树, 其中方形节点对应终止状态, 向下的三角形对应最小化节点, 向上的三角形对应最大化节点:

![图7: Alpha-Beta剪枝示例1](../0_Attachment/Pasted%20image%2020250515164638.png)

让我们逐步分析极小化极大算法如何推导这棵树--它首先遍历值为3, 12和8的节点, 并为最左侧的最小化节点赋值 $\min(3,12,8)=3$. 然后, 它为中间的最小化节点赋值 $\min(2,4,6)=2$, 为最右侧的最小化节点赋值 $\min(14,5,2)=2$, 最后为根节点的最大化节点赋值 $\max(3,2,2)=3$. 然而, 如果我们思考这一情况, 可以意识到一旦我们看到中间最小化节点的子节点值为2, 就无需再查看该节点的其他子节点. 为什么? 因为我们已经看到中间最小化节点的一个子节点值为2, 所以无论其他子节点的值如何, 中间最小化节点的值最多为2. 既然这一点已经确定, 让我们再进一步思考--根节点的最大化节点需要在左最小化节点的值3和中间最小化节点的值 ($\leq 2$) 之间做出选择, 它肯定会选择左最小化节点返回的3, 而不管中间最小化节点剩余子节点的值如何. 这正是我们可以**剪枝**的原因, 无需查看中间最小化节点的剩余子节点:

![图8: Alpha-Beta剪枝示例2](../0_Attachment/Pasted%20image%2020250515164649.png)

实现这种剪枝可以将我们的运行时间降低至 $O\left(b^{m/2}\right)$, 有效地将“可解”深度翻倍. 在实践中, 效果通常不如理论显著, 但通常可以让我们至少多搜索一两层. 这仍然非常重要, 因为能够提前思考3步的玩家比只能思考2步的玩家更有可能获胜. 这种剪枝正是带有Alpha-Beta剪枝的极小化极大算法所做的, 其实现如下:

![图9: Alpha-Beta剪枝伪代码](../0_Attachment/Pasted%20image%2020250515164715.png)

花些时间将其与普通极小化极大算法的伪代码进行比较, 注意我们现在可以在不搜索所有后继节点的情况下提前返回.

#### 3.2.2 评估函数

尽管Alpha-Beta剪枝可以帮助增加我们可以运行极小化极大算法的深度, 但对于大多数博弈来说, 这通常还远远不足以到达搜索树的底部. 因此, 我们转向**评估函数**, 这些函数接受一个状态并输出对该节点真实极小化极大值的估计. 通常, 一个好的评估函数会将“更好”的状态赋予更高的值. 评估函数广泛应用于**深度受限的极小化极大算法**中, 其中我们将位于最大可解深度的非终止节点视为终止节点, 并根据精心选择的评估函数为它们分配模拟的终止效用. 由于评估函数只能产生非终止效用值的估计, 这移除了运行极小化极大算法时最优行为的保证.

在设计运行极小化极大算法的智能体时, 通常会投入大量思考和实验来选择评估函数. 评估函数越好, 智能体的行为就越接近最优. 此外, 在使用评估函数之前深入树中更深层次也往往会带来更好的结果--将计算深埋在博弈树中可以减轻对最优性的妥协. 这些函数在博弈中的作用与启发式在标准搜索问题中的作用非常相似.

评估函数最常见的设计是特征的线性组合:

$Eval(s)=w_{1}f_{1}(s)+w_{2}f_{2}(s)+...+w_{n}f_{n}(s)$

每个 $f_{i}(s)$ 对应从输入状态 $s$ 中提取的一个特征, 每个特征被赋予相应的**权重** $w_{i}$. 特征只是游戏状态的某个元素, 我们可以提取并赋予数值. 例如, 在跳棋游戏中, 我们可能会构建一个包含4个特征的评估函数: 己方兵的数量, 己方王的数量, 对手兵的数量和对手王的数量. 然后, 我们会根据它们的重要性大致选择适当的权重. 在跳棋的例子中, 为己方的兵和王选择正权重, 为对手的兵和王选择负权重是最合理的. 此外, 由于王在跳棋中是比兵更有价值的棋子, 与己方或对手的王对应的特征的权重绝对值应该大于与兵对应的特征. 以下是一个符合我们刚刚讨论的特征和权重的可能评估函数:

$Eval(s)=2\cdot agent\_kings(s)+agent\_pawns(s)-2\cdot opponent\_kings(s)-opponent\_pawns(s)$

可以看出, 评估函数的设计可以非常自由, 甚至不必是线性函数. 例如, 在强化学习应用中, 基于神经网络的非线性评估函数非常常见. 最重要的是要记住, 评估函数应尽可能频繁地为更好的位置赋予更高的分数. 这可能需要大量微调和实验, 以测试使用具有多种不同特征和权重的评估函数的智能体的性能.

### 3.3 期望最大算法

我们已经了解了极小极大算法的工作原理, 以及如何通过完整的极小极大搜索来应对最优对手的最优策略. 然而, 极小极大算法在应对某些特定情境时存在天然限制. 由于极小极大算法假设对手总是采取最优策略, 因此在面对不一定总是最优应对的场景时 (例如带有随机性的卡牌或骰子游戏, 或行为随机/次优的不可预测对手), 该算法往往会表现出过度悲观. 关于这类随机性场景的详细讨论, 我们将在课程后半部分讲解**马尔可夫决策过程**时展开.

这种随机性可以通过极小极大算法的泛化形式--**期望最大算法**来建模. 期望最大算法在博弈树中引入了机会节点 (chance nodes), 这些节点不再像极小化节点那样考虑最坏情况, 而是计算平均情况下的**期望效用**. 具体而言:

- 极小化节点计算子节点的最小效用值
- 机会节点计算期望效用或期望值

节点价值计算规则如下:

- $\forall$ 智能体控制的状态: $V(s)=\max_{s^{\prime}\in\text{后继状态}(s)} V\left(s^{\prime}\right)$
- $\forall$ 机会状态: $V(s)=\sum_{s^{\prime}\in 后继状态(s)}p(s^{\prime}|s)V(s^{\prime})$
- $\forall$ 终止状态: $V(s)=$ 已知值

其中$p\left(s^{\prime}\mid s\right)$表示:

1. 非确定性动作导致状态从s转移到s'的概率
2. 对手选择动作导致状态转移的概率

由此可见, 极小极大算法只是期望最大算法的特例--当机会节点给最低价值子节点分配概率1, 其他子节点概率0时, 两者等价. 通常这些概率由游戏本身的特性决定.

期望最大算法的伪代码与极小极大算法非常相似, 仅需将极小化节点替换为计算期望效用的机会节点:

![图1: 期望最大算法伪代码](../0_Attachment/Pasted%20image%2020250516173838.png)

举例说明: 假设某期望最大树中所有机会节点的子节点出现概率均为1/3.

![图2: 未填充的期望最大算法](../0_Attachment/Pasted%20image%2020250516174015.png)

根据价值计算规则, 从左至右三个机会节点的值分别为:

-  $\frac{1}{3}\cdot 3+\frac{1}{3}\cdot 12+\frac{1}{3}\cdot 9=\boxed{8}$
-  $\frac{1}{3}\cdot 2+\frac{1}{3}\cdot 4+\frac{1}{3}\cdot 6=\boxed{4}$
-  $\frac{1}{3}\cdot 15+\frac{1}{3}\cdot 6+\frac{1}{3}\cdot 0=\boxed{7}$

 最大化节点选择这三个值中的最大值.

![图3: 已填充的期望最大算法](../0_Attachment/Pasted%20image%2020250516174023.png)

重要提示: 期望最大算法通常需要遍历机会节点的所有子节点, 不能像极小极大算法那样进行剪枝. 因为单个极端值可能显著影响期望值的计算结果. 不过当节点值存在已知有限边界时, 剪枝仍有可能实现.

#### 3.3.1 混合节点类型

虽然极小极大和期望最大算法分别要求交替出现最大化/极小化节点和最大化/机会节点, 但许多游戏并不严格遵循这种交替模式. 以吃豆人游戏为例:

- 一个吃豆人 (最大化层) 移动后, 通常有多个幽灵 (多个极小化层) 轮流移动
- 若存在随机行为的幽灵, 可设计为"最大化-机会-极小化"的混合层次

![图4: 混合层](../0_Attachment/Pasted%20image%2020250516174317.png)

这种灵活的层次设计允许我们针对任何零和博弈, 构建结合期望最大与极小极大特性的混合博弈树.

### 3.4 通用博弈模型

并非所有博弈都是零和的. 当不同智能体在博弈中具有不直接冲突的独立目标时, 可以使用多智能体效用元组来建模:

- 每个节点价值表示为效用元组$(u_1,u_2,...)$
- 每个智能体在自身决策层最大化对应的效用分量

例如某博弈树中:

![图1: 多智能体效用](../0_Attachment/Pasted%20image%2020250515170755.png)

- 红色节点最大化红色效用分量
- 绿色节点最大化绿色分量
- 蓝色节点最大化蓝色分量

最终根节点效用元组为(5,2,5). 这种设计通过计算自然产生协作行为, 因为最终选择的解通常会为所有参与方提供合理效用.

### 3.5 蒙特卡洛树搜索 (MCTS) 

对于围棋等高分支因子的场景, 极小极大算法不再适用. **MCTS算法**基于两个核心理念:

1. **rollout 评估**: 从状态s出发, 根据某种策略 (如随机策略) 进行多次完整对局, 统计胜负次数
2. **选择性搜索**: 不受深度限制地探索可能改进根节点决策的子树部分

算法流程示例:

![图1: MCTS 示例1](../0_Attachment/Pasted%20image%2020250516174430.png)

1. 对当前状态的三个可选动作 (左/中/右) 各模拟100次
2. 记录每个动作的胜率百分比
3. 若某个动作早期表现明显较差, 可将剩余模拟次数分配给其他动作

示例: 将中间动作的剩余模拟分配给左侧和右侧动作.

![图2: MCTS 示例2](../0_Attachment/Pasted%20image%2020250516174623.png)

一个有趣的情况是, 某些动作产生的获胜百分比相似, 但其中一个动作使用的模拟次数要少得多, 如下图所示. 在这种情况下, 使用较少模拟的动作的估计值将具有更高的方差, 因此我们可能需要为该作分配更多模拟, 以便对真实的获胜百分比更有信心.

![图3: MCTS 示例1](../0_Attachment/Pasted%20image%2020250516175124.png)

**UCB1算法**通过以下公式平衡"潜力"与"不确定性":
$$
UCB1(n)=\frac{U(n)}{N(n)}+C\times\sqrt{\frac{\log N(PARENT(n))}{N(n)}}
$$

 其中:

- $N(n)$: 节点n的模拟总次数
- $U(n)$: 父节点玩家 $PARENT(n)$ 的胜利次数
- $C$: 用户指定的探索系数

**MCTS UCT算法**在树搜索问题中使用 UCB 标准, 会重复执行以下三步:

1. 使用UCB准则从根节点向下遍历至未扩展的叶节点
2. 为该叶节点添加新子节点, 并通过rollout确定其胜率
3. 反向传播更新从子节点到根节点的统计信息

当模拟次数$N\rightarrow\infty$时, UCT 的行为会趋近于极小极大智能体.

### 3.6 总结

本章从标准路径搜索问题转向对抗性搜索问题, 主要算法包括:

- **极小极大算法**: 适用于最优对手, 可通过 $\alpha-\beta$ 剪枝优化. 在未知对手情况下表现更保守稳健
- **期望最大算法**: 适用于次优对手, 通过概率分布计算期望价值

由于完全展开博弈树计算量过大, 我们引入了评估函数进行早期终止. 针对高分支因子问题, 提出了可并行化的 MCTS 和 UCT 算法. 最后讨论了非零和博弈的通用建模方法.

## 4. 马尔可夫决策过程 (MDPs) 

### 4.1 马尔可夫决策过程

马尔可夫决策过程由以下属性定义:

- **状态集合** $S$: MDP 中的状态表示方式与传统搜索问题相同
- **动作集合** $A$: 动作表示方式也与传统搜索问题相同
- **初始状态**
- **可能的终止状态 (一个或多个) **
- **可能的折扣因子** $\gamma$ (后文详述) 
- **转移函数** $T(s,a,s')$: 由于引入了非确定性动作, 需要量化从状态 $s$ 采取动作 $a$ 后到达 $s'$ 的概率. 转移函数即为此概率分布: $T(s,a,s') = P(s' \mid s,a)$
- **奖励函数** $R(s,a,s')$: 通常包含每步的“存活”奖励和到达终止状态的大额奖励. 奖励可正可负, 智能体的目标是最大化累计奖励

为某种情境构建马尔可夫决策过程 (MDP) 与为搜索问题构建状态空间图非常相似, 但需要注意几个额外的要点. 以赛车激励示例为例:

![图1: 赛车示例](../0_Attachment/Pasted%20image%2020250517105659.png)

- 状态集合 $S = \{\text{cool, warm, overheated}\}$
- 动作集合 $A = \{\text{slow, fast}\}$
- 与状态空间图类似, 这三个状态分别由节点表示, 而边代表动作. **Overheated (过热) ​**​ 是一个终止状态, 因为一旦赛车智能体进入该状态, 就无法再执行任何动作以获取进一步奖励 (它是 MDP 中的吸收状态, 没有出边)
- 值得注意的是, 对于**非确定性动作**, 同一状态下的同一动作可能对应多条边, 每条边指向不同的后继状态. 每条边不仅标注了对应的动作, 还标注了**转移概率**和**相应的奖励**. 具体如下:
- **转移函数**:
  - $T(\text{cool}, \text{slow}, \text{cool}) = 1$
  - $T(\text{warm}, \text{slow}, \text{cool}) = 0.5$
  - $T(\text{warm}, \text{slow}, \text{warm}) = 0.5$
  - $T(\text{cool}, \text{fast}, \text{cool}) = 0.5$
  - $T(\text{cool}, \text{fast}, \text{warm}) = 0.5$
  - $T(\text{warm}, \text{fast}, \text{overheated}) = 1$
- **奖励函数**:
  - $R(\text{cool}, \text{slow}, \text{cool}) = 1$
  - $R(\text{warm}, \text{slow}, \text{cool}) = 1$
  - $R(\text{warm}, \text{slow}, \text{warm}) = 1$
  - $R(\text{cool}, \text{fast}, \text{cool}) = 2$
  - $R(\text{cool}, \text{fast}, \text{warm}) = 2$
  - $R(\text{warm}, \text{fast}, \text{overheated}) = -10$

我们通过离散**时间步长**来表示智能体在不同 MDP 状态间的转移过程, 其中 $s_t \in S$ 表示智能体在时间步 $t$ 时所处的状态, $a_t \in A$​ 表示智能体在时间步 ​**t**​ 时采取的动作. 智能体从时间步 ​0​ 的初始状态 $s_0$​ 开始, 并在每个时间步执行一个动作. 因此, 智能体在 MDP 中的转移过程可以建模如下:
$$
s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} \cdots
$$
此外, 由于智能体的目标是在所有时间步上最大化其累积奖励, 我们可以用数学方式将其表达为以下效用函数的最大化:
$$
U([s_0,a_0,s_1,...]) = \sum_{t=0}^\infty R(s_t,a_t,s_{t+1})
$$
马尔可夫决策过程与状态空间图类似, 都可以展开为搜索树. 在这些搜索树中, 不确定性通过 **Q 状态** (又称**动作状态**) 进行建模, 其本质与期望最大化的机会节点完全相同. 这种建模方式非常恰当, 因为 Q 状态利用概率来模拟环境将智能体置于特定状态的不确定性, 正如期望最大化机会节点通过概率来模拟敌对智能体通过其选择的动作将我方智能体置于特定状态的不确定性. 从状态s采取动作a所形成的Q状态, 可用元组 $(s, a)$ 表示.

*DS V3: MDP可展开为包含**Q 状态 (动作状态) **的搜索树, Q 状态 $(s,a)$ 表示从 $s$ 执行 $a$ 后的不确定性分支 (类似 expectimax 的几率节点)*

观察我们赛车展开的搜索树 (截取至深度为2的层级):

![图2: 赛车搜索树](../0_Attachment/Pasted%20image%2020250517110919.png)

绿色节点代表Q状态 (动作状态), 即已从某个状态执行了动作, 但尚未确定后续状态. 需要注意的是, 智能体在Q状态中**不消耗任何时间步**, 它们本质上只是为方便表示和开发MDP算法而构建的抽象概念.

#### 4.1.1 有限时域与折扣因子

**问题**: 我们的赛车 MDP 存在一个固有缺陷--我们未对赛车可执行动作并获取奖励的时间步数设置任何限制. 按照当前设定, 赛车可能永远在每个时间步都选择 $a = \text{slow}$, 安全且高效地获得无限奖励, 同时完全规避过热风险.

**解决方案**

1. **有限时域**: 设定智能体的“寿命”时间步数$n$.

2. **折扣因子** $\gamma$: 奖励随时间指数衰减, 时间步$t$的奖励变为$\gamma^t R(s_t,a_t,s_{t+1})$. 折扣效用函数为:
   $$
   U([s_0,a_0,s_1,...]) = R(s_0,a_0,s_1) + \gamma R(s_1,a_1,s_2) + \gamma^2 R(s_2,a_2,s_3) + \ldots
   $$
   当$|\gamma|<1$时, 总效用有限:
   $$
   U \leq \sum_{t=0}^{\infty} \gamma^t R_{\text{max}} = \frac{R_{\text{max}}}{1-\gamma}
   $$
   通常选择$0 < \gamma < 1$ (负值无实际意义)

#### 4.1.2 马尔可夫性

MDP满足**马尔可夫性质** (无记忆性): 未来仅依赖当前状态, 与过去无关. 数学表达:
$$
P(S_{t+1}=s_{t+1} \mid S_t=s_t, A_t=a_t, \ldots, S_0=s_0) = P(S_{t+1}=s_{t+1} \mid S_t=s_t, A_t=a_t)  
$$
转移函数即为此概率:
$$
\boxed{T(s,a,s') = P(s' \mid s,a)}
$$

### 4.2 求解马尔可夫决策过程

求解 MDP 即找到**最优策略** $\pi^*: S \rightarrow A$, 该策略使智能体获得最大期望效用.

示例 MDP 分析:

![图1: 简单 MDP](../0_Attachment/Pasted%20image%2020250517113252.png)

- 状态: $\{a,b,c,d,e\}$
- 动作: $\{\text{East}, \text{West}, \text{Exit}\}$
- 折扣因子 $\gamma=0.1$

两种可能策略如下:

![图2: 两种可能策略](../0_Attachment/Pasted%20image%2020250517113514.png)

比较两种策略后发现 **Policy 2** 更优.

#### 4.2.1 贝尔曼方程

定义两个核心量:

1. **状态最优值** $V^*(s)$: 从$s$出发的最优期望效用
2. **Q状态最优值** $Q^*(s,a)$: 从$(s,a)$出发的最优期望效用

贝尔曼方程定义:
$$
V^*(s) = \max_a \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^*(s') \right]  
$$
Q值定义:
$$
Q^*(s,a) = \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^*(s') \right]  
$$
两者关系:
$$
V^*(s) = \max_a Q^*(s,a)
$$
贝尔曼方程是动态规划方程, 通过递归分解问题. $[R(s,a,s') + \gamma V^*(s')]$表示即时奖励加上后续最优折扣效用. $Q^*$是期望效用加权和, $V^*$是最大期望效用 (类似期望最大化算法).

满足贝尔曼方程的值即为最优值:
$$
\forall s \in S, \, V(s) = V^*(s)
$$

### 4.3 值迭代

既然我们已经有了一个检验MDP中状态值最优性的框架, 接下来自然要问: 如何实际计算这些最优值? 为此, 我们需要引入时间限制值 (这是施加有限时间步长的自然结果). 状态s在时间步长限制为k时的限时值记作$V_{k}(s)$, 表示从s出发在k个时间步内终止的MDP中可获取的最大期望效用. 等价地说, 这就是在MDP搜索树上运行深度为k的期望最大化算法返回的结果.

值迭代是一种动态规划算法, 它通过逐步延长时限来计算限时值, 直到收敛 (即每个状态的V值与前一次迭代相同: $\forall s, V_{k+1}(s)=V_{k}(s)$). 其操作步骤如下:

1. $\forall s\in S$, 初始化$V_{0}(s)=0$. 这很直观, 因为将时间限制设为0意味着在终止前无法采取任何动作, 因此无法获得任何奖励.
2. 重复以下更新规则直至收敛:
   $\forall s\in S,\,V_{k+1}(s)\leftarrow\max_{a}\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V_{k}(s^{\prime})]$
   在值迭代的第k次迭代中, 我们使用每个状态的k步限时值来生成(k+1)步限时值. 本质上, 我们利用子问题的解 (所有$V_k(s)$) 迭代构建更大子问题的解 (所有$V_{k+1}(s)$) ; 这正是使值迭代成为动态规划算法的原因.

注意, 虽然贝尔曼方程看起来与上述更新规则几乎相同, 但它们并不相同. 贝尔曼方程给出了最优性的条件, 而更新规则给出了一种迭代更新值直至收敛的方法. 当达到收敛时, 贝尔曼方程对每个状态都成立: $\forall s\in S,\,V_{k}(s)=V_{k+1}(s)=V^{*}(s)$.

为简洁起见, 我们经常用简写 $V_{k+1}\leftarrow B U_{k}$ 表示 $U_{k+1}(s)\leftarrow\max_{a}\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V_{k}\left(s^{\prime}\right)\right]$, 其中 B 称为贝尔曼算子. 贝尔曼算子是$\gamma$的**压缩映射**. 为证明这一点, 我们需要以下一般不等式: *(没看懂什么是压缩映射)*

$|\max_{z}f(z)-\max_{z}h(z)|\leq\max_{z}|f(z)-h(z)|.$

现在考虑在相同状态下评估的两个值函数$V(s)$和$V^{\prime}(s)$. 我们通过以下方式证明贝尔曼更新B是关于最大范数的$\gamma\in(0,1)$压缩映射:
$$
\begin{aligned}&|BV(s)-BV^{\prime}(s)|\\ &=\left|\left(\max_{a}\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V(s^{\prime})]\right)-\left(\max_{a}\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{\prime}(s^{\prime})]\right)\right|\\ &\leq\max_{a}\left|\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V(s^{\prime})]-\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{\prime}(s^{\prime})]\right|\\ &=\max_{a}\left|\gamma\sum_{s^{\prime}}T(s,a,s^{\prime})V(s^{\prime})-\gamma\sum_{s^{\prime}}T(s,a,s^{\prime})V^{\prime}(s^{\prime})\right|\\ &=\gamma\max_{a}\left|\sum_{s^{\prime}}T(s,a,s^{\prime})(V(s^{\prime})-V^{\prime}(s^{\prime}))\right|\\ &\leq\gamma\max_{a}\left|\sum_{s^{\prime}}T(s,a,s^{\prime})\max_{s^{\prime}}|V(s^{\prime})-V^{\prime}(s^{\prime})|\right|\\ &=\gamma\max_{s^{\prime}}|V(s^{\prime})-V^{\prime}(s^{\prime})|\\ &=\gamma||V(s^{\prime})-V^{\prime}(s^{\prime})||_{\infty},\end{aligned}
$$
其中第一个不等式来自上述一般不等式, 第二个不等式来自取V和$V^{\prime}$之间差值的最大值, 在倒数第二步中, 我们利用了无论选择哪个动作a概率总和为1的性质. 最后一步使用了向量 $x=(x_{1},\ldots,x_{n})$的最大范数定义, 即 $||x||_{\infty}=\max(|x_{1}|,\ldots,|x_{n}|).$

由于我们刚刚证明了通过贝尔曼更新的值迭代是$\gamma$的压缩映射, 我们知道值迭代会收敛, 且当达到满足$V^{*}=BU^{*}$的不动点时收敛完成. 让我们通过重新审视之前的赛车MDP实例, 引入折扣因子$\gamma=0.5$, 来看几个值迭代更新的实际操作:

![图1: 赛车 MDP](../0_Attachment/Pasted%20image%2020250517140907.png)

我们通过初始化所有$V_0(s)=0$开始值迭代:

|       | cool | warm | overheated |
| ----- | ---- | ---- | ---------- |
| $V_0$ | 0    | 0    | 0          |

在第一轮更新中, 我们可以如下计算$\forall s\in S,~{}V_{1}(s)$:
$$
\begin{aligned} V_{1}(cool)&=\max\{1\cdot[1+0.5\cdot 0],\quad 0.5\cdot[2+0.5\cdot 0]+0.5\cdot[2+0.5\cdot 0]\}\\ &=\max\{1,2\}\\ &=\boxed{2}\\ V_{1}(warm)&=\max\{0.5\cdot[1+0.5\cdot 0]+0.5\cdot[1+0.5\cdot 0],\quad 1\cdot[-10+0.5\cdot 0]\}\\ &=\max\{1,-10\}\\ &=\boxed{1}\\ V_{1}(overheated)&=\max\{\}\\ &=\boxed{0}\end{aligned}
$$

|       | cool | warm | overheated |
| ----- | ---- | ---- | ---------- |
| $U_0$ | 0    | 0    | 0          |
| $U_1$ | 2    | 1    | 0          |

类似地, 我们可以重复该过程, 利用新发现的$U_1(s)$值计算第二轮更新$V_2(s)$:
$$
\begin{aligned} V_{2}(cool)&=\max\{1\cdot[1+0.5\cdot 2],\,0.5\cdot[2+0.5\cdot 2]+0.5\cdot[2+0.5\cdot 1]\}\\ &=\max\{2,2.75\}\\ &=\boxed{2.75}\\ V_{2}(warm)&=\max\{0.5\cdot[1+0.5\cdot 2]+0.5\cdot[1+0.5\cdot 1],\,1\cdot[-10+0.5\cdot 0]\}\\ &=\max\{1.75,-10\}\\ &=\boxed{1.75}\\ V_{2}(overheated)&=\max\{\}\\ &=\boxed{0}\end{aligned}
$$

|       | cool | warm | overheated |
| ----- | ---- | ---- | ---------- |
| $V_0$ | 0    | 0    | 0          |
| $V_1$ | 2    | 1    | 0          |
| $V_2$ | 2.75 | 1.75 | 0          |

值得注意的是, 任何终止状态的$V^*(s)$必须为0, 因为从终止状态无法采取任何动作来获取奖励.

#### 4.3.1 策略提取

回想一下, 我们解决MDP的最终目标是确定一个最优策略. 一旦使用称为**策略提取**的方法确定了所有状态的最优值, 就可以做到这一点. 策略提取背后的直觉非常简单: 如果你处于状态s, 你应该采取产生最大期望效用的动作a. 毫不奇怪, a是将我们带到具有最大Q值的Q状态的动作, 从而允许最优策略的正式定义:
$$
\forall s\in S,\,\pi^{*}(s)=\underset{a}{argmax}\,Q^{*}(s,a)=\underset{a}{argmax}\,\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]
$$
出于性能原因, 记住策略提取最好具有状态的最优Q值是有用的, 在这种情况下, 只需要一个argmax操作就可以确定状态的最优动作. 仅存储每个$V^{*}(s)$意味着我们必须在使用argmax之前用贝尔曼方程重新计算所有必要的Q值, 相当于执行深度为1的期望最大化.

#### 4.3.2 Q值迭代

在使用值迭代求解最优策略时, 我们首先找到所有最优值, 然后使用策略提取提取策略. 然而, 你可能已经注意到, 我们还处理了另一种编码最优策略信息的值: Q值.

Q值迭代是一种计算有限时域Q值的动态规划算法. 它由以下方程描述:
$$
Q_{k+1}(s,a)\leftarrow\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q_{k}(s^{\prime},a^{\prime})]
$$
注意, 这个更新规则与值迭代的更新规则只有轻微修改. 实际上, 唯一的真正区别是动作上的max操作符的位置发生了变化, 因为我们在状态中选择动作后才转换, 但在Q状态中转换后才选择新动作. 一旦我们有了每个状态和动作的最优Q值, 我们就可以通过简单地选择具有最高Q值的动作为状态找到策略.

### 4.4 策略迭代

值迭代可能相当慢. 在每次迭代中, 我们必须更新所有$|S|$个状态的值 (其中$|n|$表示基数操作符) , 每个状态都需要迭代所有$|A|$个动作, 因为我们计算每个动作的Q值. 这些Q值的计算又需要再次迭代每个$|S|$个状态, 导致时间复杂度较差, 为$O\left(|S|^{2}|A|\right)$. 此外, 当我们只想确定MDP的最优策略时, 值迭代往往会进行大量过度计算, 因为通过策略提取计算的策略通常比状态值本身收敛得快得多. 解决这些缺陷的方法是使用**策略迭代**作为替代方案, 这是一种在保持值迭代最优性的同时提供显著性能提升的算法. 策略迭代操作如下:

1. 定义一个初始策略. 这可以是任意的, 但初始策略越接近最终最优策略, 策略迭代收敛得越快.
2. 重复以下步骤直至收敛:

- 使用**策略评估**评估当前策略. 对于策略$\pi$, 策略评估意味着计算所有状态s的$V^{\pi}(s)$, 其中$V^{\pi}(s)$是遵循$\pi$时从状态s开始的期望效用: $V^{\pi}(s)=\sum_{s^{\prime}}T(s,\pi(s), s^{\prime})\left[R\left(s,\pi(s), s^{\prime}\right)+\gamma V^{\pi}\left(s^{\prime}\right)\right]$. 将策略迭代的第i次迭代的策略定义为 $\pi_{i}$. 由于我们为每个状态固定了一个单一动作, 我们不再需要 max 操作符, 这实际上给我们留下了由上述规则生成的$|S|$个方程的系统 (方程变量只有 $V^{\pi}(s_1), V^{\pi}(s_2) ...$). 然后可以通过简单地解这个系统来计算每个 $V^{\pi_{i}}(s)$. 或者, 我们也可以像值迭代中那样, 使用以下更新规则直至收敛来计算 $V^{\pi_{i}}(s)$: $V_{k+1}^{\pi_{i}}(s)\leftarrow\sum_{s^{\prime}}T(s,\pi_{i}(s),s^{\prime})[R(s,\pi_{i}(s),s^{\prime})+\gamma V_{k}^{\pi_{i}}(s^{\prime})]$. 然而, 第二种方法在实践中通常较慢.
- 一旦我们评估了当前策略, 使用**策略改进**生成一个更好的策略. 策略改进利用策略评估生成的状态值上的策略提取来生成这个新的改进策略: $\pi_{i+1}(s)=\underset{a}{\operatorname{argmax}}\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V^{\pi_{i}}\left(s^{\prime}\right)\right]$. 如果$\pi_{i+1}=\pi_{i}$, 算法已经收敛, 我们可以得出结论 $\pi_{i+1}=\pi_{i}=\pi^{*}$.

让我们最后一次通过我们的赛车例子 (已经厌倦了吗?) 来看看我们是否使用策略迭代得到与值迭代相同的策略. 回想一下, 我们使用的是折扣因子 $\gamma=0.5$. 我们以初始策略"总是慢速"开始:

![图1: 赛车例子](../0_Attachment/Pasted%20image%2020250517140907.png)

|           | cool | warm | overheated |
| --------- | ---- | ---- | ---------- |
| $\pi_{0}$ | slow | slow | —          |

由于终止状态没有输出动作, 任何策略都不能为其赋值. 因此, 可以合理地不考虑过热状态, 并简单地为任何终止状态s分配$\forall i, V^{\pi_{i}}(s)=0$. 下一步是在$\pi_{0}$上运行一轮策略评估:
$$
\begin{aligned}V^{\pi_{0}}(\text{ cool})&=1\cdot\left[1+0.5\cdot V^{\pi_{0}}(\text{ cool})\right]\\ V^{\pi_{0}}(\text{ warm})&=0.5\cdot\left[1+0.5\cdot V^{\pi_{0}}(\text{ cool})\right]\\ &+0.5\cdot\left[1+0.5\cdot V^{\pi_{0}}(\text{ warm})\right]\end{aligned}
$$
解这个关于$V^{\pi_{0}}(cool)$和$V^{\pi_{0}}(warm)$的方程组得到:

|               | cool | warm | overheated |
| ------------- | ---- | ---- | ---------- |
| $V^{\pi_{0}}$ | 2    | 2    | 0          |

我们现在可以使用这些值进行策略提取:
$$
\begin{aligned}\pi_{1}(cool)&=\operatorname*{argmax}\{slow:1\cdot[1+0.5\cdot 2],\,fast:0.5\cdot[2+0.5\cdot 2]+0.5\cdot[2+0.5\cdot 2]\}\\ &=\operatorname*{argmax}\{slow:2,\,fast:3\}\\ &=\boxed{fast}\\ \pi_{1}(warm)&=\operatorname*{argmax}\{slow:0.5\cdot[1+0.5\cdot 2]+0.5\cdot[1+0.5\cdot 2],\,fast:1\cdot[-10+0.5\cdot 0]\}\\ &=\operatorname*{argmax}\{slow:2,\,fast:-10\}\\ &=\boxed{slow}\end{aligned}
$$
运行第二轮策略迭代得到 $\pi_{2}(cool)=fast$和$\pi_{2}(warm)=slow.$ 由于这与$\pi_{1}$相同, 我们可以得出结论$\pi_{1}=\pi_{2}=\pi^{*}.$ 验证这一点作为练习!

|           | cool | warm |
| --------- | ---- | ---- |
| $\pi_{0}$ | slow | slow |
| $\pi_{1}$ | fast | slow |
| $\pi_{2}$ | fast | slow |

这个例子展示了策略迭代的真正威力: 仅通过两次迭代, 我们已经得出了赛车MDP的最优策略! 这比我们在同一MDP上运行值迭代时的情况要多, 后者在我们执行的两个更新后仍需要多次迭代才能收敛.

### 4.5 总结

上述材料有很多可能引起混淆的地方. 我们涵盖了值迭代, 策略迭代, 策略提取和策略评估, 所有这些看起来都很相似, 使用了贝尔曼方程, 但有细微的变化.

以下是每种算法目的的总结:

● **值迭代**: 用于通过迭代更新直至收敛来计算状态的最优值.

● **策略评估**: 用于计算特定策略下状态的值.

● **策略提取**: 用于在给定某些状态值函数的情况下确定策略. 如果状态值是最优的, 则该策略将是最优的. 此方法在运行值迭代以从最优状态值计算最优策略后使用, 或作为策略迭代中的子程序以计算当前估计状态值的最佳策略.

● **策略迭代**: 一种封装了策略评估和策略提取的技术, 用于迭代收敛到最优策略. 由于策略通常比状态值收敛得快得多, 它往往优于值迭代.

*PPT补充*

- **值迭代**和**策略迭代**计算的是相同的内容 (所有最优值) 
- **在值迭代中**:
  
  - 每次迭代都会同时更新状态值, 并 (隐式地) 更新策略
  - 我们不显式追踪策略, 而是通过对动作取最大值来隐式重新计算策略
- **在策略迭代中**: 先进行多轮策略评估 (每轮评估很快, 因为只需考虑固定策略下的单个动作) 
  - 策略评估完成后, 选择新策略 (此步骤类似值迭代过程, 速度较慢) 
  - 新策略必然更优 (否则算法终止)

## 5. 强化学习 (RL)

### 5.1 强化学习

在前文中, 我们讨论了马尔可夫决策过程 (MDP), 并通过值迭代和策略迭代等技术求解最优状态值和提取最优策略. 求解MDP属于**离线规划**的范畴--智能体完全掌握**状态转移**函数和**奖励**函数的所有信息, 无需实际执行任何动作即可预先计算出 MDP 编码世界中的最优行动方案.

本节将讨论**在线规划**的场景: 智能体对世界的奖励和转移函数没有先验知识 (仍以MDP表示). 在在线规划中, 智能体必须进行**探索**--通过执行动作并接收**反馈** (包括转移到的后继状态和相应奖励), 利用这些反馈通过**强化学习**过程来估计最优策略, 最终基于该策略进行利用以实现奖励最大化.

![图1: 反馈循环](../0_Attachment/Pasted%20image%2020250517160439.png)

首先介绍基本术语. 在线规划的每个时间步中, 智能体从状态$s$出发, 执行动作$a$后转移到后继状态$s^{\prime}$并获得奖励$r$. 每个 $(s,a,s^{\prime},r)$ 元组称为一个**样本**. 通常智能体会持续执行动作并连续收集样本, 直到到达终止状态. 这样的样本序列称为一个**回合**. 在探索阶段, 智能体通常需要经历多个回合以收集足够的学习数据.

强化学习分为两类: **基于模型的学习**和**无模型学习**. 基于模型的学习试图利用探索获得的样本来估计状态转移函数和奖励函数, 然后通过常规的值迭代或策略迭代求解MDP; 而无模型学习则直接估计状态值或Q值, 无需构建MDP中奖励和转移函数的显式模型.

### 5.2 基于模型的学习

在基于模型学习中, 智能体通过统计每个Q状态$(s,a)$下转移到各状态$s^{\prime}$的次数, 构建转移函数的近似表示$\hat{T}\left(s, a, s^{\prime}\right)$. 具体方法是将观测到的元组$\left(s, a, s^{\prime}\right)$计数除以该Q状态$(s,a)$出现的总次数进行**归一化**, 使结果具有概率意义.

考虑以下MDP示例: 状态集$S=\{A, B, C, D, E, x\}$ (其中$x$为终止状态) , 折扣因子$\gamma=1$:

![图1: MDP 示例](../0_Attachment/Pasted%20image%2020250517161123.png)

假设智能体按图示探索策略$\pi_{\text{explore}}$ (方向三角形表示指向的动作, 蓝色方块表示选择exit动作) 进行4个回合的探索, 得到以下结果:

![图2: 示例回合](../0_Attachment/Pasted%20image%2020250517161113.png)

现共收集12个样本 (每回合3个), 统计如下:

| s    | a     | s'   | count |
| ---- | ----- | ---- | ----- |
| A    | exit  | x    | 1     |
| B    | east  | C    | 2     |
| C    | east  | A    | 1     |
| C    | east  | D    | 3     |
| D    | exit  | x    | 3     |
| E    | north | C    | 2     |

根据$T\left(s, a, s^{\prime}\right)=P\left(s^{\prime}\mid a, s\right)$的定义, 可通过计数归一化估计转移函数, 并直接从探索获得的奖励构建奖励函数:

**转移函数**: $\hat{T}(s,a,s^{\prime})$
$$
\begin{aligned}
\hat{T}(A,exit, x)&=\frac{\#(A,exit, x)}{\#(A,exit)}=\frac{1}{1}= 1\\ 
\hat{T}(B,east, C)&=\frac{\#(B,east, C)}{\#(B,east)}=\frac{2}{2}= 1\\ 
\hat{T}(C,east, A)&=\frac{\#(C,east, A)}{\#(C,east)}=\frac{1}{4}= 0.25\\ 
\hat{T}(C,east, D)&=\frac{\#(C,east, D)}{\#(C,east)}=\frac{3}{4}= 0.75\\ 
\hat{T}(D,exit, x)&=\frac{\#(D,exit, x)}{\#(D,exit)}=\frac{3}{3}= 1\\ 
\hat{T}(E,north, C)&=\frac{\#(E,north, C)}{\#(E,north)}=\frac{2}{2}= 1
\end{aligned}
$$
**奖励函数**: $\hat{R}(s,a,s^{\prime})$
$$
\begin{aligned}
\hat{R}(A,exit, x)&=-10\\ 
\hat{R}(B,east, C)&=-1\\ 
\hat{R}(C,east, A)&=-1\\ 
\hat{R}(C,east, D)&=-1\\ 
\hat{R}(D,exit, x)&=+10\\ 
\hat{R}(E,north, C)&=-1
\end{aligned}
$$
根据**大数定律**, 随着探索回合增加, $\hat{T}$将收敛于真实$T$, $\hat{R}$也会通过新发现的$(s,a,s')$元组不断完善. 当训练达到预期时, 可终止探索阶段, 利用当前的$\hat{T}$和$\hat{R}$通过值迭代或策略迭代生成利用策略$\pi_{exploit}$, 使智能体转向以奖励最大化为目标的行动模式.

后文将讨论探索与利用时间的分配策略. 基于模型的学习虽然简单直观且高效 (仅需计数和归一化), 但维护所有$(s,a,s')$元组的计数可能带来存储开销. 下一节将介绍无模型学习方法, 通过避免显式计数来消除这种内存负担.

### 5.3 无模型学习

现在进入无模型学习! 存在多种无模型学习算法, 我们将介绍其中三种: 直接评估, 时序差分学习和Q学习. 直接评估和时序差分学习属于**被动强化学习**算法类别. 在被动强化学习中, 智能体遵循给定策略, 通过经历多个回合来学习该策略下各状态的价值--这与已知转移函数$T$和奖励函数$R$时MDP的策略评估过程完全一致. Q学习则属于第二类无模型学习算法, 称为**主动强化学习**, 学习过程中智能体可利用反馈持续更新策略, 最终通过充分探索得到最优策略.

#### 5.3.1 直接评估

第一种被动强化学习技术是**直接评估**, 其名称已暗示了它的简单直接. 该方法固定某个策略$\pi$, 让智能体遵循$\pi$经历多个回合. 智能体通过回合收集样本时, 会记录从各状态获得的总效用及访问次数. 任意时刻, 状态$s$的价值估计可通过总效用除以访问次数得到. 假设折扣因子$\gamma=1$, 我们通过示例说明:

![图1: 示例图片](../0_Attachment/Pasted%20image%2020250517163151.png)

在第一个回合中, 从状态$D$到终止状态获得总奖励$10$, 从状态$C$获得$(-1)+10=9$, 从状态$B$获得$(-1)+(-1)+10=8$. 完成该过程后, 各状态的总奖励及价值估计如下:

| s    | 总奖励 | 访问次数 | $V^{\pi}(s)$ |
| :--- | :----- | -------- | ------------ |
| A    | $-10$  | 1        | -10          |
| B    | 16     | 2        | 8            |
| C    | 16     | 4        | 4            |
| D    | 30     | 3        | 10           |
| E    | $-4$   | 2        | $-2$         |

尽管直接评估最终能学习各状态价值, 但因忽略状态转移信息, 收敛速度通常过慢.

![图2: 带标注的示例](../0_Attachment/Pasted%20image%2020250517163233.png)

例如, $V^{\pi}(E)=-2$而$V^{\pi}(B)=8$, 但根据贝尔曼方程, 两者应有相同价值 (因后继状态均为$C$且转移奖励均为$-1$). 差异仅因偶然的采样偏差导致, 这种问题可通过时序差分学习缓解.

#### 5.3.2 时序差分学习

时序差分学习 (Temporal Difference, TD学习) 的核心思想是**从每次经验中学习**, 而非像直接评估那样仅记录总奖励和访问次数. 策略评估中, 我们利用固定策略和贝尔曼方程生成的方程组来确定状态价值 (或通过类似值迭代的更新方式):
$$
V^{\pi}(s)=\sum_{s^{\prime}}T(s,\pi(s),s^{\prime})[R(s,\pi(s),s^{\prime})+\gamma V^{\pi}(s^{\prime})]
$$
TD学习的关键在于**在无权重的情况下计算加权平均**, 通过指数移动平均实现. 初始化 $\forall s,\,V^{\pi}(s)=0$ 后, 每个时间步智能体从状态$s$执行动作 $\pi(s)$, 转移到 $s^{\prime}$ 并获得奖励 $R(s,\pi(s),s^{\prime})$. 样本值计算为:
$$
\text{sample}=R(s,\pi(s),s^{\prime})+\gamma V^{\pi}\left(s^{\prime}\right)
$$
随后通过以下规则将样本融入现有模型:
$$
V^{\pi}(s)\leftarrow(1-\alpha)V^{\pi}(s)+\alpha\cdot{\text{sample}}
$$
其中$\alpha$ (学习率) 控制新旧样本的权重比例. 初始可设$\alpha=1$, 随后逐步衰减至0. 展开更新规则可见:
$$
V_{k}^{\pi}(s)\leftarrow\alpha\cdot[(1-\alpha)^{k-1}\cdot \text{sample}_{1}+...+\text{sample}_{k}]
$$
旧样本因$(1-\alpha)^n$衰减而被指数级降权, 这正是TD学习的优势:

- 每步即时学习, 利用状态转移信息更新 $V^{\pi}(s)$ 而不是放到最后才计算
- 旧样本自动降权
- 比直接评估更快收敛

#### 5.3.3 Q学习

直接评估和TD学习最终能学习策略下的真实状态价值, 但存在根本缺陷: 最优策略需要Q值知识. 传统方法需已知转移函数 $T$ 和奖励函数 $R$ 才能通过贝尔曼方程计算Q值:
$$
Q^{*}(s,a)=\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]
$$
Q学习通过**直接学习Q值**跳过该限制, 完全无需模型. 其更新规则 (Q值迭代) 为:
$$
Q_{k+1}(s,a)\leftarrow\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q_{k}(s^{\prime},a^{\prime})]
$$
实际中通过采样实现 (动作已确定. $s'$ 为该次动作的样本):
$$
\text{sample}=R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})
$$
并融入指数移动平均:
$$
Q(s,a)\leftarrow(1-\alpha)Q(s,a)+\alpha\cdot\text{sample}
$$
只要我们投入足够的时间进行探索, 并以适当的速度降低学习率 $\alpha$, Q学习就能为每个Q状态找到最优的Q值. 这正是Q学习的革命性所在--时序差分学习 (TD学习) 和直接评估法需要先遵循策略来学习策略下状态的价值, 再通过其他技术判断策略的最优性; 而Q学习即使采取次优或随机行动, 也能直接学习到最优策略. 这种方法被称为**离策略学习** (与直接评估法和TD学习这类**同策略学习**形成对比).

Q学习是**离策略学习**的典范--即使采取次优动作也能学习最优策略, 这是其革命性所在.

#### 5.3.4 近似Q学习

Q学习是一种卓越的学习技术, 至今仍是强化学习领域发展的核心. 然而, 它仍有改进的空间. 目前, Q学习仅以表格形式存储所有状态的 Q 值, 但考虑到强化学习的大多数应用场景涉及数千甚至数百万个状态, 这种存储方式效率并不高. 这意味着在训练过程中我们无法遍历所有状态, 而且即便能够遍历, 也会因内存限制而无法存储所有 Q 值.

![图3.1](../0_Attachment/Pasted%20image%2020250517162911.png)
![图3.2](../0_Attachment/Pasted%20image%2020250517162915.png)
![图3.3](../0_Attachment/Pasted%20image%2020250517162917.png)

如图, 如果Pacman在运行基础Q学习后发现图3.1的情况不利, 它仍然不会意识到图3.2甚至图3.3的情况同样不利. **近似Q学习**试图通过掌握少量通用场景并推演至众多相似情境来解决这一问题. 实现学习经验泛化的关键在于**基于特征的状态表示法**--该方法将每个状态表征为一个被称为**特征向量**的多维数组. 例如, Pacman的特征向量可能编码为:

- 最近幽灵距离
- 最近食物距离
- 幽灵数量
- 是否被困 (0/1) 

此时状态价值和Q值表示为**线性函数**:
$$
V(s)=\vec{w}\cdot\vec{f}(s)=\sum_{i}w_{i}f_{i}(s) \\
Q(s,a)=\vec{w}\cdot\vec{f}(s,a)=\sum_{i}w_{i}f_{i}(s,a)
$$
定义**差异值**:
$$
\text{difference}=\left[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\right]-Q(s,a)
$$
权重更新规则为:
$$
w_{i}\leftarrow w_{i}+\alpha\cdot\text{difference}\cdot f_{i}(s,a)
$$
不同于为每个状态单独存储 Q 值, 近似Q学习只需存储一个权重向量, 即可按需实时计算 Q 值. 这不仅使Q学习具备了更强的泛化能力, 还大幅提升了内存使用效率.

关于Q学习的最后一点说明是, 我们可以用**差值**的形式重新表述精确Q学习的更新规则如下:
$$
Q(s,a)\leftarrow Q(s,a)+\alpha\cdot\text{difference}
$$
这第二种表示方式为我们提供了对更新规则略有不同但同样有价值的解读: 它计算采样估计值与当前 $Q(s,a)$ 模型之间的差值, 并按差值大小成比例地调整模型, 使其向估计值方向偏移. 

### 5.4 探索与利用

目前我们已经介绍了多种智能体学习最优策略的方法, 并强调了"充分探索"的必要性, 但尚未具体说明"充分"的含义. 接下来我们将讨论两种分配探索与利用时间的方法: $\epsilon$-贪心策略和探索函数.

#### 5.4.1 $\epsilon$-贪心策略

采用$\epsilon$-贪心策略的智能体会设定一个概率值$0\leq\epsilon\leq 1$, 以$\epsilon$的概率随机行动进行探索, 同时以$(1-\epsilon)$的概率遵循当前既定策略进行利用. 这种策略实现简单但存在调控难点: 若$\epsilon$取值过大, 即使在学习到最优策略后, 智能体仍会保持随机行为; 反之若$\epsilon$过小, 则探索频率不足, 导致Q学习 (或其他学习算法) 收敛速度缓慢. 解决方法是通过人工调整并随时间逐步降低$\epsilon$值.

#### 5.4.2 探索函数

探索函数通过改进的Q值迭代更新机制, 自动赋予访问频次较低的状态优先权, 从而避免人工调整$\epsilon$的问题. 改进后的更新公式为:
$$
Q(s,a)\leftarrow(1-\alpha)Q(s,a)+\alpha\cdot[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}f(s^{\prime},a^{\prime})]
$$
其中$f$表示探索函数. 探索函数的设计具有灵活性, 常见形式为:
$$
f(s,a)=Q(s,a)+\frac{k}{N(s,a)}
$$
这里$k$为预设常数, $N(s,a)$表示Q状态$(s,a)$的访问次数. 智能体在状态$s$时总是选择当前$f(s,a)$值最高的动作, 从而无需概率性地决定探索或利用. 探索行为通过探索函数自动实现--因为$\frac{k}{N(s, a)}$项会给访问频次低的动作提供足够"奖励值", 使其可能超越高Q值动作被选中. 随着状态访问频次增加, 该项奖励会逐渐趋近于0, $f(s, a)$最终回归到$Q(s,a)$, 使策略逐渐转向完全利用.

### 5.5 本章总结

必须牢记强化学习建立在马尔可夫决策过程 (MDP) 基础上, 其目标是通过推导最优策略来解决MDP问题. 与价值迭代, 策略迭代等方法不同, 强化学习在未知转移函数$T$和奖励函数$R$的情况下, 要求智能体通过在线试错而非离线计算来学习最优策略. 主要方法包括:

● 基于模型的学习: 通过计算估计转移函数$T$和奖励函数$R$的值, 并基于这些估计值运用价值迭代或策略迭代等MDP求解方法.

● 无模型学习: 避免估计$T$和$R$, 直接通过其他方法估计状态值或Q值:

- 直接评估: 遵循策略$\pi$, 简单统计各状态获得的总奖励及访问次数. 当采样足够时, 虽然收敛速度慢且忽略状态转移信息, 但能逼近$\pi$下的真实状态值.
- 时序差分学习 (TD学习): 遵循策略$\pi$, 采用指数移动平均与采样值相结合的方式, 最终收敛到$\pi$下的真实状态值. TD学习与直接评估都属于同策略学习, 即在评估当前策略价值后再决定是否更新策略.

Q学习: 通过Q值迭代更新直接通过试错学习最优策略, 属于异策略学习的典型案例--即使在采取次优动作时仍能学习最优策略.

近似Q学习: 原理与Q学习相同, 但采用基于特征的状态表示来实现泛化学习.

● 为量化不同强化学习算法的性能, 我们使用遗憾值 (Regret) 概念. 遗憾值反映了从初始时刻就采取最优策略获得的总奖励与实际学习算法累计获得总奖励之间的差异.

## 6. 贝叶斯网络

### 6.1 概率论回顾

我们假设您已在CS70课程中学习过概率论基础, 因此本节笔记默认您已掌握概率论的基本概念, 如概率密度函数 (PDF) , 条件概率, 独立性和条件独立性. 以下是我们将使用的概率规则摘要.

**随机变量**表示结果未知的事件. **概率分布**是对结果赋予权重的过程, 需满足以下条件:
$$
\begin{array}{l}
0 \leq P(\omega) \leq 1 \\
\sum_{\omega} P(\omega) = 1
\end{array}
$$
例如, 若A是二元变量 (只能取两个值) , 则 $P(A=0)=p$ 且 $P(A=1)=1-p$, 其中 $p \in [0,1]$

我们约定: 大写字母表示随机变量, 小写字母表示该变量的具体取值

符号 $P(A, B, C)$ 表示变量A, B, C的**联合分布**. 联合分布中顺序无关, 即 $P(A, B, C) = P(C, B, A)$

可通过**链式法则** (The Chain Rule) (也称乘积法则, The Product Rule) 展开联合分布:
$$
\begin{array}{l}
P(A, B) = P(A \mid B)P(B) = P(B \mid A)P(A) \\
P(A_1, A_2, \ldots, A_k) = P(A_1)P(A_2 \mid A_1) \ldots P(A_k \mid A_1, \ldots, A_{k-1})
\end{array}
$$
通过**边缘化** (或称“求和消元”) 可获得A, B的边缘分布: 
 $P(A, B) = \sum_c P(A, B, C=c)$
 A的边缘分布也可表示为: 
 $P(A) = \sum_b \sum_c P(A, B=b, C=c)$

对概率分布进行操作时, 结果可能未归一化 (总和不为1) . 此时需进行**归一化**: 用分布中所有条目之和除以每个条目.

**条件概率**表示在已知某些事实下事件的概率. 例如 $P(A \mid B=b)$ 表示已知B取值为b时A的概率分布, 定义为:
$$
P(A \mid B) = \frac{P(A, B)}{P(B)}
$$
结合条件概率定义与链式法则, 可得**贝叶斯规则**:
$$
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}
$$
若随机变量A与B**相互独立**, 记为 $A \perp\!\!\!\perp B$ (等价于 $B \perp\!\!\!\perp A$) . 此时满足: 
$$
P(A, B) = P(A)P(B)
$$
例如两次独立的硬币抛掷. 根据上式与链式法则可推导出 $P(A \mid B) = P(A)$ 和 $P(B \mid A) = P(B)$. 

若随机变量A与B在给定变量C时**条件独立**, 记为 $A \perp\!\!\!\perp B \mid C$ (等价于 $B \perp\!\!\!\perp A \mid C$). *(举例: 当有蛀牙时, 牙疼与检测出蛀牙是独立的)* 此时满足: 
$$
P(A, B \mid C) = P(A \mid C)P(B \mid C)
$$
这意味着若已知C的取值, 则A与B互不影响. 等价关系还包括: 
$$
\begin{array}{l}
P(A \mid B, C) = P(A \mid C) \\
P(B \mid A, C) = P(B \mid C)
\end{array}
$$
注意这三个等式与相互独立等式的形式一致, 仅增加了对C的条件依赖!

### 6.2 概率推断

在人工智能中, 我们常需建模非确定性事件间的关系. 例如: 

- 若天气预报降雨概率40%, 我该带伞吗?
- 若冰淇淋球数越多越可能掉落, 我该买几球? 
- 若15分钟前我的行车路线上发生事故, 该现在出发还是30分钟后?

这些问题 (及更多) 都可通过**概率推断**解决. 

此前课程中, 我们将世界建模为始终已知的特定状态. 接下来我们将采用新模型: 每个可能的世界状态都有其概率. 例如构建天气模型, 其状态包含季节, 温度和天气, 可能给出 $P(\text{winter}, 35^\circ, \text{cloudy}) = 0.023$, 表示“冬季, 35度, 多云”这一具体结果的概率.

更准确地说, 我们的模型是**联合分布**--即描述每个可能**结果** (变量**赋值**) 概率的表格. 例如下表: 

| 季节 S | 温度 T | 天气 W | 概率 P |
| :----- | :----- | ------ | ------ |
| summer | hot    | sun    | 0.30   |
| summer | hot    | rain   | 0.05   |
| summer | cold   | sun    | 0.10   |
| summer | cold   | rain   | 0.05   |
| winter | hot    | sun    | 0.10   |
| winter | hot    | rain   | 0.05   |
| winter | cold   | sun    | 0.15   |
| winter | cold   | rain   | 0.20   |

此模型可回答我们感兴趣的问题, 例如: 

1. 晴天概率是多少? $P(W=\text{sun})$
2. 已知冬季时的天气概率分布? $P(W \mid S=\text{winter})$
3. 已知寒冷且雨天时是冬季的概率? $P(S=\text{winter} \mid T=\text{cold}, W=\text{rain})$
4. 已知寒冷时的季节与天气联合分布? $P(S, W \mid T=\text{cold})$

#### 枚举推断 

给定联合PDF, 我们可通过**枚举推断** (inference by enumeration, IBE) 计算任意概率分布 $P(Q_1 \ldots Q_m \mid e_1 \ldots e_n)$. 该过程涉及三类变量:

1. **查询变量** $Q_i$: 未知变量, 出现在条件符号左侧
2. **证据变量** $e_i$: 已知观测变量, 出现在条件符号右侧
3. **隐藏变量**: 存在于联合分布但不在目标分布中的变量

枚举推断算法步骤如下:

1. 筛选所有与证据变量一致的观测行
2. 对隐藏变量求和消元 (边缘化)
3. 归一化表格使其成为概率分布 (总和为1)

例如计算 $P(W \mid S=\text{winter})$ 时:	

1. 选择季节为winter的4行
2. 对温度T求和消元并归一化, 得到:

| W    | S      | 未归一化和     | 概率                   |
| ---- | ------ | -------------- | ---------------------- |
| sun  | winter | 0.10+0.15=0.25 | $0.25/(0.25+0.25)=0.5$ |
| rain | winter | 0.05+0.20=0.25 | $0.25/(0.25+0.25)=0.5$ |

因此 $P(W=\text{sun} \mid S=\text{winter}) = 0.5$, $P(W=\text{rain} \mid S=\text{winter}) = 0.5$, 表明冬季晴天和雨天的概率均为50%. 

只要拥有联合PDF表, 枚举推断可计算任意概率分布 (包括多查询变量情形).

### 6.3 贝叶斯网络表示法

虽然枚举推理可以计算我们所需的任何查询概率, 但在计算机内存中表示整个联合分布对于实际问题是不现实的--如果我们要表示的 $n$ 个变量每个都可以取 $d$ 个可能的值 (其**定义域**大小为 $d$) , 那么我们的联合分布表将有 $d^{n}$ 个条目, 这个数量随变量数量呈指数级增长, 存储起来非常不切实际!

贝叶斯网络通过利用条件概率的思想避免了这个问题. 概率信息不是存储在一个巨大的表中, 而是分布在多个较小的条件概率表 (conditional probability table, CPT) 中, 同时用一个**有向无环图** (directed acyclic graph, DAG) 来捕捉变量之间的关系. 局部概率表和 DAG 共同编码了足够的信息, 可以计算我们原本需要整个大型联合分布才能得到的任何概率分布. 我们将在下一节中看到这是如何实现的.

我们正式定义贝叶斯网络由以下部分组成:

1. 一个有向无环图, 其中每个节点对应一个变量 $X$.
2. 每个节点的条件分布 $P(X \mid A_1...A_n),$ 其中 $A_i$ 是 $X$ 的第 $i$ 个父节点, 存储为条件概率表 (CPT) . 每个CPT有 $n+2$ 列: 一列对应每个父变量 $A_1...A_n$的值, 一列对应 $X$ 的值, 还有一列对应 $X$ 给定其父节点的条件概率.

贝叶斯网络图的结构编码了不同节点之间的条件独立关系. 这些条件独立性使我们能够存储多个小表而不是一个大表.

重要的是, 贝叶斯网络节点之间的边并不意味着这些节点之间存在特定的**因果**关系, 或者变量必然相互依赖. 它只是表示节点之间可能存在**某种**关系.

作为贝叶斯网络的一个例子, 考虑一个模型, 其中有五个二元随机变量如下:

- B: 发生入室盗窃.
- A: 警报响起.
- E: 发生地震.
- J: 约翰打电话.
- M: 玛丽打电话.

假设警报可能在入室盗窃或地震发生时响起, 而玛丽和约翰如果听到警报会打电话. 我们可以用下图表示这些依赖关系.

![图1: 基础贝叶斯网络样例](../0_Attachment/Pasted%20image%2020250725231717.png)

在这个贝叶斯网络中, 我们将存储概率表 $P(B)$, $P(E)$, $P(A|B,E)$, $P(J|A)$ 和 $P(M|A)$.

给定图的所有CPT, 我们可以使用以下规则计算给定赋值的概率:

$P(X1,X2,...,Xn)=\prod_{i=1}^{n}P(X_{i}|parents(X_{i}))$

对于上面的警报模型, 我们实际上可以如下计算联合概率:

$P(-b,-e,+a,+j,-m)=P(-b)\cdot P(-e)\cdot P(+a|-b,-e)\cdot P(+j|+a)\cdot P(-m|+a)$

我们将在下一节中看到这个关系是如何成立的.

作为现实检验, 重要的是要理解贝叶斯网络只是一种模型. 模型试图捕捉世界运作的方式, 但由于它们总是简化, 所以总是错误的. 然而, 通过良好的建模选择, 它们仍然可以成为足够好的近似, 对解决现实世界中的实际问题有用.

一般来说, 一个好的模型可能不会考虑每个变量, 甚至不会考虑变量之间的每个交互. 但是通过在图的结构中做出建模假设, 我们可以产生极其高效的推理技术, 这些技术通常比枚举推理等简单程序更实用.

### 6.4 贝叶斯网络的结构

在本课程中, 我们将参考两条可以通过查看贝叶斯网络的图形结构推断出的贝叶斯网络独立性规则:

- **每个节点在给定其所有父节点的情况下, 与图中所有其祖先节点 (非后代) 条件独立.** ![图1: 父节点](../0_Attachment/Pasted%20image%2020250727004041.png)
- **每个节点在给定其马尔可夫覆盖的情况下, 与所有其他变量条件独立.** 一个变量的马尔可夫覆盖包括其父节点, 子节点以及子节点的其他父节点. ![图2: 马尔可夫覆盖](../0_Attachment/Pasted%20image%2020250727004151.png)

使用这些工具, 我们可以回到上一节的断言: 我们可以通过连接贝叶斯网络的CPT来获得所有变量的联合分布.

$P(X_{1},X_{2},\ldots,X_{n})=\prod_{i=1}^{n}P(X_{i}\mid\text{parents}(X_{i}))$

联合分布与贝叶斯网络的CPT之间的这种关系之所以成立, 是因为图给出的条件独立关系. 我们将通过一个例子来证明这一点.

让我们重新审视前面的例子. 我们有CPT $P(B), P(E), P(A\mid B, E), P(J\mid A)$ 和 $P(M\mid A)$, 以及下图:

![图3: 基础贝叶斯网络样例](../0_Attachment/Pasted%20image%2020250725231717.png)

对于这个贝叶斯网络, 我们试图证明以下关系:

$P(B,E,A,J,M)=P(B)P(E)P(A\mid B,E)P(J\mid A)P(M\mid A)$

我们可以用另一种方式展开联合分布: 使用链式法则. 如果我们用拓扑排序 (父节点在子节点之前) 展开联合分布, 我们得到以下方程:

$P(B, E, A, J, M)= P(B) P(E\mid B) P(A\mid B, E) P(J\mid B, E, A) P(M\mid B, E, A, J)$

注意, 在第一个方程中, 每个变量都表示在一个CPT $P(var\mid Parents(var))$ 中, 而在第二个方程中, 每个变量都表示在一个CPT $P(\operatorname{var}\mid\operatorname{Parents}(\operatorname{var}),\operatorname{Ancestors}(\operatorname{var}))$ 中.

我们依赖于上面的第一个条件独立关系, 即在给定所有父节点的情况下, 每个节点与图中所有其祖先节点条件独立[^1].

因此, 在贝叶斯网络中, $P(\operatorname{var}\mid\operatorname{Parents}(\operatorname{var}),\operatorname{Ancestors}(\operatorname{var}))=P(\operatorname{var}\mid\operatorname{Parents}(\operatorname{var}))$, 所以这两个方程是相等的. 贝叶斯网络中的条件独立性允许多个较小的条件概率表来表示整个联合概率分布.

[^1]: 在其他地方, 这个假设可能被定义为"一个节点在给定其父节点的情况下, 与其非后代节点条件独立". 我们总是希望做出尽可能最小的假设并证明我们所需的内容, 因此我们将采用祖先节点的假设

### 6.5 D-分离

在分析随机变量时, 一个关键问题是判断变量之间是否独立或条件独立. 贝叶斯网络通过其拓扑结构为我们提供了一种快速判断这些关系的方法.

我们已知**在给定父节点的情况下, 任意节点都与其所有祖先节点条件独立**.

接下来我们将展示三种经典的三节点两边的贝叶斯网络结构 (三元组) 及其表达的条件独立关系. 

#### 6.5.1 因果链

![图1: 无观测的因果链](../0_Attachment/Pasted%20image%2020250727105637.png)

![图2: 已观测的因果链](../0_Attachment/Pasted%20image%2020250727105642.png)

图1展示了一个**因果链**结构, 其联合概率分布表示为:
$$
P(x,y,z)=P(z|y)P(y|x)P(x)
$$
需要注意的是, $X$ 和 $Z$ 并不保证独立. 例如:
$$
P(y|x) = \begin{cases} 1 & \text{if } x = y \\ 0 & \text{else } \end{cases}
\\
P(z|y) = \begin{cases} 1 & \text{if } z = y \\ 0 & \text{else } \end{cases}
$$

此时 $P(z\mid x)=1$ 当且仅当 $x=z$, 说明 $X$ 和 $Z$ 不独立

但在观测Y的情况下 (图2) , 有 $X \perp\!\!\!\perp Z\mid Y$, 即:
$$
P(X|Z,Y)=P(X|Y)
$$
证明如下:
$$
P(X | Z, y) = \frac{P(X, Z, y)}{P(Z, y)} = \frac{P(Z|y) P(y|X) P(X)}{\sum_{x} P(x, y, Z)} = \frac{P(Z|y) P(y|X) P(X)}{P(Z|y) \sum_{x} P(y|x)P(x)} = \frac{P(y|X) P(X)}{\sum_{x} P(y|x)P(x)} = P(X|y)
$$

#### 6.5.2 共同原因

![图3: 无观测的共同原因](../0_Attachment/Pasted%20image%2020250727105731.png)

![图4: 已观测的共同原因](../0_Attachment/Pasted%20image%2020250727105740.png)

**共同原因**结构的联合分布为:
$$
P(x, y, z) = P(x|y)P(z|y)P(y)
$$

就像因果链一样，我们可以通过以下反例来证明 $X$ 不能保证独立于 $Z$:
$$
P(x|y) = \begin{cases} 1 & \text{if } x = y \\ 0 & \text{else } \end{cases}
\\
P(z|y) = \begin{cases} 1 & \text{if } z = y \\ 0 & \text{else } \end{cases}
$$

但观测到Y时 (图4) 有 $X \perp\!\!\!\perp Z | Y$:
$$
P(X | Z, y) = \frac{P(X, Z, y)}{P(Z, y)} = \frac{P(X|y) P(Z|y) P(y)}{P(Z|y) P(y)} = P(X|y)
$$

#### 6.5.3 共同效应

![图5: 无观测的共同效应](../0_Attachment/Pasted%20image%2020250727105820.png)

![图6: 已观测的共同效应](../0_Attachment/Pasted%20image%2020250727105855.png)

共同效应结构的联合分布为:
$$
P(x,y,z)=P(y|x,z)P(x)P(z)
$$

在未观测Y时 (图5) , X和Z独立：$X \perp\!\!\!\perp Z$. 但观测Y或其子代时 (图6) , 独立性可能被破坏. 例如当 $X$, $Z$ 为二元变量且以相等的概率为真或假:
$$
P(X=true) = P(X=false) = 0.5
\\
P(Z=true) = P(Z=false) = 0.5
$$
$Y$ 表示两者是否相等:
$$
P(Y | X, Z) = \begin{cases} 1 & \text{if } X = Z \text{ and } Y = true \\ 1 & \text{if } X \ne Z \text{ and } Y = false \\ 0 & \text{else} \end{cases}
$$
如果 $Y$ 未被观测, $X$ 和 $Z$ 是独立的. 但如果 $Y$ 被观测, 那么知道 $X$ 的值就等于知道 $Z$的值, 反之亦然. 因此, 给定 $Y$时, $X$ 和 $Z$ 不是条件独立的.

共同效应可被视为与因果链和共同原因“相反”——当不对 $Y$ 进行条件化时，$X$ 和 $Z$ 必然独立. 但当对 $Y$ 进行条件化时，$X$ 和 $Z$ 是否相关取决于 $P(Y∣X,Z)$ 的具体概率值. 

同样的逻辑适用于对图中 $Y$ 的后代节点进行条件化的情况. 如图7所示，若观测到 $Y$ 的某个后代节点，则 $X$ 和 $Z$ 不能保证是独立的. 

![图7: 后代节点被观测的共同效应](../0_Attachment/Pasted%20image%2020250727105928.png)

#### 6.5.4 一般情况与D-分离算法

我们可以将前三种情况作为基础模块, 用于回答具有超过三个节点和两条边的任意贝叶斯网络中的条件独立性问题. 问题形式化表述如下:

**给定一个贝叶斯网络 $G$、两个节点 $X$ 和 $Y$, 以及表示观测变量的 (可能为空的) 节点集合 $\{Z_1,\ldots,Z_k\}$, 以下陈述是否必然成立: $X \perp\!\!\!\perp Y \mid \{Z_1,\ldots,Z_k\}$ ?**

**D-分离** (有向分离) 是贝叶斯网络图结构的一种性质, 它隐含了这种条件独立关系, 并推广了我们之前讨论的案例. 如果变量集合 $Z_1,\ldots,Z_k$ d-分离了 $X$ 和 $Y$, 则在所有可由该贝叶斯网络编码的概率分布中, 均有 $X \perp\!\!\!\perp Y \mid \{Z_1,\ldots,Z_k\}$.

我们首先介绍一个基于节点 $X$ 到 $Y$ 可达性概念的算法 (**注意: 该算法不完全正确, 稍后将修正**):

1. 在图中标记所有观测节点 $\{Z_1,\ldots,Z_k\}$ (用阴影表示).
2. 如果存在一条从 $X$ 到 $Y$ 的未被阴影节点阻塞的无向路径, 则 $X$ 和 $Y$ 是"连通的".
3. 若 $X$ 和 $Y$ 连通, 则它们在给定 $\{Z_1,\ldots,Z_k\}$ 下不条件独立；否则条件独立.

然而, 此算法仅在贝叶斯网络中不存在**共同效应** (Common Effect) 结构时有效, 因为若存在此类结构, 当共同效应中的节点 $Y$ 被激活 (观测) 时, 两个节点会变得"可达". 为此, 我们修正得到以下**d-分离算法**:

1. 标记所有观测节点 $\{Z_1,\ldots,Z_k\}$.

2. 枚举从 $X$ 到 $Y$ 的所有无向路径.

3. 对每条路径
   1. 将路径分解为三元组 (连续的3个节点段).

   2. 若所有三元组均为活跃的, 则该路径活跃并d-连接 $X$ 和 $Y$.

4. 若不存在d-连接 $X$ 和 $Y$ 的路径, 则 $X \perp\!\!\!\perp Y \| \{Z_1,\ldots,Z_k\}$

图中的任何路径均可分解为一系列由3个连续节点和2条边组成的**三元组**. 三元组的活跃性取决于其中间节点是否被观测. 若一条路径中所有三元组均活跃, 则该路径活跃并d-连接 $X$ 和 $Y$, 意味着在给定观测变量下 $X$ 不保证与 $Y$ 条件独立. 若所有路径均不活跃, 则 $X$ 和 $Y$ 在给定观测变量下条件独立.

活跃三元组: ![图8](../0_Attachment/Pasted%20image%2020250727110250.png)

非活跃三元组: ![图9](../0_Attachment/Pasted%20image%2020250727110308.png)

#### 6.5.5 应用示例

![图10](../0_Attachment/Pasted%20image%2020250727110429.png)

- $R \perp\!\!\!\perp B$ - 成立
- $R \perp\!\!\!\perp B | T$ - 不保证成立
- $R \perp\!\!\!\perp B | T'$ - 不保证成立
- $R \perp\!\!\!\perp T' | T$ - 成立

![图11](../0_Attachment/Pasted%20image%2020250727110434.png)

- $L \perp\!\!\!\perp T' | T$ - 成立
- $L \perp\!\!\!\perp B$ - 成立
- $L \perp\!\!\!\perp B | T$ - 不保证成立
- $L \perp\!\!\!\perp B | T'$ - 不保证成立
- $L \perp\!\!\!\perp B | T,R$ - 成立

![图12](../0_Attachment/Pasted%20image%2020250727110442.png)

- $T \perp\!\!\!\perp D$ - 不保证成立
- $T \perp\!\!\!\perp D | R$ - 成立
- $T \perp\!\!\!\perp D | R,S$ - 不保证成立

### 6.6 贝叶斯网络中的精确推理

推理是指寻找某个概率分布 $P\left(Q_{1}\ldots Q_{k}\mid e_{1}\ldots e_{k}\right)$ 值的问题, 正如笔记开头概率推理部分所述. 给定一个贝叶斯网络, 我们可以通过形成联合 PDF 并使用枚举推理来朴素地解决这个问题. 这需要创建并遍历一个指数级大的表格.

#### 6.6.1 变量消除

另一种方法是逐个消除隐藏变量. 为了**消除**变量 $X$, 我们:

1. 连接 (相乘) 所有涉及 $X$ 的因子.
2. 对X进行求和.

**因子**简单地定义为**未归一化的概率**. 在变量消除的整个过程中, 每个因子将与它所对应的概率成比例, 但每个因子的潜在分布不一定像概率分布那样总和为1 (参考 PPT 的 Factor Zoo). 变量消除的伪代码如下:

![图1: 变量消除](../0_Attachment/Pasted%20image%2020250731111425.png)

让我们通过一个例子来具体说明这些概念. 假设我们有一个如下所示的模型, 其中 $T$, $C$, $S$ 和 $E$ 可以取二进制值. 这里, $T$ 表示冒险者拿走宝藏的概率, $C$ 表示冒险者拿走宝藏时笼子掉落的概率, $S$ 表示冒险者拿走宝藏时蛇被释放的概率, $E$表示根据笼子和蛇的状态信息, 冒险者逃脱的概率.

![图2: 变量消除](../0_Attachment/Pasted%20image%2020250731111445.png)

在这种情况下, 我们有因子 $P(T), P(C\mid T), P(S\mid T)$ 和 $P(E\mid C, S)$. 假设我们想计算 $P(T\mid+e)$. 枚举推理的方法是形成一个16行的联合PDF $P(T, C, S, E)$, 然后只选择与 +e 对应的行, 对 $C$ 和 $S$ 进行求和, 最后归一化.

另一种方法是逐个消除 $C$ 和 $S$. 我们将按以下步骤进行:

1. 连接 (相乘) 所有涉及C的因子, 形成 $f_{1}(C,+e, T, S)=P(C\mid T)\cdot P(+e\mid C, S)$. 有时这被写作 $P(C,+e\mid T, S)$

2. 从这个新因子中对C进行求和, 留下一个新因子 $f_{2}(+e, T, S)$, 有时写作 $P(+e\mid T, S)$
3. 连接所有涉及S的因子, 形成 $F_{3}(+e, S, T)=P(S\mid T)\cdot f_{2}(+e, T, S)$, 有时写作 $P(+e, S\mid T)$
4. 对S进行求和, 得到 $f_{4}(+e, T)$, 有时写作 $P(+e\mid T)$
5. 连接剩余的因子, 得到 $f_{5}(+e, T)=f_{4}(+e, T)\cdot P(T)$

6. 一旦我们有了 $f_{5}(+e, T)$, 我们就可以通过归一化轻松计算 $P(T\mid+e)$


在编写连接得到的因子时, 我们可以使用因子表示法, 如 $f_{1}(C,+e, T, S)$, 它忽略条件栏, 并简单地列出该因子中包含的变量.

或者, 我们可以写作 $P(C,+e\mid T, S)$, 即使这不能保证是一个有效的概率分布 (例如所有行概率和不为1) . 为了机械地推导这个表达式, 注意原始因子中条件条左侧的所有变量 (这里是 $P(C\mid T)$ 中的C和 $P(E\mid C, S)$ 中的E) 都保留在条的左侧. 然后, 所有剩余的变量 (这里是T和S) 放在条的右侧.

这种编写因子的方法基于链式法则的重复应用. 在上面的例子中, 我们知道条件栏的两侧不能有相同的变量. 此外, 我们知道:
$$
P(T,C,S,+e)=P(T)P(S|T)P(C|T)P(+e|C,S)=P(S,T)P(C|T)P(+e|C,S)
$$
因此:
$$
P(C|T)P(+e|C,S)=\frac{P(T,C,S,+e)}{P(S,T)}=P(C,+e|T,S)
$$

虽然变量消除过程在概念上更为复杂, 但生成的任何因子的最大大小仅为8行, 而不是16行 (如果我们形成整个联合PDF的话).

另一种看待这个问题的方法是观察到 $P(T\mid+e)$ 的计算可以通过枚举推理进行
$$
\alpha\sum_{s}\sum_{c}P(T)P(s|T)P(c|T)P(+e|c,s)
$$

或者通过变量消除进行:
$$
\alpha P(T)\sum_{s}P(s|T)\sum_{c}P(c|T)P(+e|c,s)
$$

我们可以看到这些方程是等价的, 只是在变量消除中, 我们将与求和无关的项移到了每个求和的外部!

关于变量消除的最后一点需要注意的是, 只有在我们能够将最大因子的大小限制在一个合理的值时, 它才能比枚举推理有所改进.

### 6.7 贝叶斯网络中的近似推理: 采样

概率推理的另一种方法是通过简单地计数样本来隐式计算查询的概率. 这不会像IBE或变量消除那样产生精确的解, 但这种近似推理通常足够好, 尤其是可以大幅节省计算量.

例如, 假设我们想计算 $P(+t\mid+e)$. 如果我们有一台神奇的机器, 可以从我们的分布中生成样本, 我们可以收集所有 $E=+e$ 的样本, 然后计算其中 $T=+t$ 的样本比例. 我们只需查看样本就能轻松计算任何我们想要的推理. 让我们看看一些不同的生成样本的方法.

#### 6.7.1 先验采样

给定一个贝叶斯网络模型, 我们可以轻松编写一个模拟器. 例如, 考虑下面给出的仅包含两个变量 T 和 C 的简化模型的 CPT.

![图1: TC模型](../0_Attachment/Pasted%20image%2020250731173154.png)

一个简单的Python模拟器可以这样编写:

```python
import random

def get_t():
    if random.random() < 0.99:
        return True
    return False

def get_c(t):
    if t and random.random() < 0.95:
        return True
    return False

def get_sample():
    t = get_t()
    c = get_c(t)
    return [t, c]
```

我们称这种简单的方法为**先验采样** (prior sampling). 这种方法的缺点是为了分析不太可能发生的情景, 可能需要生成大量的样本. 如果我们想计算 $P(C\mid-t)$, 我们将不得不丢弃 99% 的样本.

#### 6.7.2 拒绝采样

缓解上述问题的一种方法是修改我们的程序, 提前拒绝任何与我们的证据不一致的样本. 例如, 对于查询 $P(C\mid-t)$, 除非t为假, 否则我们将避免生成C的值. 这仍然意味着我们必须丢弃大部分样本, 但至少我们生成的坏样本创建时间更短. 我们称这种方法为**拒绝采样** (rejection sampling).

![](../0_Attachment/Pasted%20image%2020250731180904.png)

以上两种方法有效的原因是: 任何有效样本的出现概率与联合PDF中指定的相同.

#### 6.7.3 似然加权

一种更奇特的方法是**似然加权** (likelihood weighting), 它确保我们永远不会生成坏样本. 在这种方法中, 我们手动将所有变量设置为查询中的证据. 例如, 如果我们想计算 $P(C\mid-t)$, 我们只需声明 $t$ 为假. 这里的问题是, 这可能会产生与正确分布不一致的样本.

如果我们简单地强制某些变量等于证据, 那么我们的样本出现的概率仅等于非证据变量的 CPT 的乘积. 这意味着联合 PDF 不能保证是正确的 (尽管对于某些情况, 如我们的双变量贝叶斯网络, 可能是正确的). 相反, 如果我们采样了变量 $Z_1$ 到 $Z_p$ 并固定了证据变量 $E_1$ 到 $E_m$, 则样本的概率为 $P(Z_1 \ldots Z_p, E_1 \ldots E_m) = \prod_{i=1}^p P(Z_i \mid \text{Parents}(Z_i))$. 缺失的是样本的概率不包括所有 $P(E_i \mid \text{Parents}(E_i))$ 的概率, 即并非每个CPT都参与.

似然加权通过为每个样本使用一个权重来解决这个问题, 该权重是给定采样变量的证据变量的概率. 也就是说, 我们不是平等地计数所有样本, 而是可以为样本 $j$ 定义一个权重 $w_{j}$, 反映给定采样值时证据变量的观测值的可能性. 通过这种方式, 我们确保每个CPT都参与. 为此, 我们遍历贝叶斯网络中的每个变量, 像正常采样一样, 如果变量不是证据变量, 则采样一个值, 如果变量是证据, 则改变样本的权重.

例如, 假设我们想计算 $P(T \mid +c, +e)$. 对于第 $j$ 个样本, 我们将执行以下算法:

1. 将 $w_j$ 设为1.0, 且 $c = \text{true}$ 和 $e = \text{true}$.
2. 对于 $T$: 这不是一个证据变量, 所以我们从 $P(T)$ 采样 $t_{j}$.
3. 对于 $C$: 这是一个证据变量, 所以我们将样本的权重乘以 $P\left(+c\mid t_{j}\right)$, 即 $w_j = w_j \cdot P(+c \mid t_j)$.
4. 对于 $S$: 从 $P(S\mid t_{j})$ 采样 $s_{j}$.
5. 对于 $E$: 将样本的权重乘以 $P\left(+e\mid+c, s_{j}\right)$, 即 $w_{j}=w_{j}\cdot P\left(+e\mid+c, s_{j}\right)$.

然后, 当我们执行通常的计数过程时, 我们将样本 $j$ 的权重设为 $w_{j}$ 而不是1, 其中 $0\leq w_{j}\leq 1$. 这种方法是有效的, 因为在最终的概率计算中, 权重有效地替代了缺失的CPT. 实际上, 我们确保每个样本的加权概率由以下公式给出:
$$
P(z_1 \ldots z_p, e_1 \ldots e_m) = \left[\prod_{i=1}^p P(z_i \mid \text{Parents}(z_i))\right] \cdot \left[\prod_{i=1}^m P(e_i \mid \text{Parents}(e_i))\right]
$$

似然加权的伪代码如下:

![图2: 似然加权](../0_Attachment/Pasted%20image%2020250731173213.png)

对于我们所有的三种采样方法 (先验采样, 拒绝采样和似然加权), 我们可以通过生成更多的样本来获得更高的准确性. 然而, 在这三种方法中, 似然加权是计算效率最高的, 原因超出了本课程的范围.

#### 6.7.4 吉布斯采样

**吉布斯采样** (Gibbs Sampling)是第四种采样方法. 在这种方法中, 我们首先将所有变量设置为某个完全随机的值 (不考虑任何CPT). 然后, 我们反复选择一个变量, 清除其值, 并根据当前分配给所有其他变量的值重新采样它.

对于上面的T, C, S, E例子, 我们可能会分配 $t = \text{true}$, $c = \text{true}$, $s = \text{false}$ 和 $e = \text{true}$. 我们选择四个变量中的一个来重新采样, 比如 $S$, 并清除它. 然后我们从分布P(S|+t,+c,+e)中选取一个新变量. 这需要我们了解这个条件分布. 事实证明, 我们可以轻松计算任何单个变量给定所有其他变量的分布. 更具体地说, $P(S \mid T, C, E)$ 可以仅使用连接了 $S$ 与它的邻居的 CPT 来计算. 因此, 在一个典型的贝叶斯网络中, 大多数变量只有少量的邻居, 我们可以在线性时间内为每个变量预先计算给定其所有邻居的条件分布.

我们不会证明这一点, 但如果我们重复这个过程足够多次, 尽管我们可能从一个低概率的值分配开始, 但我们的后续样本最终会收敛到正确的分布. 如果你好奇, 可以阅读维基百科吉布斯采样文章中的"失败模式"部分, 了解一些超出本课程范围的注意事项. 

吉布斯采样的伪代码如下:

![图3: 吉布斯采样](../0_Attachment/Pasted%20image%2020250731173225.png)

### 6.8 总结

贝叶斯网络是联合概率分布的一种强大表示. 其拓扑结构编码了独立性和条件独立性关系, 我们可以使用它来建模任意分布, 以进行推理和采样.

在本笔记中, 我们介绍了两种概率推理方法: 精确推理和概率推理 (采样). 在精确推理中, 我们保证得到精确正确的概率, 但计算量可能过大.

所涵盖的精确推理算法包括:

- 枚举推理
- 变量消除

我们可以转向采样以使用更少的计算量来近似解.

所涵盖的采样算法包括:

- 先验采样
- 拒绝采样
- 似然加权
- 吉布斯采样
