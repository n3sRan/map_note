# Introduction to Artificial Intelligence

> CS188 25 Spring 教材/课堂笔记, 由 DS V3 ＆GPT-5-mini 翻译.

## 1. 搜索

### 1.1 智能体

在人工智能中, 核心问题是创建理性智能体 (rational agent) — 这种实体具有目标或偏好, 并试图执行一系列行动以在这些目标下产生最佳/最优的预期结果. 理性智能体存在于特定环境中, 环境的具体形式取决于智能体的实例化. 智能体通过传感器与环境交互, 并通过执行器对环境施加行动. 举个简单的例子, 一个跳棋智能体的环境是虚拟的跳棋棋盘, 它在此与对手对弈, 移动棋子就是其行动. 环境和其中的智能体共同构成了一个世界.

**反射型智能体** (reflex agent) 不会考虑行动的后果, 而是仅根据当前世界状态选择行动. 这类智能体通常不如**规划型智能体** (planning agent) 表现优异, 后者会维护一个世界模型, 并利用该模型模拟执行各种行动. 随后, 智能体可以推测行动的假设性后果, 并选择最佳行动. 这种模拟的"智能"类似于人类在判断任何情境下的最佳行动时的思考方式 — 即前瞻性思考.

为了定义任务环境, 我们使用**PEAS描述** (Performance Measure, Environment, Actuators, Sensors):

- **性能指标** (Performance Measure): 描述智能体试图提升的效用.
- **环境** (Environment): 总结智能体行动的范围及其影响因素.
- **执行器与传感器** (Actuators and Sensors): 智能体作用于环境和从中获取信息的方法.

智能体的设计很大程度上取决于其所处环境的类型. 我们可以通过以下方式对环境进行分类:

- **部分可观测环境** (partially observable environments): 智能体无法获取完整的状态信息, 因此必须对世界状态进行内部估计. 与之相对的是**完全可观测环境** (fully observable environments), 智能体可以获取其状态的完整信息.
- **随机环境** (stochastic environments): 转移模型存在不确定性, 即在特定状态下执行某个行动可能产生多种不同概率的结果. 与之相对的是**确定性环境** (deterministic environments), 行动在特定状态下只会产生一个确定的结果.
- **多智能体环境** (multi-agent environments): 智能体与其他智能体共同行动. 因此, 智能体可能需要随机化其行动以避免被其他智能体"预测".
- **静态环境** (static environments): 智能体行动时环境不会发生变化. 与之相对的是**动态环境** (dynamic environments), 环境会随着智能体的交互而改变.
- **已知物理规则的环境**: 转移模型 (即使是随机的) 对智能体已知, 智能体可以在规划路径时利用该模型. 若物理规则未知, 智能体需通过行动主动学习未知的动态变化.

### 1.2 状态空间与搜索问题

为了创建理性的规划型智能体, 我们需要一种数学方法来表示智能体所处的环境. 为此, 必须形式化地定义**搜索问题**: 给定智能体的当前状态 (其在环境中的配置), 如何以最佳方式达到满足其目标的新状态? 一个搜索问题包含以下要素:

- **状态空间** (state space): 世界中所有可能状态的集合.
- **每个状态下可用的行动集合**.
- **转移模型** (transition model): 在当前状态下执行特定行动后输出下一个状态.
- **行动成本** (action cost): 执行行动后从一个状态转移到另一个状态所消耗的成本.
- **初始状态** (start state): 智能体最初存在的状态.
- **目标测试** (goal test): 以状态为输入的函数, 判断该状态是否为目标状态.

从根本上说, 解决搜索问题的方法是: 首先考虑初始状态, 然后利用行动, 转移和成本方法探索状态空间, 迭代计算各状态的子状态, 直到到达目标状态. 此时, 我们就确定了从初始状态到目标状态的路径 (通常称为**计划**). 状态的考虑顺序由预定义的策略决定. 稍后我们将介绍不同类型的策略及其用途.

在继续讨论如何解决搜索问题之前, 必须区分**世界状态** (world state) 和**搜索状态** (search state). 世界状态包含给定状态的所有信息, 而搜索状态仅包含规划所需的世界信息 (主要是出于空间效率的考虑). 为了说明这些概念, 我们将引入本课程的经典示例 — **吃豆人** (Pacman). 吃豆人游戏的规则很简单: 吃豆人必须在一个迷宫中导航, 吃掉所有 (小) 食物豆, 同时不被巡逻的幽灵吃掉. 如果吃豆人吃掉一个 (大) 能量豆, 他会在一定时间内免疫幽灵的攻击, 并获得吃掉幽灵以得分的能力.

![](<../0_Attachment/Pasted image 20250511174709.png>)

考虑一个简化版的游戏, 迷宫中只有吃豆人和食物豆. 我们可以提出两种不同的搜索问题: **路径规划** (pathing) 和**吃掉所有豆子** (eat-all-dots). 路径规划试图解决从位置 $(x_1, y_1)$ 到位置 $(x_2, y_2)$ 的最优路径问题, 而吃掉所有豆子则试图解决在最短时间内吃掉迷宫中所有食物豆的问题. 以下是两种问题的状态, 行动, 转移模型和目标测试:

**路径规划**

- 状态: $(x, y)$ 位置
- 行动: 北, 南, 东, 西
- 转移模型 (获取下一状态): 仅更新位置
- 目标测试: 是否 $(x,y) = \text{END}$?

**吃掉所有豆子**

- 状态: $\{(x,y) \text{位置}, \text{豆子的布尔值}\}$
- 行动: 北, 南, 东, 西
- 转移模型 (获取下一状态): 更新位置和布尔值
- 目标测试: 所有豆子的布尔值是否为假?

注意, 路径规划的状态包含的信息少于吃掉所有豆子的状态, 因为后者必须维护一个布尔数组, 记录每个食物豆是否被吃掉. 世界状态可能包含更多信息, 例如吃豆人移动的总距离或所有访问过的位置, 以及当前的 $(x, y)$ 位置和豆子的布尔值.

#### 1.2.1 状态空间大小

在估计解决搜索问题的计算运行时间时, 一个重要的问题是状态空间的大小. 这通常通过**基本计数原理** (fundamental counting principle) 来完成, 该原理指出: 如果一个世界中有 $n$ 个可变对象, 每个对象可以分别取 $x_1, x_2, \ldots, x_n$ 个不同的值, 那么状态的总数为 $x_1 \cdot x_2 \cdot \ldots \cdot x_n$. 我们以吃豆人为例说明这一概念:

![](<../0_Attachment/Pasted image 20250511201419.png>)

假设可变对象及其对应的可能性数量如下:

- 吃豆人位置: 吃豆人可以在 120 个不同的 $(x, y)$ 位置, 且只有一个吃豆人.
- 吃豆人方向: 可以是北, 南, 东, 西, 共 4 种可能.
- 幽灵位置: 有两个幽灵, 每个幽灵可以在 12 个不同的 $(x, y)$ 位置.
- 食物豆配置: 有 30 个食物豆, 每个食物豆可以被吃掉或未被吃掉.

根据基本计数原理, 吃豆人有 120 个位置, 4 种方向, 幽灵的配置为 $12 \cdot 12$ (每个幽灵有 12 种位置), 食物豆的配置为 $2 \cdot 2 \cdot \ldots \cdot 2 = 2^{30}$ (每个食物豆有两种可能值 — 被吃掉或未被吃掉). 因此, 总状态空间大小为: $120 \cdot 4 \cdot 12^2 \cdot 2^{30}$.

#### 1.2.2 状态空间图与搜索树

现在我们已经建立了状态空间的概念以及完整定义状态空间所需的四个组成部分, 接下来可以开始解决搜索问题了. 最后一块拼图是**状态空间图** (state space graph) 和**搜索树** (search tree).

回想一下, 图由一组节点和一组连接节点对的边定义, 这些边可能还带有权重. 状态空间图的节点表示状态, 从一个状态到其子状态的有向边表示行动, 边的权重表示执行相应行动的成本. 通常, 状态空间图太大而无法存储在内存中 (即使是我们简单的吃豆人示例也有约 $10^{13}$ 个可能状态), 但它们在概念上对解决问题很有帮助. 还需注意, 在状态空间图中, 每个状态只表示一次 — 没有必要多次表示一个状态, 这一点在推理搜索问题时非常有用.

与状态空间图不同, 搜索树没有限制状态出现的次数. 这是因为搜索树虽然也是一种图, 其节点表示状态, 边表示状态之间的行动, 但每个节点不仅编码状态本身, 还编码从初始状态到该状态的完整路径 (或计划). 观察以下状态空间图和对应的搜索树:

![](<../0_Attachment/Pasted image 20250511201616.png>)

在状态空间图中, 高亮路径 $(S \rightarrow d \rightarrow e \rightarrow r \rightarrow f \rightarrow G)$ 在对应的搜索树中表现为从初始状态 $S$ 到目标状态 $G$ 的路径. 类似地, 从初始节点到任何其他节点的每条路径在搜索树中都表现为从根 $S$ 到某个后代节点的路径. 由于从一个状态到另一个状态通常存在多种方式, 状态往往会在搜索树中多次出现. 因此, 搜索树的大小通常大于或等于对应的状态空间图.

我们已经确定, 即使是简单问题的状态空间图也可能非常庞大, 因此问题来了: 如果这些结构太大而无法存储在内存中, 我们如何进行有用的计算? 答案在于如何计算当前状态的子状态 — 我们只存储当前正在处理的状态, 并根据需要使用相应的 `getNextState`, `getAction` 和 `getActionCost` 方法动态生成新状态. 通常, 搜索问题通过搜索树解决, 我们谨慎地存储少量节点进行观察, 迭代地用子节点替换节点, 直到到达目标状态. 有多种方法可以决定这种迭代替换的顺序, 接下来我们将介绍这些方法.

### 1.3 无信息搜索

从起始状态到目标状态寻找规划方案的标准流程是: 维护一个由搜索树衍生的部分规划方案的外部边界. 我们通过从边界中移除一个节点 (根据给定策略选择) 对应的部分规划, 并用其所有子节点替换该边界节点, 从而持续扩展边界. 移除并用子节点替换边界元素相当于丢弃一个长度为$n$的规划, 同时考虑所有由其衍生的长度为$(n+1)$的规划. 这一过程持续进行, 直到最终从边界中移除一个目标状态, 此时我们确认该目标状态对应的部分规划即为从起始状态到目标状态的路径.

实际实现中, 此类算法通常会在节点对象中编码父节点信息, 节点距离以及状态信息. 上述流程称为**树搜索**, 其伪代码如下:

```pseudocode
function TREE-SEARCH(problem, frontier) return a solution or failure
    frontier ← INSERT(MAKE-NODE(INITIAL-STATE[problem]), frontier)
    while not IS-EMPTY(frontier) do
        node ← POP(frontier)
        if problem.IS-GOAL(node.STATE) then return node
        for each child-node in EXPAND(problem, node) do
            add child-node to frontier
    return failure
```

伪代码中的`EXPAND`函数通过考虑所有可用动作, 返回从给定节点可到达的所有可能节点. 其伪代码如下:

```pseudocode
function EXPAND(problem, node) yields nodes
    s ← node.STATE
    for each action in problem.ACTIONS(s) do
        s' ← problem.RESULT(s, action)
        yield NODE(STATE=s', PARENT=node, ACTION=action)
```

当搜索树中目标状态的位置未知时, 我们只能从**无信息搜索**技术中选择树搜索策略. 下面将依次介绍三种策略: 深度优先搜索, 广度优先搜索和一致代价搜索. 针对每种策略, 我们将从以下维度分析其基本特性:

- **完备性**: 若存在解, 给定无限计算资源时, 该策略是否能保证找到解?
- **最优性**: 该策略是否能保证找到最低成本路径?
- **分支因子b**: 每次从边界取出节点并用其子节点替换时, 边界节点数量的增长为$O(b)$. 在搜索树的第k层, 存在$O(b^{k})$个节点.
- **最大深度m**
- **最浅解深度s**

#### 1.3.1 深度优先搜索 (DFS)

- **描述**: 总是选择从起始节点出发最深的边界节点进行扩展.
- **边界表示**: 需使用后进先出 (LIFO) 栈结构, 确保最新添加的对象具有最高优先级.
  ![](../0_Attachment/Pasted%20image%2020250511204140.png)
- **完备性**: 不完备. 若状态空间图中存在环路, 搜索树将无限深, DFS可能永远无法找到解.
- **最优性**: 非最优. DFS仅找到搜索树中"最左侧"的解, 不考虑路径成本.

- **时间复杂度**: 最坏情况下需探索整个搜索树, 为$O(b^{m})$.
- **空间复杂度**: 最坏情况下边界需维护每层$b$个节点, 共$m$层, 为$O(bm)$.

#### 1.3.2 广度优先搜索 (BFS) 

- **描述**: 总是选择从起始节点出发最浅的边界节点进行扩展.
- **边界表示**: 使用先进先出 (FIFO) 队列, 确保按插入顺序访问节点.
- **完备性**: 完备. 若解存在, 其最浅深度$s$有限, BFS终将搜索到该深度.
- **最优性**: 通常非最优 (除非所有边成本相同, 此时退化为一致代价搜索的特例).
  ![](../0_Attachment/Pasted%20image%2020250511204202.png)
- **时间复杂度**: 需搜索$1 + b + b^{2} + \ldots + b^{s}$个节点, 为$O(b^{s})$.
- **空间复杂度**: 边界在最坏情况下包含最浅解深度的所有节点, 为$O(b^{s})$.

#### 1.3.3 一致代价搜索 (UCS) 

- **描述**: 总是选择从起始节点出发路径成本最低的边界节点进行扩展.
- **边界表示**: 采用基于堆的优先队列, 优先级为从起始节点到该节点的路径成本 (后向成本).
  ![](../0_Attachment/Pasted%20image%2020250511204229.png)
- **完备性**: 完备. 若目标状态存在, 其最短路径长度必然有限.
- **最优性**: 在边成本非负时最优. 按路径成本递增顺序探索, 保证找到最低成本解 (本质是Dijkstra算法的变体). 负边成本会破坏最优性保证.
- **时间复杂度**: 设最优路径成本为$C^*$, 状态空间图中两节点间最小成本为$\varepsilon$, 则需探索深度$1$到$\frac{C^*}{\varepsilon}$的节点, 为$O(b^{\frac{C^*}{\varepsilon}})$.
- **空间复杂度**: 边界包含最优解深度的所有节点, 为$O(b^{\frac{C^*}{\varepsilon}})$.

需要强调的是, 上述三种无信息搜索策略本质相同, 仅扩展策略不同, 其共性已体现在前述树搜索伪代码中.

### 1.4 启发式搜索

一致代价搜索 (UCS) 的优点在于其完备性和最优性, 但由于它从起始状态向各个方向均匀扩展, 搜索速度可能较慢. 如果我们能获得关于搜索方向的某种指引, 就能显著提升性能并更快地"锁定"目标状态. 这正是启发式搜索的核心思想.

#### 1.4.1 启发函数

启发函数是估算到目标状态距离的驱动力 — 它们是接收状态输入并输出相应估计值的函数. 该函数的具体计算方式与待解决的搜索问题相关. 出于后续将介绍的$A^{*}$搜索的原因, 我们通常希望启发函数能给出到目标剩余距离的下界, 因此启发函数通常是对原始问题松弛化 (移除部分约束条件) 后的解. 以吃豆人游戏为例, 考虑之前描述的路径规划问题. 解决该问题的常用启发函数是曼哈顿距离, 对于两点 $(x_{1},y_{1})$ 和 $(x_{2},y_{2})$ 定义如下:

$Manhattan(x_{1},y_{1},x_{2},y_{2})=|x_{1}-x_{2}|+|y_{1}-y_{2}|$

![](../0_Attachment/Pasted%20image%2020250513101121.png)

上述可视化展示了曼哈顿距离所解决的松弛问题 — 假设吃豆人希望到达迷宫左下角, 该函数在忽略墙壁的情况下计算当前位置到目标位置的距离. 这个距离是松弛化问题中的精确目标距离, 对应原始问题中的目标距离估计值. 通过启发函数, 我们可以轻松实现智能体的决策逻辑, 使其在选择动作时"优先"扩展估计更接近目标的状态. 这种"优先"概念非常强大, 被以下两种实现启发函数的搜索算法所采用: 贪婪搜索和$A^{*}$算法.

#### 1.4.2 贪婪搜索

- **描述** - 贪婪搜索是一种总是选择**启发值最低**的边界节点进行扩展的探索策略, 对应其认为最接近目标的状态.

- **边界表示** - 贪婪搜索与UCS操作相同, 使用优先队列表示边界. 区别在于贪婪搜索使用启发值 (**估计的前向成本**) 而非**计算的后向成本** (到该状态的路径边权总和) 来确定优先级.

- **完备性与最优性** - 贪婪搜索不能保证找到存在的目标状态, 也不具有最优性, 特别是在选择较差启发函数时. 其行为在不同场景下表现不稳定, 可能直接抵达目标, 也可能像受错误引导的DFS那样探索错误区域.

#### 1.4.3 A\*搜索

- **描述** - $A^{*}$搜索是一种总是选择**总成本估计值最低**的边界节点进行扩展的探索策略, 其中总成本是从起始节点到目标节点的完整成本.

- **边界表示** - 与贪婪搜索和UCS类似, $A^{*}$也使用优先队列表示边界. 唯一区别在于优先级选择方法: $A^{*}$将UCS使用的后向总成本 (路径边权总和) 与贪婪搜索使用的启发值 (前向成本估计) 相加, 得到从起点到目标的**估计总成本**. 由于我们需要最小化从起点到目标的总成本, 这是绝佳的选择.

- **完备性与最优性** - 在适当启发函数下 (稍后讨论), $A^{*}$搜索兼具完备性和最优性. 它综合了我们目前讨论的所有搜索策略的优点, 融合了贪婪搜索的高速性和UCS的最优性与完备性!

#### 1.4.4 可采纳性

在讨论了启发函数及其在贪婪搜索和$A^{*}$搜索中的应用后, 现在我们来探讨构成"良好"启发函数的条件. 首先用以下定义重新表述UCS, 贪婪搜索和$A^{*}$中优先级队列排序的方法:

- $g(n)$ - UCS计算的后向总成本函数
- $h(n)$ - 贪婪搜索使用的启发函数 (前向成本估计) 
- $f(n)$ - $A^{*}$搜索使用的估计总成本函数, 满足$f(n)=g(n)+h(n)$

在探讨"良好"启发函数之前, 需先回答: 是否任意启发函数都能保持$A^{*}$的完备性和最优性? 显然很容易找到破坏这两个宝贵性质的启发函数. 例如考虑启发函数$h(n)=1-g(n)$, 在任何搜索问题中使用该函数都会得到:

$f(n)=g(n)+h(n)=g(n)+(1-g(n))=1$

此时 $A^{*}$ 退化为所有边权相同的BFS. 如前所述, 在边权非常数的一般情况下, BFS不能保证最优性.

$A^{*}$ 树搜索保持最优性的条件称为**可采纳性**. 可采纳性约束要求可采纳启发函数的估计值既非负也不高估.

定义$h^{*}(n)$为从节点n到目标状态的真实最优前向成本, 可采纳性约束的数学表述为:

$\forall n, 0\leq h(n)\leq h^{*}(n)$

**定理**: 对于给定搜索问题, 若启发函数h满足可采纳性约束, 则在该问题上使用$A^{*}$树搜索将得到最优解.

**证明**: 假设搜索树中存在两个可达目标状态: 最优目标A和次优目标B. 由于A可从起始状态到达, A的某个祖先节点n (可能包含A自身) 必在当前边界上. 我们断言n将在B之前被扩展, 依据以下三点:

1. $g(A) < g(B)$. 因为A最优而B次优, A到起点的后向成本更低
2. $h(A)=h(B)=0$. 根据可采纳性约束, 对于目标状态有$h^{*}(n)=0$, 故$0\leq h(n)\leq 0$
3. $f(n)\leq f(A)$. 根据h的可采纳性, $f(n)=g(n)+h(n)\leq g(n)+h^{*}(n)=g(A)=f(A)$

结合1和2可得$f(A) < f(B)$:

$f(A)=g(A)+h(A)=g(A)<g(B)=g(B)+h(B)=f(B)$

再结合3可得:

$f(n)\leq f(A)\wedge f(A) < f(B) → f(n) < f(B)$

因此n先于B被扩展. 由于n具有任意性, 可推出A的**所有**祖先节点 (含A自身) 都在B之前扩展.

树搜索存在的一个问题是可能陷入状态空间图中的循环无限搜索. 即使不考虑无限循环, 由于到达同一节点可能存在多条路径, 我们往往会多次访问同一节点, 导致指数级工作量. 解决方案是记录已扩展节点集合, 避免重复扩展. 这种优化后的树搜索称为**图搜索**. 此外, 保持最优性还需考虑另一个关键因素. 参考以下状态空间图及对应的搜索树 (带权值和启发值) :

![](../0_Attachment/Pasted%20image%2020250513102135.png)

显然最优路径是$SACG$, 总路径成本$1+1+3=5$. 另一条路径$SBCG$成本为$1+2+3=6$. 但由于节点A的启发值远大于B, 节点C首先作为B的子节点沿次优路径被扩展并加入"已到达"集合. 当$A^{*}$通过A访问C时, 由于已存在更差路径的C节点, 不会重新扩展, 导致错过最优解. 因此, 在$A^{*}$图搜索中保持最优性不仅需要检查节点是否已访问, 还需判断是否找到更优路径.

```pseudocode
function A*-GRAPH-SEARCH(problem, frontier) return a solution or failure
    reached ← an empty dict mapping nodes to the cost to each one
    frontier ← INSERT((MAKE-NODE(INITIAL-STATE[problem]), 0), frontier)
    while not IS-EMPTY(frontier) do
        node, node.CostToNode ← POP(frontier)
        if problem.IS-GOAL(node.STATE) then return node
        if node.STATE is not in reached or reached[node.STATE] > node.CostToNode then
            reached[node.STATE] = node.CostToNode
            for each child-node in EXPAND(problem, node) do
                frontier ← INSERT((child-node, child-node.COST + CostToNode), frontier)
    return failure
```

实现时需注意: 将reached集合存储为哈希集合而非列表至关重要, 因为列表查询成员需要$O(n)$时间复杂度, 这会抵消图搜索的性能优势.

重要提示: 根据定义, 可采纳启发函数必须满足对于任何目标状态G有$h(G)=0$.

#### 1.4.5 支配性

在建立可采纳性及其对保持$A^{*}$搜索最优性的作用后, 我们回到最初的问题: 如何创建"良好"启发函数, 以及如何比较启发函数的优劣. 标准度量是**支配性**. 若启发函数a支配b, 则对于状态空间图中所有节点, a的估计目标距离都不小于b. 数学表述为:

$\forall n: h_{a}(n)\geq h_{b}(n)$

支配性直观体现了启发函数的优劣 — 如果一个可采纳启发函数支配另一个, 则它必然更优, 因为它总能给出更接近真实目标距离的估计. 此外, **平凡启发函数**定义为$h(n)=0$, 使用它会将$A^{*}$退化为UCS. 所有可采纳启发函数都支配平凡启发函数. 在搜索问题的半格结构中, 平凡启发函数通常位于底层. 下图展示了一个包含多个启发函数$h_{a}, h_{b}, h_{c}$的半格结构示例, 从底层的平凡启发函数到顶层的真实目标距离:

![](../0_Attachment/Pasted%20image%2020250513102302.png)

通常, 对多个可采纳启发函数取最大值得到的函数仍保持可采纳性. 这是因为所有启发函数对任意状态的输出值都受限于可采纳性条件$0\leq h(n)\leq h^{*}(n)$, 该范围内的最大值仍在此范围内. 实践中通常为搜索问题生成多个可采纳启发函数, 并取其输出的最大值构成支配 (即优于) 所有单个启发函数的新启发函数.

### 1.5 局部搜索

在前文中, 我们关注的是找到目标状态以及抵达该状态的最优路径. 但在某些问题中, 我们只关心如何找到目标状态 — 重建路径可能是微不足道的. 例如在数独问题中, 最优配置就是目标状态. 一旦找到这个配置, 只需逐个填入数字即可知道如何达成目标.

局部搜索算法让我们无需关注抵达路径即可找到目标状态. 在局部搜索问题中, 状态空间由"完整"解的集合构成. 我们通过这些算法寻找满足某些约束或优化目标函数的配置.

![](../0_Attachment/Pasted%20image%2020250513102838.png)

上图展示了状态空间上目标函数的一维曲线图. 对于该函数, 我们希望找到对应最高目标值的状态. 局部搜索算法的核心思想是: 从每个状态出发, 算法会向目标值更高的邻近状态移动, 直到达到 (期望中的全局) 最大值. 我们将介绍四种此类算法: **爬山法**, **模拟退火**, **局部束搜索**和**遗传算法**. 这些算法也常用于优化任务中以最大化或最小化目标函数.

#### 1.5.1 爬山搜索

爬山搜索算法 (又称**最陡上升法**) 从当前状态移动到能最大程度提升目标值的邻近状态. 该算法不维护搜索树, 仅跟踪状态及其对应的目标值. 爬山法的"贪婪性"使其容易陷入**局部最大值** (见图4.1), 因为这些点在局部看来就是全局最大值; 同时也容易受困于**高原区域** (见图4.1). 高原可分为两类: 无法通过任何方向获得改进的"平坦区域" ("平坦局部最大值"), 以及进展缓慢的平坦区域 ("肩部区域").

爬山法的变体包括**随机爬山法** (从可能的上升移动中随机选择动作), 实践证明这种变体能以更多迭代次数为代价收敛到更高极值. 另一变体是允许**随机横向移动**, 即接受不严格提升目标值的移动, 帮助算法逃离"肩部区域".

```pseudocode
function HILL-CLIMBING(problem) returns a state
	current ← make-node(problem.initial-state)
	loop do
		neighbor ← a highest-valued successor of current
		if neighbor.value ≤ current.value then
			return current.state
        current ← neighbor
```

如上所示, 爬山算法会迭代移动到目标值更高的状态, 直到无法继续优化. 该算法是不完备的. 而**随机重启爬山法**通过从随机选择的初始状态进行多次爬山搜索, 最终必然能收敛到全局最大值, 因此是完备的.

需要注意的是, 后续课程中会出现的"梯度下降"概念与爬山法本质相同, 区别在于前者是最小化代价函数而非最大化目标函数.

#### 1.5.2 模拟退火搜索

第二个局部搜索算法是模拟退火. 该算法结合了随机游走 (随机移动到邻近状态) 和爬山法, 旨在获得完备且高效的搜索算法. 模拟退火允许移动到可能降低目标值的状态.

算法在每个时间步随机选择移动方向: 若移动导致目标值升高则总是接受; 若导致目标值降低则以一定概率接受. 这个概率由温度参数决定 — 初始高温允许更多"不良"移动, 随后按预定"退火计划"逐步降温. 理论上, 若降温足够缓慢, 模拟退火算法将以接近1的概率达到全局最大值.

![图3: 模拟退火算法](../0_Attachment/Pasted%20image%2020250513103419.png)

#### 1.5.3 局部束搜索

局部束搜索是爬山法的另一变体, 关键区别在于该算法每次迭代跟踪k个状态 (线程). 算法从随机初始化k个状态开始, 每轮像爬山法那样选择k个新状态. 这不是简单的k个并行爬山法 — 核心在于算法会从所有线程的后继状态中选出k个最优状态. 若任一线程找到最优值, 算法即终止.

k个线程间可以共享信息, 使得目标值高的"优质线程"能吸引其他线程向该区域靠拢. 与爬山法类似, 局部束搜索也容易陷入"平坦"区域. 随机束搜索 (类比随机爬山法) 可缓解此问题.

#### 1.5.4 遗传算法

最后介绍遗传算法, 这是局部束搜索的变体, 广泛应用于优化任务. 如其名所示, 该算法受进化论启发. 遗传算法开始时如同束搜索, 随机初始化k个状态 (称为种群), 每个状态 (个体) 用有限字母表的字符串表示.

以课程中提到的8皇后问题为例: 该约束满足问题要求在8×8棋盘上放置8个互不攻击的皇后 (即不在同行, 同列或同对角线). 之前介绍的所有算法都可用于解决此问题.

在遗传算法中, 我们用1-8的数字表示每列中皇后的行位置 (图4.6(a)). 每个个体通过评估函数 (适应度函数) 进行评分, 8皇后问题中该函数值为非攻击皇后对数. 选择个体"繁殖"的概率与其适应度成正比 (图4.6(c)), 通过交叉点随机交换父代字符串生成子代 (图4.6(d)), 每个子代还有独立概率发生随机变异.

(a) 8皇后问题: 在棋盘放置8个互不攻击的皇后 (皇后会攻击同行, 同列或同对角线的棋子). 该布局接近解, 但第四和第七列的皇后在对角线上互相攻击. (b) 启发式代价估计h=17的8皇后状态. 棋盘显示每个可能后继状态的h值 (通过移动列内皇后获得), 有8个最佳移动选项h=12. 爬山算法将任选其一.

![图4: 8皇后问题](../0_Attachment/Pasted%20image%2020250513103653.png)

![图5: 遗传算法例子](../0_Attachment/Pasted%20image%2020250513103658.png)

![图6: 遗传算法伪代码](../0_Attachment/Pasted%20image%2020250513103704.png)

与随机束搜索类似, 遗传算法在探索状态空间时尝试向上移动, 并在线程间交换信息. 其核心优势在于交叉操作 — 已进化出高评分的字母块可与其他高评分块组合, 从而产生总分更高的解.

![图7: 8皇后问题解](../0_Attachment/Pasted%20image%2020250513103713.png)

图4.6(c)中前两个父代与图4.6(d)中第一个子代对应的8皇后状态. 绿色列在交叉步骤丢失, 红色列被保留.  (图4.6中的数字说明: 第1行是最底行, 第8行是最顶行) 

### 1.6 总结

我们探讨了搜索问题及其核心要素: 状态空间, 动作集合, 转移函数, 动作成本, 初始状态与目标状态. 智能体通过传感器和执行器与环境交互, 其行为由智能体函数全面定义. 理性智能体以期望效用最大化为目标. 最后, 我们使用PEAS (性能指标, 环境, 执行器, 传感器) 框架来描述任务环境.

关于搜索问题, 可通过多种技术解决, 包括但不限于CS 188课程中研究的五种方法:
- 广度优先搜索
- 深度优先搜索
- 统一成本搜索
- 贪婪搜索
- A*搜索

前三种属于无信息搜索, 后两种则采用启发式估计目标距离的启发式搜索以优化性能. 我们还区分了上述技术中的树搜索与图搜索算法.

此外, 我们讨论了局部搜索算法及其应用场景. 当不关注路径而需满足约束或优化目标时, 这类方法能节省空间并在庞大状态空间中寻找可行解. 涉及的经典局部搜索方法包括:
- 爬山法
- 模拟退火
- 局部束搜索
- 遗传算法

函数优化思想将在后续课程中重现, 尤其在神经网络章节会再次涉及.

## 2 约束满足问题 (CSPs) 

### 2.1 约束满足问题

在之前的笔记中, 我们学习了如何找到搜索问题的最优解, 这是一种规划问题. 现在, 我们将学习解决另一类相关问题 — **约束满足问题 (CSPs)**. 与搜索问题不同, CSPs 是一种**识别问题**, 即我们只需判断某个状态是否为目标状态, 而无需考虑如何到达该目标. CSPs 由以下三个要素定义:

1. **变量**: CSPs 包含一组 $N$ 个变量 $X_{1},\ldots, X_{N}$, 每个变量可以从某个定义的值集合中取一个值.
2. **定义域**: 一个集合 $\left\{x_{1},\ldots, x_{d}\right\}$, 表示 CSP 变量所有可能的取值.
3. **约束**: 约束定义了变量值的限制条件, 可能涉及其他变量.

以 **N皇后问题** 为例: 给定一个 $N \times N$ 的棋盘, 能否找到一种放置 $N$ 个皇后的配置, 使得没有两个皇后互相攻击?

![图1: N皇后问题](../0_Attachment/Pasted%20image%2020250514094742.png)

我们可以将其建模为 CSP:

1. **变量**: $X_{ij}$, 其中 $0 \leq i, j < N$. 每个 $X_{ij}$ 表示棋盘上的一个格子, $i$ 和 $j$ 分别表示行号和列号.
2. **定义域**: $\{0,1\}$. $X_{ij}$ 取值为 0 或 1, 布尔值表示位置 $(i,j)$ 是否有皇后.
3. **约束**:
   - **行约束**: $\forall i, j, k \left(X_{ij}, X_{ik}\right) \in \{(0,0),(0,1),(1,0)\}$, 确保同一行最多一个皇后.
   - **列约束**: $\forall i, j, k \left(X_{ij}, X_{kj}\right) \in \{(0,0),(0,1),(1,0)\}$, 确保同一列最多一个皇后.
   - 对角线约束:
     - $\forall i,j,k \left(X_{ij}, X_{i+k,j+k}\right) \in \{(0,0),(0,1),(1,0)\}$ (主对角线).
     - $\forall i,j,k \left(X_{ij}, X_{i+k,j-k}\right) \in \{(0,0),(0,1),(1,0)\}$ (副对角线).
   - **总数约束**: $\sum_{i,j} X_{ij} = N$, 确保恰好放置 $N$ 个皇后.

约束满足问题是 **NP问题**, 这意味着目前没有已知的多项式时间算法可以求解. 对于 $N$ 个变量且每个变量定义域大小为 $O(d)$ 的问题, 可能的赋值组合数为 $O(d^N)$, 随变量数量指数增长. 为解决这一问题, 我们通常将 CSPs 转化为搜索问题:

- **状态**: 部分赋值 (部分变量已赋值, 其余未赋值).
- **后继函数**: 为当前状态中一个未赋值的变量赋予一个值, 生成新状态.
- **目标测试**: 检查所有变量是否已赋值且满足所有约束.

CSPs 比传统搜索问题具有更多结构特征, 我们可以结合启发式方法利用这些结构, 在合理时间内找到解.

#### 2.1.1 约束图

以 **地图着色问题** 为例: 给定一组颜色, 为地图着色, 要求相邻区域颜色不同.

![图2: 图着色漫画](../0_Attachment/Pasted%20image%2020250514094837.png)

CSPs 常表示为 **约束图**, 其中:

- **节点**: 变量.
- **边**: 变量间的约束.

约束类型:

1. **一元约束**: 涉及单个变量, 不体现在约束图中, 仅用于修剪变量的定义域.
2. **二元约束**: 涉及两个变量, 对应图中的普通边.
3. **高阶约束**: 涉及三个及以上变量, 需特殊表示.

**示例**: 澳大利亚地图着色问题

![图3: 澳大利亚地图](../0_Attachment/Pasted%20image%2020250514094902.png)

- **约束**: 相邻州颜色不同.
- **约束图**: 为每对相邻州添加一条边.

约束图的价值在于揭示 CSP 的结构信息, 例如稀疏/密集连接, 树形结构等, 这些信息可指导求解策略.

![图4: 澳大利亚的约束图](../0_Attachment/Pasted%20image%2020250514094930.png)

### 2.2 约束满足问题的求解

传统上, CSPs 通过 **回溯搜索** (Backtracking Search) 求解. 回溯搜索是深度优先搜索 (DFS) 的优化版本, 改进基于以下原则:

1. **变量顺序固定**: 按固定顺序为变量赋值 (赋值具有交换性).
2. **前瞻性赋值**: 仅为当前变量选择不与已赋值变量冲突的值. 若无合法值, 则回溯到上一个变量重新赋值.

**伪代码** (递归回溯) :

![图1: 回溯搜索的伪代码](../0_Attachment/Pasted%20image%2020250514095000.png)

**对比 DFS 与回溯搜索**:

- DFS 盲目赋值, 导致大量无效搜索.
- 回溯搜索通过约束检查减少回溯次数.

进一步优化手段包括:

- **过滤** (如前向检查, 弧相容).
- **变量/值排序启发式**.
- **利用问题结构** (如树形 CSPs 可线性时间求解).

![图2: DFS 对比回溯搜索](../0_Attachment/Pasted%20image%2020250514195221.png)

### 2.3 过滤技术

我们考虑的第一个CSP性能优化方法是**过滤**, 即通过提前移除已知会导致回溯的值来修剪未分配变量的值域. 最基础的过滤方法是**前向检查**: 每当给变量$X_{i}$赋值时, 会修剪与之共享约束的未分配变量的值域, 这些值若被赋值将违反约束条件. 每当新变量被赋值时, 我们可以运行前向检查, 并在约束图中修剪与新赋值变量相邻的未分配变量的值域. 以地图着色问题为例, 未分配变量及其可能值如下:

![图1: 前向过滤示例](../0_Attachment/Pasted%20image%2020250514095223.png)

当依次赋值$WA=red$和$Q=green$时, 与WA, Q或两者相邻的NT, NSW和SA等州的值域大小会因值被淘汰而减小. 前向检查的思想可以推广为**弧一致性**原则. 对于弧一致性, 我们将CSP约束图中的每条无向边视为**两个**方向相反的有向边, 每条有向边称为**弧**. 弧一致性算法流程如下:

1. **初始化**: 将CSP约束图中所有弧存入队列Q

2. **迭代处理**:
   - 从Q中移除弧$X_{i}\longrightarrow X_{j}$, 并确保对于尾部变量 $X_{i}$ 的**每个**剩余值$v$, 头部变量 $X_{j}$ **至少存在一个**剩余值 $w$ 使得赋值 $X_{i}=v,\,X_{j}=w$ 不违反任何约束. 若 $X_{i}$ 的某个值 $v$ 与 $X_{j}$ 的所有剩余值都冲突, 则从 $X_{i}$ 的值域中移除 $v$
   - 若在弧 $X_{i}\longrightarrow X_{j}$ 的一致性检查中移除了 $X_{i}$ 的至少一个值, 则将所有形如 $X_{k}\longrightarrow X_{i}$ 的弧加入Q ($X_{k}$为所有未分配变量). 若 $X_{k}\longrightarrow X_{i}$ 已在Q中则无需重复添加

3. **终止条件**: 当Q为空或某个变量的值域为空时 (触发回溯)

弧一致性算法通常不够直观, 我们通过地图着色示例演示:

初始时将共享约束的未分配变量间所有弧加入队列Q:
 $Q=[ SA\rightarrow V, V\rightarrow SA, SA\rightarrow NSW, NSW\rightarrow SA, SA\rightarrow NT, NT\rightarrow SA, V\rightarrow NSW, NSW\rightarrow V]$
![图2: 弧一致性示例 1](../0_Attachment/Pasted%20image%2020250514095417.png)

处理第一个弧 $SA \rightarrow V$ 时, SA的值域 {blue} 与V的值域 {red,green,blue} 存在有效组合, 故无需修剪. 但处理 $V\rightarrow SA$ 时, 若 $V=blue$ 会导致 SA 无有效值, 因此从V的值域中移除 blue.

由于修剪了V的值域, 需将所有以V为头的弧重新入队 ($SA\rightarrow V$和$NSW\rightarrow V$), 最终更新后的队列为:
 $Q=[SA\rightarrow NSW, NSW\rightarrow SA, SA\rightarrow NT, NT\rightarrow SA, V\rightarrow NSW, NSW\rightarrow V, SA\rightarrow V]$

![图3: 弧一致性示例 2](../0_Attachment/Pasted%20image%2020250514095509.png)

该过程持续进行, 直到处理弧$SA\rightarrow NT$时从SA的值域中移除blue导致其值域为空, 触发回溯. 注意在Q中$NSW\rightarrow SA$先于$SA\rightarrow NT$被处理, 该弧的一致性检查会从NSW的值域中移除blue.

![图4: 弧一致性示例 3](../0_Attachment/Pasted%20image%2020250514095543.png)

弧一致性通常通过AC-3算法实现 (Arc Consistency Algorithm \#3), 其最坏时间复杂度为 $O\left(e d^{3}\right)$ ($e$为弧数量, $d$为最大值域大小). 相比前向检查, 弧一致性是更全面的值域修剪技术, 能减少回溯次数, 但需要更多计算资源. 因此在选择CSP的过滤技术时需权衡这一利弊.

关于一致性的扩展说明: 弧一致性是更广义的**k-一致性**概念的子集. k-一致性保证对于CSP中任意k个节点, 对其中k-1个节点的一致性赋值能确保第 $k^{th}$ 个节点至少存在一个有效值. 进一步地, **强k-一致性**要求图不仅满足k-一致性, 还同时满足$k-1, k-2,\ldots, 1$ 一致性. 显然, 更高阶的一致性需要更高计算成本. 在此定义下, 弧一致性等价于**2-一致性**.

![图5: AC-3算法伪代码](../0_Attachment/Pasted%20image%2020250514095619.png)

### 2.4 排序策略

在CSP求解过程中, 我们通常需要固定变量和值的顺序. 实践中更有效的方法是动态计算下一个变量和值的选择, 主要遵循两大原则: **最小剩余值**和**最少约束值**:

1. **最小剩余值 (MRV) **: 选择剩余有效值最少的未分配变量 (即约束最严格的变量). 直观而言, 约束最严格的变量最容易因值耗尽导致回溯, 因此应优先赋值.
2. **最少约束值 (LCV) **: 选择对未分配变量值域修剪最少 *(使用的最少的值)* 的赋值. 这需要额外计算 (如为每个值重新运行弧一致性/前向检查), 但合理使用仍能提升速度.

#### 2.4.1 结构优化

最后一类CSP优化方法利用问题结构特性. 对于**树结构CSP** (约束图**无环**的情况), 可将求解时间从 $O(d^N)$ 降至 $O(nd^2)$ (与变量数量线性相关), 具体步骤如下:

1. **根节点选择**: 任选约束图中一个节点作为树根 (根据图论, 树结构中任意节点均可作为根)

2. **边定向**: 将所有无向边转换为从根节点向外指向的有向边, 然后对所得有向无环图进行拓扑排序 (使所有边向右指). 示例如下: ![图1: 树结构CSP](../0_Attachment/Pasted%20image%2020250514200756.png)

3. **反向弧一致性检查**: 从$i=n$到$i=2$依次检查所有弧$\operatorname{Parent}\left(X_{i}\right)\longrightarrow X_{i}$的一致性. 此过程会修剪部分值域. 示例如下: ![图2: 修建树](../0_Attachment/Pasted%20image%2020250514200920.png)

4. **前向赋值**: 从$X_{1}$到$X_{n}$依次为每个$X_{i}$赋值, 确保与其父节点值一致. 由于已对所有弧强制执行一致性, 每个节点的子节点至少存在一个有效值, 因此该过程能保证得到正确解.

对于**近似**树结构的CSP, 可通过**割集条件法**扩展上述算法:

1. 找到约束图中最小的变量子集 (称为**割集**), 移除后得到树结构 (例如地图着色中的SA州是最小割集)
2. 为割集中所有变量赋值, 并修剪相邻节点的值域
3. 对剩余的树结构CSP使用上述算法求解

![图3: 割集示例](../0_Attachment/Pasted%20image%2020250514201131.png)

割集大小为 $c$ 时, 可能需要进行最多 $d^{c}$ 次回溯. 由于剩余树结构CSP有 $(n-c)$ 个变量, 其求解时间为 $O\left((n-c) d^{2}\right)$ . 因此割集条件法的总时间复杂度为 $O\left(d^{c}(n-c)d^{2}\right)$ , 在$c$较小时效率极高.

### 2.5 局部搜索

作为最后一个值得关注的主题, 回溯搜索并非解决约束满足问题 (CSP) 的唯一算法. 另一种广泛使用的算法是**局部搜索**, 其核心思想看似简单却异常有效. 局部搜索通过迭代改进实现 — 先**随机**分配初始值, 然后迭代选择随机冲突变量, 将其值重新赋值为违反最少约束的值 (该策略称为**最小冲突启发式**), 直到不再存在约束冲突. 在此策略下, N皇后等约束满足问题的求解变得极其高效 (时间和空间层面). 例如在下图4皇后的例子中, 仅需2次迭代即可得到解:

![图1: 4皇后问题](../0_Attachment/Pasted%20image%2020250514201241.png)

实际上, 局部搜索不仅对任意大规模N皇后问题表现出近乎常数时间的高成功率, 对任何随机生成的CSP也如此！然而尽管有这些优势, 局部搜索既不完备也不最优, 因此不一定收敛到最优解. 此外, 存在一个临界比例, 超过该比例后局部搜索的计算成本会急剧上升:

![图2: bi'l](../0_Attachment/Pasted%20image%2020250514201331.png)

上图展示了状态空间上目标函数的一维曲线. 我们的目标是找到对应最高目标值的状态. 局部搜索算法的核心思想是: 从每个状态出发, 逐步向目标值更高的邻近状态移动, 直到达到 (期望中的全局) 最大值.

我们将介绍三种此类算法: 爬山法, 模拟退火和遗传算法. 这些算法也常用于优化任务中以最大化或最小化目标函数.

#### 2.5.1 爬山搜索

爬山搜索算法 (或称最陡上升法) 从当前状态移动到能提升目标值的邻近状态. 该算法不维护搜索树, 仅记录状态及其对应目标值. 爬山法的"贪婪性"使其易陷入局部最大值 (如下图), 因为这些点在局部看来就是全局最大值; 此外还会受困于高原区域. 高原可分为两类: 无法找到改进方向的"平坦区域" ("平坦局部最大值") 或进展缓慢的"肩部区域". 改进版本如随机爬山法 (随机选择上坡移动方向) 已被提出, 实践表明这种变体能以更多迭代次数为代价收敛到更高最大值.

![图3: 全局和局部最大值](../0_Attachment/Pasted%20image%2020250514202115.png)

爬山法的伪代码如下所示. 如其名所示, 算法迭代移动到更高目标值的状态直至无法继续改进. 爬山法是不完备的. 而**随机重启爬山法**通过从随机初始状态开始多次执行爬山搜索, 最终必然能命中全局最大值所在初始状态, 因此是完备的.

![图4: 爬山算法伪代码](../0_Attachment/Pasted%20image%2020250514202136.png)

#### 2.5.2 模拟退火搜索

第二个局部搜索算法是**模拟退火**. 该算法结合了随机游走 (随机移动到邻近状态) 和爬山法, 以得到完备且高效的搜索算法. 模拟退火允许移动到可能降低目标值的状态: 算法在每个状态随机选择移动, 若移动提升目标值则总是接受; 若降低目标值则以一定概率接受. 该概率由温度参数决定 — 初始温度较高 (允许更多"不良"移动), 随后按预定计划降温. 若降温足够缓慢, 模拟退火算法将以接近1的概率达到全局最大值.

![模拟退火算法伪代码](../0_Attachment/Pasted%20image%2020250514202306.png)

#### 2.5.3 遗传算法

最后介绍**遗传算法**, 这是局部束搜索的变体, 也广泛用于优化任务. 遗传算法以k个随机初始状态 (称为种群) 开始束搜索. 状态 (或称个体) 用有限字母表的字符串表示. 以课堂讨论的8皇后问题为例: 每个个体可用1-8的数字串表示, 对应每列皇后的行位置 (图6列a). 通过评估函数 (适应度函数) 对个体进行评分 (8皇后问题中是非互攻的皇后对数), 并按此值排序.

![图6: 遗传算法示例](../0_Attachment/Pasted%20image%2020250514202641.png)

个体被选中"繁殖"的概率与其适应度成正比. 根据这些概率选择成对个体进行繁殖 (图6列c), 通过在随机选择的交叉点交换父代字符串产生子代. 最后每个子代以独立概率发生随机变异. 遗传算法伪代码如下:

![遗传算法伪代码](../0_Attachment/Pasted%20image%2020250514202735.png)

遗传算法在探索状态空间时尝试向上移动, 并通过线程间信息交换实现优势. 其主要优点在于交叉操作 — 这使得进化出的高评分字母块能与其他优质块组合, 最终产生高总分解决方案.

### 2.6 总结

需谨记: 约束满足问题通常不存在关于变量数量的多项式时间高效解法. 但通过以下启发式方法, 我们常可在可接受时间内找到解:

● **过滤**: 提前修剪未分配变量的值域以避免不必要回溯. 我们学过的两种重要过滤技术是前向检查和弧一致性.

● **排序**: 选择下一步要赋值的变量或值以使回溯可能性最小化. 变量选择采用MRV (最小剩余值) 策略, 值选择采用LCV (最少约束值) 策略.

● **结构**: 若CSP呈树状或近似树状结构, 可运行树结构CSP算法在线性时间内求解. 类似地, 对近似树状CSP, 可采用割集调节将其转换为多个独立树状CSP分别求解.

## 3. 博弈

### 3.1 博弈

在第一篇笔记中, 我们讨论了搜索问题及其高效, 最优的解决方法 — 通过强大的通用搜索算法, 智能体可以制定最佳计划并执行以达成目标. 现在, 让我们转换思路, 考虑智能体面对一个或多个对手的场景, 这些对手会试图阻止智能体达成目标. 此时, 智能体无法再使用已学的搜索算法来制定计划, 因为我们通常无法确定对手会如何针对我们制定策略并回应我们的行动. 相反, 我们需要使用一类新的算法来解决对抗性搜索问题, 更常见的说法是博弈.

博弈有多种类型. 博弈中的行动可能具有确定性或随机性 (概率性) 的结果, 可以有任意数量的玩家, 可能是零和博弈, 也可能不是. 我们将首先讨论确定性零和博弈, 其中行动是确定性的, 且我们的收益直接等同于对手的损失, 反之亦然. 理解这类博弈的最简单方式是将其视为由单一变量值定义的博弈, 其中一方试图最大化该变量, 而另一方试图最小化它, 从而形成直接竞争. 在Pacman (吃豆人) 游戏中, 这个变量就是你的分数, 你通过快速高效地吃掉豆子来最大化分数, 而幽灵则通过先吃掉你来最小化你的分数. 许多常见的家庭游戏也属于这类博弈:

- **跳棋**: 第一个跳棋计算机程序诞生于1950年. 此后, 跳棋成为一种"已解决"的博弈, 即在双方玩家都采取最优策略的情况下, 任何局面都可以确定为某一方的胜利, 失败或平局.
- **国际象棋**: 1997年, "深蓝"成为第一个在六局比赛中击败人类国际象棋冠军加里·卡斯帕罗夫的计算机程序. 深蓝采用极其复杂的方法, 每秒评估超过2亿个局面. 当前的程序更加强大, 尽管历史意义不如深蓝.
- **围棋**: 围棋的搜索空间比国际象棋大得多, 许多人曾认为围棋程序在未来几年内都无法击败人类世界冠军. 然而, 2016年3月, 谷歌开发的AlphaGo以4比1的历史性比分击败了围棋冠军李世石.

上述所有击败世界冠军的程序都至少在一定程度上使用了我们将要介绍的对抗性搜索技术. 与普通搜索返回完整计划不同, 对抗性搜索返回一种策略或策略函数, 该函数根据智能体及其对手的当前配置推荐最佳行动. 我们将看到, 这类算法具有一种美妙的特性: 通过计算产生行为 — 我们运行的计算在概念上相对简单且广泛通用, 但自然地生成了同一团队智能体之间的协作以及对对手智能体的"智胜".

标准博弈形式化包含以下定义:

- 初始状态 $s_0$
- 玩家 $Players(s)$ 表示当前回合的玩家
- 行动 $Actions(s)$ 表示当前玩家可用的行动
- 转移模型 $Result(s,a)$
- 终止测试 $Terminal\text{-}test(s)$
- 终止效用 $Utility(s,player)$

![图1: 常见游戏](../0_Attachment/Pasted%20image%2020250515164134.png)

### 3.2 极小化极大算法

我们将讨论的第一个零和博弈算法是**极小化极大算法 (minimax)**, 其核心假设是对手行为最优, 即对手总是采取对我们最不利的行动. 为了介绍这一算法, 我们首先需要形式化**终止效用**和**状态值**的概念. 状态值是指控制该状态的智能体可以获得的最高分数. 为了理解这一点, 观察以下极其简单的Pacman游戏棋盘:

![图1: Pacman](../0_Attachment/Pasted%20image%2020250515164222.png)

假设Pacman初始有10分, 每移动一步损失1分, 直到吃掉豆子, 此时游戏进入**终止状态**并结束. 我们可以为这个棋盘构建如下的**博弈树**, 其中状态的子节点与普通搜索问题中的搜索树类似:

![图2: Pacman博弈树](../0_Attachment/Pasted%20image%2020250515164249.png)

从这棵树中可以看出, 如果Pacman直接走向豆子, 游戏结束时他的分数为8分; 而如果他在任何位置回头, 最终会得到更低的分数. 现在我们已经生成了一个包含多个终止状态和中间状态的博弈树, 可以形式化这些状态值的含义.

状态值定义为智能体从该状态可以达成的最佳结果 (**效用**). 稍后我们会更具体地形式化效用的概念, 但现在可以简单地将智能体的效用视为其分数或得分. 终止状态的值 (称为终止效用) 始终是某个确定的已知值, 是博弈的固有属性. 在Pacman的例子中, 最右侧终止状态的值就是8, 即Pacman直接走向豆子获得的分数. 此外, 非终止状态的值定义为其子节点值的最大值. 用 $V(s)$ 表示状态 $s$ 的值, 我们可以总结上述讨论:

对于所有非终止状态, $V(s)=\max_{s^{\prime}\in\text{ successors}(s)} V\left(s^{\prime}\right)$

对于所有终止状态, $V(s)=\text{已知值}$

这建立了一个简单的递归规则. 由此可以理解, 根节点的直接右子节点的值为8, 而直接左子节点的值为6, 因为这是智能体从初始状态分别向右或向左移动时可以获得的最高分数. 通过这种计算, 智能体可以确定最优行动是向右移动, 因为右子节点的值大于左子节点.

现在, 我们引入一个新的游戏棋盘, 其中有一个对抗性幽灵试图阻止Pacman吃掉豆子.

![图3 存在幽灵的Pacman](../0_Attachment/Pasted%20image%2020250515164329.png)

游戏规则规定两个智能体轮流移动, 从而生成一棵博弈树, 其中两个智能体在树的层级上交替"控制"节点. 智能体控制某个节点意味着该节点对应的状态是其回合, 因此它可以决定采取什么行动并相应地改变游戏状态. 以下是上述新双智能体游戏棋盘生成的博弈树:

![图4: Pacman博弈树](../0_Attachment/Pasted%20image%2020250515164442.png)

蓝色节点对应Pacman控制的节点, 可以决定采取什么行动; 红色节点对应幽灵控制的节点. 注意, 幽灵控制节点的所有子节点都是幽灵从其父节点状态向左或向右移动后的状态, 反之亦然. 为了简化, 我们将这棵博弈树截断为深度为2的树, 并为终止状态分配以下虚构值:

![图5: 小型博弈树](../0_Attachment/Pasted%20image%2020250515164524.png)

自然地, 添加幽灵控制的节点会改变Pacman认为最优的行动. 新的最优行动由极小化极大算法确定. 与在树的每一层对子节点效用取最大值不同, 极小化极大算法仅在Pacman控制的节点对其子节点取最大值, 而在幽灵控制的节点对其子节点取最小值. 因此, 上述两个幽灵节点的值分别为 $\min(-8,-5)=-8$ 和 $\min(-10,+8)=-10$. 相应地, Pacman控制的根节点的值为 $\max(-8,-10)=-8$. 由于Pacman希望最大化其分数, 他会选择向左移动并获得-8分, 而不是试图吃掉豆子并获得-10分. 这是"通过计算产生行为"的典型例子 — 尽管Pacman希望获得最右子节点状态中的+8分, 但通过极小化极大算法, 他"知道"一个表现最优的幽灵不会允许他实现这一目标. 为了采取最优行动, Pacman被迫对冲风险, 反直觉地远离豆子以最小化失败的幅度. 我们可以总结极小化极大算法为状态赋值的方式如下:

- 对于智能体控制的状态, $V(s)=\max_{s^{\prime}\in\text{ successors}(s)}V\left(s^{\prime}\right)$

- 对于对手控制的状态, $V(s)=\min_{s^{\prime}\in successors(s)}V(s^{\prime})$

- 对于终止状态, $V(s)=\text{已知值}$


在实现上, 极小化极大算法的行为类似于深度优先搜索 (DFS), 按照与DFS相同的顺序计算节点的值, 从最左侧的终止节点开始, 逐步向右迭代. 更准确地说, 它是对博弈树的**后序遍历**. 极小化极大算法的伪代码既优雅又直观简单, 如下所示. 注意, 极小化极大算法将返回一个行动, 该行动对应于根节点到其取值子节点的分支.

![图6: 极小化极大算法伪代码](../0_Attachment/Pasted%20image%2020250515164602.png)

#### 3.2.1 Alpha-Beta剪枝

极小化极大算法看起来近乎完美 — 简单, 最优且直观. 然而, 其执行过程与深度优先搜索非常相似, 时间复杂度也相同, 为令人沮丧的 $O(b^m)$. 回顾一下, $b$ 是分支因子, $m$ 是找到终止节点的近似树深度. 对于许多博弈来说, 这一运行时间过长. 例如, 国际象棋的分支因子 $b \approx 35$, 树深度 $m \approx 100$. 为了缓解这一问题, 极小化极大算法有一种优化方法 — **Alpha-Beta剪枝**.

从概念上讲, Alpha-Beta剪枝是这样的: 如果你试图通过查看节点的后继节点来确定其值, 那么一旦你知道该节点的值最多只能等于其父节点的最优值, 就可以停止查看. 让我们通过一个例子来理解这一复杂陈述的含义. 考虑以下博弈树, 其中方形节点对应终止状态, 向下的三角形对应最小化节点, 向上的三角形对应最大化节点:

![图7: Alpha-Beta剪枝示例1](../0_Attachment/Pasted%20image%2020250515164638.png)

让我们逐步分析极小化极大算法如何推导这棵树 — 它首先遍历值为3, 12和8的节点, 并为最左侧的最小化节点赋值 $\min(3,12,8)=3$. 然后, 它为中间的最小化节点赋值 $\min(2,4,6)=2$, 为最右侧的最小化节点赋值 $\min(14,5,2)=2$, 最后为根节点的最大化节点赋值 $\max(3,2,2)=3$. 然而, 如果我们思考这一情况, 可以意识到一旦我们看到中间最小化节点的子节点值为2, 就无需再查看该节点的其他子节点. 为什么? 因为我们已经看到中间最小化节点的一个子节点值为2, 所以无论其他子节点的值如何, 中间最小化节点的值最多为2. 既然这一点已经确定, 让我们再进一步思考 — 根节点的最大化节点需要在左最小化节点的值3和中间最小化节点的值 ($\leq 2$) 之间做出选择, 它肯定会选择左最小化节点返回的3, 而不管中间最小化节点剩余子节点的值如何. 这正是我们可以**剪枝**的原因, 无需查看中间最小化节点的剩余子节点:

![图8: Alpha-Beta剪枝示例2](../0_Attachment/Pasted%20image%2020250515164649.png)

实现这种剪枝可以将我们的运行时间降低至 $O\left(b^{m/2}\right)$, 有效地将"可解"深度翻倍. 在实践中, 效果通常不如理论显著, 但通常可以让我们至少多搜索一两层. 这仍然非常重要, 因为能够提前思考3步的玩家比只能思考2步的玩家更有可能获胜. 这种剪枝正是带有Alpha-Beta剪枝的极小化极大算法所做的, 其实现如下:

![图9: Alpha-Beta剪枝伪代码](../0_Attachment/Pasted%20image%2020250515164715.png)

花些时间将其与普通极小化极大算法的伪代码进行比较, 注意我们现在可以在不搜索所有后继节点的情况下提前返回.

#### 3.2.2 评估函数

尽管Alpha-Beta剪枝可以帮助增加我们可以运行极小化极大算法的深度, 但对于大多数博弈来说, 这通常还远远不足以到达搜索树的底部. 因此, 我们转向**评估函数**, 这些函数接受一个状态并输出对该节点真实极小化极大值的估计. 通常, 一个好的评估函数会将"更好"的状态赋予更高的值. 评估函数广泛应用于**深度受限的极小化极大算法**中, 其中我们将位于最大可解深度的非终止节点视为终止节点, 并根据精心选择的评估函数为它们分配模拟的终止效用. 由于评估函数只能产生非终止效用值的估计, 这移除了运行极小化极大算法时最优行为的保证.

在设计运行极小化极大算法的智能体时, 通常会投入大量思考和实验来选择评估函数. 评估函数越好, 智能体的行为就越接近最优. 此外, 在使用评估函数之前深入树中更深层次也往往会带来更好的结果 — 将计算深埋在博弈树中可以减轻对最优性的妥协. 这些函数在博弈中的作用与启发式在标准搜索问题中的作用非常相似.

评估函数最常见的设计是特征的线性组合:

$Eval(s)=w_{1}f_{1}(s)+w_{2}f_{2}(s)+...+w_{n}f_{n}(s)$

每个 $f_{i}(s)$ 对应从输入状态 $s$ 中提取的一个特征, 每个特征被赋予相应的**权重** $w_{i}$. 特征只是游戏状态的某个元素, 我们可以提取并赋予数值. 例如, 在跳棋游戏中, 我们可能会构建一个包含4个特征的评估函数: 己方兵的数量, 己方王的数量, 对手兵的数量和对手王的数量. 然后, 我们会根据它们的重要性大致选择适当的权重. 在跳棋的例子中, 为己方的兵和王选择正权重, 为对手的兵和王选择负权重是最合理的. 此外, 由于王在跳棋中是比兵更有价值的棋子, 与己方或对手的王对应的特征的权重绝对值应该大于与兵对应的特征. 以下是一个符合我们刚刚讨论的特征和权重的可能评估函数:

$Eval(s)=2\cdot agent\_kings(s)+agent\_pawns(s)-2\cdot opponent\_kings(s)-opponent\_pawns(s)$

可以看出, 评估函数的设计可以非常自由, 甚至不必是线性函数. 例如, 在强化学习应用中, 基于神经网络的非线性评估函数非常常见. 最重要的是要记住, 评估函数应尽可能频繁地为更好的位置赋予更高的分数. 这可能需要大量微调和实验, 以测试使用具有多种不同特征和权重的评估函数的智能体的性能.

### 3.3 期望最大算法

我们已经了解了极小极大算法的工作原理, 以及如何通过完整的极小极大搜索来应对最优对手的最优策略. 然而, 极小极大算法在应对某些特定情境时存在天然限制. 由于极小极大算法假设对手总是采取最优策略, 因此在面对不一定总是最优应对的场景时 (例如带有随机性的卡牌或骰子游戏, 或行为随机/次优的不可预测对手), 该算法往往会表现出过度悲观. 关于这类随机性场景的详细讨论, 我们将在课程后半部分讲解**马尔可夫决策过程**时展开.

这种随机性可以通过极小极大算法的泛化形式 — **期望最大算法**来建模. 期望最大算法在博弈树中引入了机会节点 (chance nodes), 这些节点不再像极小化节点那样考虑最坏情况, 而是计算平均情况下的**期望效用**. 具体而言:

- 极小化节点计算子节点的最小效用值
- 机会节点计算期望效用或期望值

节点价值计算规则如下:

- $\forall$ 智能体控制的状态: $V(s)=\max_{s^{\prime}\in\text{后继状态}(s)} V\left(s^{\prime}\right)$
- $\forall$ 机会状态: $V(s)=\sum_{s^{\prime}\in 后继状态(s)}p(s^{\prime}|s)V(s^{\prime})$
- $\forall$ 终止状态: $V(s)=$ 已知值

其中$p\left(s^{\prime}\mid s\right)$表示:

1. 非确定性动作导致状态从s转移到s'的概率
2. 对手选择动作导致状态转移的概率

由此可见, 极小极大算法只是期望最大算法的特例 — 当机会节点给最低价值子节点分配概率1, 其他子节点概率0时, 两者等价. 通常这些概率由游戏本身的特性决定.

期望最大算法的伪代码与极小极大算法非常相似, 仅需将极小化节点替换为计算期望效用的机会节点:

![图1: 期望最大算法伪代码](../0_Attachment/Pasted%20image%2020250516173838.png)

举例说明: 假设某期望最大树中所有机会节点的子节点出现概率均为1/3.

![图2: 未填充的期望最大算法](../0_Attachment/Pasted%20image%2020250516174015.png)

根据价值计算规则, 从左至右三个机会节点的值分别为:

-  $\frac{1}{3}\cdot 3+\frac{1}{3}\cdot 12+\frac{1}{3}\cdot 9=\boxed{8}$
-  $\frac{1}{3}\cdot 2+\frac{1}{3}\cdot 4+\frac{1}{3}\cdot 6=\boxed{4}$
-  $\frac{1}{3}\cdot 15+\frac{1}{3}\cdot 6+\frac{1}{3}\cdot 0=\boxed{7}$

 最大化节点选择这三个值中的最大值.

![图3: 已填充的期望最大算法](../0_Attachment/Pasted%20image%2020250516174023.png)

重要提示: 期望最大算法通常需要遍历机会节点的所有子节点, 不能像极小极大算法那样进行剪枝. 因为单个极端值可能显著影响期望值的计算结果. 不过当节点值存在已知有限边界时, 剪枝仍有可能实现.

#### 3.3.1 混合节点类型

虽然极小极大和期望最大算法分别要求交替出现最大化/极小化节点和最大化/机会节点, 但许多游戏并不严格遵循这种交替模式. 以吃豆人游戏为例:

- 一个吃豆人 (最大化层) 移动后, 通常有多个幽灵 (多个极小化层) 轮流移动
- 若存在随机行为的幽灵, 可设计为"最大化-机会-极小化"的混合层次

![图4: 混合层](../0_Attachment/Pasted%20image%2020250516174317.png)

这种灵活的层次设计允许我们针对任何零和博弈, 构建结合期望最大与极小极大特性的混合博弈树.

### 3.4 通用博弈模型

并非所有博弈都是零和的. 当不同智能体在博弈中具有不直接冲突的独立目标时, 可以使用多智能体效用元组来建模:

- 每个节点价值表示为效用元组$(u_1,u_2,...)$
- 每个智能体在自身决策层最大化对应的效用分量

例如某博弈树中:

![图1: 多智能体效用](../0_Attachment/Pasted%20image%2020250515170755.png)

- 红色节点最大化红色效用分量
- 绿色节点最大化绿色分量
- 蓝色节点最大化蓝色分量

最终根节点效用元组为(5,2,5). 这种设计通过计算自然产生协作行为, 因为最终选择的解通常会为所有参与方提供合理效用.

### 3.5 蒙特卡洛树搜索 (MCTS) 

对于围棋等高分支因子的场景, 极小极大算法不再适用. **MCTS算法**基于两个核心理念:

1. **rollout 评估**: 从状态s出发, 根据某种策略 (如随机策略) 进行多次完整对局, 统计胜负次数
2. **选择性搜索**: 不受深度限制地探索可能改进根节点决策的子树部分

算法流程示例:

![图1: MCTS 示例1](../0_Attachment/Pasted%20image%2020250516174430.png)

1. 对当前状态的三个可选动作 (左/中/右) 各模拟100次
2. 记录每个动作的胜率百分比
3. 若某个动作早期表现明显较差, 可将剩余模拟次数分配给其他动作

示例: 将中间动作的剩余模拟分配给左侧和右侧动作.

![图2: MCTS 示例2](../0_Attachment/Pasted%20image%2020250516174623.png)

一个有趣的情况是, 某些动作产生的获胜百分比相似, 但其中一个动作使用的模拟次数要少得多, 如下图所示. 在这种情况下, 使用较少模拟的动作的估计值将具有更高的方差, 因此我们可能需要为该作分配更多模拟, 以便对真实的获胜百分比更有信心.

![图3: MCTS 示例1](../0_Attachment/Pasted%20image%2020250516175124.png)

**UCB1算法**通过以下公式平衡"潜力"与"不确定性":
$$
UCB1(n)=\frac{U(n)}{N(n)}+C\times\sqrt{\frac{\log N(PARENT(n))}{N(n)}}
$$

 其中:

- $N(n)$: 节点n的模拟总次数
- $U(n)$: 父节点玩家 $PARENT(n)$ 的胜利次数
- $C$: 用户指定的探索系数

**MCTS UCT算法**在树搜索问题中使用 UCB 标准, 会重复执行以下三步:

1. 使用UCB准则从根节点向下遍历至未扩展的叶节点
2. 为该叶节点添加新子节点, 并通过rollout确定其胜率
3. 反向传播更新从子节点到根节点的统计信息

当模拟次数$N\rightarrow\infty$时, UCT 的行为会趋近于极小极大智能体.

### 3.6 总结

本章从标准路径搜索问题转向对抗性搜索问题, 主要算法包括:

- **极小极大算法**: 适用于最优对手, 可通过 $\alpha-\beta$ 剪枝优化. 在未知对手情况下表现更保守稳健
- **期望最大算法**: 适用于次优对手, 通过概率分布计算期望价值

由于完全展开博弈树计算量过大, 我们引入了评估函数进行早期终止. 针对高分支因子问题, 提出了可并行化的 MCTS 和 UCT 算法. 最后讨论了非零和博弈的通用建模方法.

## 4. 马尔可夫决策过程

### 4.1 马尔可夫决策过程

马尔可夫决策过程 (Markov Decision Process, MDP) 由以下属性定义:

- **状态集合** $S$: MDP 中的状态表示方式与传统搜索问题相同
- **动作集合** $A$: 动作表示方式也与传统搜索问题相同
- **初始状态**
- **可能的终止状态 (一个或多个) **
- **可能的折扣因子** $\gamma$ (后文详述) 
- **转移函数** $T(s,a,s')$: 由于引入了非确定性动作, 需要量化从状态 $s$ 采取动作 $a$ 后到达 $s'$ 的概率. 转移函数即为此概率分布: $T(s,a,s') = P(s' \mid s,a)$
- **奖励函数** $R(s,a,s')$: 通常包含每步的"存活"奖励和到达终止状态的大额奖励. 奖励可正可负, 智能体的目标是最大化累计奖励

为某种情境构建马尔可夫决策过程 (MDP) 与为搜索问题构建状态空间图非常相似, 但需要注意几个额外的要点. 以赛车激励示例为例:

![图1: 赛车示例](../0_Attachment/Pasted%20image%2020250517105659.png)

- 状态集合 $S = \{\text{cool, warm, overheated}\}$
- 动作集合 $A = \{\text{slow, fast}\}$
- 与状态空间图类似, 这三个状态分别由节点表示, 而边代表动作. **Overheated (过热) ​**​ 是一个终止状态, 因为一旦赛车智能体进入该状态, 就无法再执行任何动作以获取进一步奖励 (它是 MDP 中的吸收状态, 没有出边)
- 值得注意的是, 对于**非确定性动作**, 同一状态下的同一动作可能对应多条边, 每条边指向不同的后继状态. 每条边不仅标注了对应的动作, 还标注了**转移概率**和**相应的奖励**. 具体如下:
- **转移函数**:
  - $T(\text{cool}, \text{slow}, \text{cool}) = 1$
  - $T(\text{warm}, \text{slow}, \text{cool}) = 0.5$
  - $T(\text{warm}, \text{slow}, \text{warm}) = 0.5$
  - $T(\text{cool}, \text{fast}, \text{cool}) = 0.5$
  - $T(\text{cool}, \text{fast}, \text{warm}) = 0.5$
  - $T(\text{warm}, \text{fast}, \text{overheated}) = 1$
- **奖励函数**:
  - $R(\text{cool}, \text{slow}, \text{cool}) = 1$
  - $R(\text{warm}, \text{slow}, \text{cool}) = 1$
  - $R(\text{warm}, \text{slow}, \text{warm}) = 1$
  - $R(\text{cool}, \text{fast}, \text{cool}) = 2$
  - $R(\text{cool}, \text{fast}, \text{warm}) = 2$
  - $R(\text{warm}, \text{fast}, \text{overheated}) = -10$

我们通过离散**时间步长**来表示智能体在不同 MDP 状态间的转移过程, 其中 $s_t \in S$ 表示智能体在时间步 $t$ 时所处的状态, $a_t \in A$​ 表示智能体在时间步 ​**t**​ 时采取的动作. 智能体从时间步 ​0​ 的初始状态 $s_0$​ 开始, 并在每个时间步执行一个动作. 因此, 智能体在 MDP 中的转移过程可以建模如下:
$$
s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} \cdots
$$
此外, 由于智能体的目标是在所有时间步上最大化其累积奖励, 我们可以用数学方式将其表达为以下效用函数的最大化:
$$
U([s_0,a_0,s_1,...]) = \sum_{t=0}^\infty R(s_t,a_t,s_{t+1})
$$
马尔可夫决策过程与状态空间图类似, 都可以展开为搜索树. 在这些搜索树中, 不确定性通过 **Q 状态** (又称**动作状态**) 进行建模, 其本质与期望最大化的机会节点完全相同. 这种建模方式非常恰当, 因为 Q 状态利用概率来模拟环境将智能体置于特定状态的不确定性, 正如期望最大化机会节点通过概率来模拟敌对智能体通过其选择的动作将我方智能体置于特定状态的不确定性. 从状态s采取动作a所形成的Q状态, 可用元组 $(s, a)$ 表示.

*DS V3: MDP可展开为包含**Q 状态 (动作状态) **的搜索树, Q 状态 $(s,a)$ 表示从 $s$ 执行 $a$ 后的不确定性分支 (类似 expectimax 的几率节点)*

观察我们赛车展开的搜索树 (截取至深度为2的层级):

![图2: 赛车搜索树](../0_Attachment/Pasted%20image%2020250517110919.png)

绿色节点代表Q状态 (动作状态), 即已从某个状态执行了动作, 但尚未确定后续状态. 需要注意的是, 智能体在Q状态中**不消耗任何时间步**, 它们本质上只是为方便表示和开发MDP算法而构建的抽象概念.

#### 4.1.1 有限时域与折扣因子

**问题**: 我们的赛车 MDP 存在一个固有缺陷 — 我们未对赛车可执行动作并获取奖励的时间步数设置任何限制. 按照当前设定, 赛车可能永远在每个时间步都选择 $a = \text{slow}$, 安全且高效地获得无限奖励, 同时完全规避过热风险.

**解决方案**

1. **有限时域**: 设定智能体的"寿命"时间步数$n$.

2. **折扣因子** $\gamma$: 奖励随时间指数衰减, 时间步$t$的奖励变为$\gamma^t R(s_t,a_t,s_{t+1})$. 折扣效用函数为:
   $$
   U([s_0,a_0,s_1,...]) = R(s_0,a_0,s_1) + \gamma R(s_1,a_1,s_2) + \gamma^2 R(s_2,a_2,s_3) + \ldots
   $$
   当$|\gamma|<1$时, 总效用有限:
   $$
   U \leq \sum_{t=0}^{\infty} \gamma^t R_{\text{max}} = \frac{R_{\text{max}}}{1-\gamma}
   $$
   通常选择$0 < \gamma < 1$ (负值无实际意义)

#### 4.1.2 马尔可夫性

MDP满足**马尔可夫性质** (无记忆性): 未来仅依赖当前状态, 与过去无关. 数学表达:
$$
P(S_{t+1}=s_{t+1} \mid S_t=s_t, A_t=a_t, \ldots, S_0=s_0) = P(S_{t+1}=s_{t+1} \mid S_t=s_t, A_t=a_t)  
$$
转移函数即为此概率:
$$
\boxed{T(s,a,s') = P(s' \mid s,a)}
$$

### 4.2 求解马尔可夫决策过程

求解 MDP 即找到**最优策略** $\pi^*: S \rightarrow A$, 该策略使智能体获得最大期望效用.

示例 MDP 分析:

![图1: 简单 MDP](../0_Attachment/Pasted%20image%2020250517113252.png)

- 状态: $\{a,b,c,d,e\}$
- 动作: $\{\text{East}, \text{West}, \text{Exit}\}$
- 折扣因子 $\gamma=0.1$

两种可能策略如下:

![图2: 两种可能策略](../0_Attachment/Pasted%20image%2020250517113514.png)

比较两种策略后发现 **Policy 2** 更优.

#### 4.2.1 贝尔曼方程

定义两个核心量:

1. **状态最优值** $V^*(s)$: 从$s$出发的最优期望效用
2. **Q状态最优值** $Q^*(s,a)$: 从$(s,a)$出发的最优期望效用

贝尔曼方程定义:
$$
V^*(s) = \max_a \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^*(s') \right]  
$$
Q值定义:
$$
Q^*(s,a) = \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^*(s') \right]  
$$
两者关系:
$$
V^*(s) = \max_a Q^*(s,a)
$$
贝尔曼方程是动态规划方程, 通过递归分解问题. $[R(s,a,s') + \gamma V^*(s')]$表示即时奖励加上后续最优折扣效用. $Q^*$是期望效用加权和, $V^*$是最大期望效用 (类似期望最大化算法).

满足贝尔曼方程的值即为最优值:
$$
\forall s \in S, \, V(s) = V^*(s)
$$

### 4.3 值迭代

既然我们已经有了一个检验MDP中状态值最优性的框架, 接下来自然要问: 如何实际计算这些最优值? 为此, 我们需要引入时间限制值 (这是施加有限时间步长的自然结果). 状态s在时间步长限制为k时的限时值记作$V_{k}(s)$, 表示从s出发在k个时间步内终止的MDP中可获取的最大期望效用. 等价地说, 这就是在MDP搜索树上运行深度为k的期望最大化算法返回的结果.

值迭代是一种动态规划算法, 它通过逐步延长时限来计算限时值, 直到收敛 (即每个状态的V值与前一次迭代相同: $\forall s, V_{k+1}(s)=V_{k}(s)$). 其操作步骤如下:

1. $\forall s\in S$, 初始化$V_{0}(s)=0$. 这很直观, 因为将时间限制设为0意味着在终止前无法采取任何动作, 因此无法获得任何奖励.
2. 重复以下更新规则直至收敛:
   $\forall s\in S,\,V_{k+1}(s)\leftarrow\max_{a}\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V_{k}(s^{\prime})]$
   在值迭代的第k次迭代中, 我们使用每个状态的k步限时值来生成(k+1)步限时值. 本质上, 我们利用子问题的解 (所有$V_k(s)$) 迭代构建更大子问题的解 (所有$V_{k+1}(s)$) ; 这正是使值迭代成为动态规划算法的原因.

注意, 虽然贝尔曼方程看起来与上述更新规则几乎相同, 但它们并不相同. 贝尔曼方程给出了最优性的条件, 而更新规则给出了一种迭代更新值直至收敛的方法. 当达到收敛时, 贝尔曼方程对每个状态都成立: $\forall s\in S,\,V_{k}(s)=V_{k+1}(s)=V^{*}(s)$.

为简洁起见, 我们经常用简写 $V_{k+1}\leftarrow B U_{k}$ 表示 $U_{k+1}(s)\leftarrow\max_{a}\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V_{k}\left(s^{\prime}\right)\right]$, 其中 B 称为贝尔曼算子. 贝尔曼算子是$\gamma$的**压缩映射**. 为证明这一点, 我们需要以下一般不等式: *(没看懂什么是压缩映射)*

$|\max_{z}f(z)-\max_{z}h(z)|\leq\max_{z}|f(z)-h(z)|.$

现在考虑在相同状态下评估的两个值函数$V(s)$和$V^{\prime}(s)$. 我们通过以下方式证明贝尔曼更新B是关于最大范数的$\gamma\in(0,1)$压缩映射:
$$
\begin{aligned}&|BV(s)-BV^{\prime}(s)|\\ &=\left|\left(\max_{a}\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V(s^{\prime})]\right)-\left(\max_{a}\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{\prime}(s^{\prime})]\right)\right|\\ &\leq\max_{a}\left|\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V(s^{\prime})]-\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{\prime}(s^{\prime})]\right|\\ &=\max_{a}\left|\gamma\sum_{s^{\prime}}T(s,a,s^{\prime})V(s^{\prime})-\gamma\sum_{s^{\prime}}T(s,a,s^{\prime})V^{\prime}(s^{\prime})\right|\\ &=\gamma\max_{a}\left|\sum_{s^{\prime}}T(s,a,s^{\prime})(V(s^{\prime})-V^{\prime}(s^{\prime}))\right|\\ &\leq\gamma\max_{a}\left|\sum_{s^{\prime}}T(s,a,s^{\prime})\max_{s^{\prime}}|V(s^{\prime})-V^{\prime}(s^{\prime})|\right|\\ &=\gamma\max_{s^{\prime}}|V(s^{\prime})-V^{\prime}(s^{\prime})|\\ &=\gamma||V(s^{\prime})-V^{\prime}(s^{\prime})||_{\infty},\end{aligned}
$$
其中第一个不等式来自上述一般不等式, 第二个不等式来自取V和$V^{\prime}$之间差值的最大值, 在倒数第二步中, 我们利用了无论选择哪个动作a概率总和为1的性质. 最后一步使用了向量 $x=(x_{1},\ldots,x_{n})$的最大范数定义, 即 $||x||_{\infty}=\max(|x_{1}|,\ldots,|x_{n}|).$

由于我们刚刚证明了通过贝尔曼更新的值迭代是$\gamma$的压缩映射, 我们知道值迭代会收敛, 且当达到满足$V^{*}=BU^{*}$的不动点时收敛完成. 让我们通过重新审视之前的赛车MDP实例, 引入折扣因子$\gamma=0.5$, 来看几个值迭代更新的实际操作:

![图1: 赛车 MDP](../0_Attachment/Pasted%20image%2020250517140907.png)

我们通过初始化所有$V_0(s)=0$开始值迭代:

|       | cool | warm | overheated |
| ----- | ---- | ---- | ---------- |
| $V_0$ | 0    | 0    | 0          |

在第一轮更新中, 我们可以如下计算$\forall s\in S,~{}V_{1}(s)$:
$$
\begin{aligned} V_{1}(cool)&=\max\{1\cdot[1+0.5\cdot 0],\quad 0.5\cdot[2+0.5\cdot 0]+0.5\cdot[2+0.5\cdot 0]\}\\ &=\max\{1,2\}\\ &=\boxed{2}\\ V_{1}(warm)&=\max\{0.5\cdot[1+0.5\cdot 0]+0.5\cdot[1+0.5\cdot 0],\quad 1\cdot[-10+0.5\cdot 0]\}\\ &=\max\{1,-10\}\\ &=\boxed{1}\\ V_{1}(overheated)&=\max\{\}\\ &=\boxed{0}\end{aligned}
$$

|       | cool | warm | overheated |
| ----- | ---- | ---- | ---------- |
| $U_0$ | 0    | 0    | 0          |
| $U_1$ | 2    | 1    | 0          |

类似地, 我们可以重复该过程, 利用新发现的$U_1(s)$值计算第二轮更新$V_2(s)$:
$$
\begin{aligned} V_{2}(cool)&=\max\{1\cdot[1+0.5\cdot 2],\,0.5\cdot[2+0.5\cdot 2]+0.5\cdot[2+0.5\cdot 1]\}\\ &=\max\{2,2.75\}\\ &=\boxed{2.75}\\ V_{2}(warm)&=\max\{0.5\cdot[1+0.5\cdot 2]+0.5\cdot[1+0.5\cdot 1],\,1\cdot[-10+0.5\cdot 0]\}\\ &=\max\{1.75,-10\}\\ &=\boxed{1.75}\\ V_{2}(overheated)&=\max\{\}\\ &=\boxed{0}\end{aligned}
$$

|       | cool | warm | overheated |
| ----- | ---- | ---- | ---------- |
| $V_0$ | 0    | 0    | 0          |
| $V_1$ | 2    | 1    | 0          |
| $V_2$ | 2.75 | 1.75 | 0          |

值得注意的是, 任何终止状态的$V^*(s)$必须为0, 因为从终止状态无法采取任何动作来获取奖励.

#### 4.3.1 策略提取

回想一下, 我们解决MDP的最终目标是确定一个最优策略. 一旦使用称为**策略提取**的方法确定了所有状态的最优值, 就可以做到这一点. 策略提取背后的直觉非常简单: 如果你处于状态s, 你应该采取产生最大期望效用的动作a. 毫不奇怪, a是将我们带到具有最大Q值的Q状态的动作, 从而允许最优策略的正式定义:
$$
\forall s\in S,\,\pi^{*}(s)=\underset{a}{argmax}\,Q^{*}(s,a)=\underset{a}{argmax}\,\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]
$$
出于性能原因, 记住策略提取最好具有状态的最优Q值是有用的, 在这种情况下, 只需要一个argmax操作就可以确定状态的最优动作. 仅存储每个$V^{*}(s)$意味着我们必须在使用argmax之前用贝尔曼方程重新计算所有必要的Q值, 相当于执行深度为1的期望最大化.

#### 4.3.2 Q值迭代

在使用值迭代求解最优策略时, 我们首先找到所有最优值, 然后使用策略提取提取策略. 然而, 你可能已经注意到, 我们还处理了另一种编码最优策略信息的值: Q值.

Q值迭代是一种计算有限时域Q值的动态规划算法. 它由以下方程描述:
$$
Q_{k+1}(s,a)\leftarrow\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q_{k}(s^{\prime},a^{\prime})]
$$
注意, 这个更新规则与值迭代的更新规则只有轻微修改. 实际上, 唯一的真正区别是动作上的max操作符的位置发生了变化, 因为我们在状态中选择动作后才转换, 但在Q状态中转换后才选择新动作. 一旦我们有了每个状态和动作的最优Q值, 我们就可以通过简单地选择具有最高Q值的动作为状态找到策略.

### 4.4 策略迭代

值迭代可能相当慢. 在每次迭代中, 我们必须更新所有$|S|$个状态的值 (其中$|n|$表示基数操作符), 每个状态都需要迭代所有$|A|$个动作, 因为我们计算每个动作的Q值. 这些Q值的计算又需要再次迭代每个$|S|$个状态, 导致时间复杂度较差, 为$O\left(|S|^{2}|A|\right)$. 此外, 当我们只想确定MDP的最优策略时, 值迭代往往会进行大量过度计算, 因为通过策略提取计算的策略通常比状态值本身收敛得快得多. 解决这些缺陷的方法是使用**策略迭代**作为替代方案, 这是一种在保持值迭代最优性的同时提供显著性能提升的算法. 策略迭代操作如下:

1. 定义一个初始策略. 这可以是任意的, 但初始策略越接近最终最优策略, 策略迭代收敛得越快.
2. 重复以下步骤直至收敛:

- 使用**策略评估**评估当前策略. 对于策略$\pi$, 策略评估意味着计算所有状态s的$V^{\pi}(s)$, 其中$V^{\pi}(s)$是遵循$\pi$时从状态s开始的期望效用: $V^{\pi}(s)=\sum_{s^{\prime}}T(s,\pi(s), s^{\prime})\left[R\left(s,\pi(s), s^{\prime}\right)+\gamma V^{\pi}\left(s^{\prime}\right)\right]$. 将策略迭代的第i次迭代的策略定义为 $\pi_{i}$. 由于我们为每个状态固定了一个单一动作, 我们不再需要 max 操作符, 这实际上给我们留下了由上述规则生成的$|S|$个方程的系统 (方程变量只有 $V^{\pi}(s_1), V^{\pi}(s_2)...$). 然后可以通过简单地解这个系统来计算每个 $V^{\pi_{i}}(s)$. 或者, 我们也可以像值迭代中那样, 使用以下更新规则直至收敛来计算 $V^{\pi_{i}}(s)$: $V_{k+1}^{\pi_{i}}(s)\leftarrow\sum_{s^{\prime}}T(s,\pi_{i}(s),s^{\prime})[R(s,\pi_{i}(s),s^{\prime})+\gamma V_{k}^{\pi_{i}}(s^{\prime})]$. 然而, 第二种方法在实践中通常较慢.
- 一旦我们评估了当前策略, 使用**策略改进**生成一个更好的策略. 策略改进利用策略评估生成的状态值上的策略提取来生成这个新的改进策略: $\pi_{i+1}(s)=\underset{a}{\operatorname{argmax}}\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V^{\pi_{i}}\left(s^{\prime}\right)\right]$. 如果$\pi_{i+1}=\pi_{i}$, 算法已经收敛, 我们可以得出结论 $\pi_{i+1}=\pi_{i}=\pi^{*}$.

让我们最后一次通过我们的赛车例子 (已经厌倦了吗?) 来看看我们是否使用策略迭代得到与值迭代相同的策略. 回想一下, 我们使用的是折扣因子 $\gamma=0.5$. 我们以初始策略"总是慢速"开始:
![图1: 赛车例子](../0_Attachment/Pasted%20image%2020250517140907.png)

|           | cool | warm | overheated |
| --------- | ---- | ---- | ---------- |
| $\pi_{0}$ | slow | slow | —          |

由于终止状态没有输出动作, 任何策略都不能为其赋值. 因此, 可以合理地不考虑过热状态, 并简单地为任何终止状态s分配$\forall i, V^{\pi_{i}}(s)=0$. 下一步是在$\pi_{0}$上运行一轮策略评估:
$$
\begin{aligned}V^{\pi_{0}}(\text{ cool})&=1\cdot\left[1+0.5\cdot V^{\pi_{0}}(\text{ cool})\right]\\ V^{\pi_{0}}(\text{ warm})&=0.5\cdot\left[1+0.5\cdot V^{\pi_{0}}(\text{ cool})\right]\\ &+0.5\cdot\left[1+0.5\cdot V^{\pi_{0}}(\text{ warm})\right]\end{aligned}
$$
解这个关于$V^{\pi_{0}}(cool)$和$V^{\pi_{0}}(warm)$的方程组得到:

|               | cool | warm | overheated |
| ------------- | ---- | ---- | ---------- |
| $V^{\pi_{0}}$ | 2    | 2    | 0          |

我们现在可以使用这些值进行策略提取:
$$
\begin{aligned}\pi_{1}(cool)&=\operatorname*{argmax}\{slow:1\cdot[1+0.5\cdot 2],\,fast:0.5\cdot[2+0.5\cdot 2]+0.5\cdot[2+0.5\cdot 2]\}\\ &=\operatorname*{argmax}\{slow:2,\,fast:3\}\\ &=\boxed{fast}\\ \pi_{1}(warm)&=\operatorname*{argmax}\{slow:0.5\cdot[1+0.5\cdot 2]+0.5\cdot[1+0.5\cdot 2],\,fast:1\cdot[-10+0.5\cdot 0]\}\\ &=\operatorname*{argmax}\{slow:2,\,fast:-10\}\\ &=\boxed{slow}\end{aligned}
$$
运行第二轮策略迭代得到 $\pi_{2}(cool)=fast$和$\pi_{2}(warm)=slow.$ 由于这与$\pi_{1}$相同, 我们可以得出结论$\pi_{1}=\pi_{2}=\pi^{*}.$ 验证这一点作为练习!

|           | cool | warm |
| --------- | ---- | ---- |
| $\pi_{0}$ | slow | slow |
| $\pi_{1}$ | fast | slow |
| $\pi_{2}$ | fast | slow |

这个例子展示了策略迭代的真正威力: 仅通过两次迭代, 我们已经得出了赛车MDP的最优策略! 这比我们在同一MDP上运行值迭代时的情况要多, 后者在我们执行的两个更新后仍需要多次迭代才能收敛.

### 4.5 总结

上述材料有很多可能引起混淆的地方. 我们涵盖了值迭代, 策略迭代, 策略提取和策略评估, 所有这些看起来都很相似, 使用了贝尔曼方程, 但有细微的变化.

以下是每种算法目的的总结:

● **值迭代**: 用于通过迭代更新直至收敛来计算状态的最优值.

● **策略评估**: 用于计算特定策略下状态的值.

● **策略提取**: 用于在给定某些状态值函数的情况下确定策略. 如果状态值是最优的, 则该策略将是最优的. 此方法在运行值迭代以从最优状态值计算最优策略后使用, 或作为策略迭代中的子程序以计算当前估计状态值的最佳策略.

● **策略迭代**: 一种封装了策略评估和策略提取的技术, 用于迭代收敛到最优策略. 由于策略通常比状态值收敛得快得多, 它往往优于值迭代.

*PPT补充*

- **值迭代**和**策略迭代**计算的是相同的内容 (所有最优值) 
- **在值迭代中**:
  
  - 每次迭代都会同时更新状态值, 并 (隐式地) 更新策略
  - 我们不显式追踪策略, 而是通过对动作取最大值来隐式重新计算策略
- **在策略迭代中**: 先进行多轮策略评估 (每轮评估很快, 因为只需考虑固定策略下的单个动作) 
  - 策略评估完成后, 选择新策略 (此步骤类似值迭代过程, 速度较慢) 
  - 新策略必然更优 (否则算法终止)

## 5. 强化学习

### 5.1 强化学习

在前文中, 我们讨论了马尔可夫决策过程 (MDP), 并通过值迭代和策略迭代等技术求解最优状态值和提取最优策略. 求解MDP属于**离线规划**的范畴 — 智能体完全掌握**状态转移**函数和**奖励**函数的所有信息, 无需实际执行任何动作即可预先计算出 MDP 编码世界中的最优行动方案.

本节将讨论**在线规划**的场景: 智能体对世界的奖励和转移函数没有先验知识 (仍以MDP表示). 在在线规划中, 智能体必须进行**探索** — 通过执行动作并接收**反馈** (包括转移到的后继状态和相应奖励), 利用这些反馈通过**强化学习** (reinforcement learning, RL) 过程来估计最优策略, 最终基于该策略进行利用以实现奖励最大化.

![图1: 反馈循环](../0_Attachment/Pasted%20image%2020250517160439.png)

首先介绍基本术语. 在线规划的每个时间步中, 智能体从状态$s$出发, 执行动作$a$后转移到后继状态$s^{\prime}$并获得奖励$r$. 每个 $(s,a,s^{\prime},r)$ 元组称为一个**样本**. 通常智能体会持续执行动作并连续收集样本, 直到到达终止状态. 这样的样本序列称为一个**回合**. 在探索阶段, 智能体通常需要经历多个回合以收集足够的学习数据.

强化学习分为两类: **基于模型的学习**和**无模型学习**. 基于模型的学习试图利用探索获得的样本来估计状态转移函数和奖励函数, 然后通过常规的值迭代或策略迭代求解MDP; 而无模型学习则直接估计状态值或Q值, 无需构建MDP中奖励和转移函数的显式模型.

### 5.2 基于模型的学习

在基于模型学习中, 智能体通过统计每个Q状态$(s,a)$下转移到各状态$s^{\prime}$的次数, 构建转移函数的近似表示$\hat{T}\left(s, a, s^{\prime}\right)$. 具体方法是将观测到的元组$\left(s, a, s^{\prime}\right)$计数除以该Q状态$(s,a)$出现的总次数进行**归一化**, 使结果具有概率意义.

考虑以下MDP示例: 状态集$S=\{A, B, C, D, E, x\}$ (其中$x$为终止状态), 折扣因子$\gamma=1$:

![图1: MDP 示例](../0_Attachment/Pasted%20image%2020250517161123.png)

假设智能体按图示探索策略$\pi_{\text{explore}}$ (方向三角形表示指向的动作, 蓝色方块表示选择exit动作) 进行4个回合的探索, 得到以下结果:

![图2: 示例回合](../0_Attachment/Pasted%20image%2020250517161113.png)

现共收集12个样本 (每回合3个), 统计如下:

| s    | a     | s'   | count |
| ---- | ----- | ---- | ----- |
| A    | exit  | x    | 1     |
| B    | east  | C    | 2     |
| C    | east  | A    | 1     |
| C    | east  | D    | 3     |
| D    | exit  | x    | 3     |
| E    | north | C    | 2     |

根据$T\left(s, a, s^{\prime}\right)=P\left(s^{\prime}\mid a, s\right)$的定义, 可通过计数归一化估计转移函数, 并直接从探索获得的奖励构建奖励函数:

**转移函数**: $\hat{T}(s,a,s^{\prime})$
$$
\begin{aligned}
\hat{T}(A,exit, x)&=\frac{\#(A,exit, x)}{\#(A,exit)}=\frac{1}{1}= 1\\ 
\hat{T}(B,east, C)&=\frac{\#(B,east, C)}{\#(B,east)}=\frac{2}{2}= 1\\ 
\hat{T}(C,east, A)&=\frac{\#(C,east, A)}{\#(C,east)}=\frac{1}{4}= 0.25\\ 
\hat{T}(C,east, D)&=\frac{\#(C,east, D)}{\#(C,east)}=\frac{3}{4}= 0.75\\ 
\hat{T}(D,exit, x)&=\frac{\#(D,exit, x)}{\#(D,exit)}=\frac{3}{3}= 1\\ 
\hat{T}(E,north, C)&=\frac{\#(E,north, C)}{\#(E,north)}=\frac{2}{2}= 1
\end{aligned}
$$
**奖励函数**: $\hat{R}(s,a,s^{\prime})$
$$
\begin{aligned}
\hat{R}(A,exit, x)&=-10\\ 
\hat{R}(B,east, C)&=-1\\ 
\hat{R}(C,east, A)&=-1\\ 
\hat{R}(C,east, D)&=-1\\ 
\hat{R}(D,exit, x)&=+10\\ 
\hat{R}(E,north, C)&=-1
\end{aligned}
$$
根据**大数定律**, 随着探索回合增加, $\hat{T}$将收敛于真实$T$, $\hat{R}$也会通过新发现的$(s,a,s')$元组不断完善. 当训练达到预期时, 可终止探索阶段, 利用当前的$\hat{T}$和$\hat{R}$通过值迭代或策略迭代生成利用策略$\pi_{exploit}$, 使智能体转向以奖励最大化为目标的行动模式.

后文将讨论探索与利用时间的分配策略. 基于模型的学习虽然简单直观且高效 (仅需计数和归一化), 但维护所有$(s,a,s')$元组的计数可能带来存储开销. 下一节将介绍无模型学习方法, 通过避免显式计数来消除这种内存负担.

### 5.3 无模型学习

现在进入无模型学习! 存在多种无模型学习算法, 我们将介绍其中三种: 直接评估, 时序差分学习和Q学习. 直接评估和时序差分学习属于**被动强化学习**算法类别. 在被动强化学习中, 智能体遵循给定策略, 通过经历多个回合来学习该策略下各状态的价值 — 这与已知转移函数$T$和奖励函数$R$时MDP的策略评估过程完全一致. Q学习则属于第二类无模型学习算法, 称为**主动强化学习**, 学习过程中智能体可利用反馈持续更新策略, 最终通过充分探索得到最优策略.

#### 5.3.1 直接评估

第一种被动强化学习技术是**直接评估**, 其名称已暗示了它的简单直接. 该方法固定某个策略$\pi$, 让智能体遵循$\pi$经历多个回合. 智能体通过回合收集样本时, 会记录从各状态获得的总效用及访问次数. 任意时刻, 状态$s$的价值估计可通过总效用除以访问次数得到. 假设折扣因子$\gamma=1$, 我们通过示例说明:

![图1: 示例图片](../0_Attachment/Pasted%20image%2020250517163151.png)

在第一个回合中, 从状态$D$到终止状态获得总奖励$10$, 从状态$C$获得$(-1)+10=9$, 从状态$B$获得$(-1)+(-1)+10=8$. 完成该过程后, 各状态的总奖励及价值估计如下:

| s    | 总奖励 | 访问次数 | $V^{\pi}(s)$ |
| :--- | :----- | -------- | ------------ |
| A    | $-10$  | 1        | -10          |
| B    | 16     | 2        | 8            |
| C    | 16     | 4        | 4            |
| D    | 30     | 3        | 10           |
| E    | $-4$   | 2        | $-2$         |

尽管直接评估最终能学习各状态价值, 但因忽略状态转移信息, 收敛速度通常过慢.

![图2: 带标注的示例](../0_Attachment/Pasted%20image%2020250517163233.png)

例如, $V^{\pi}(E)=-2$而$V^{\pi}(B)=8$, 但根据贝尔曼方程, 两者应有相同价值 (因后继状态均为$C$且转移奖励均为$-1$). 差异仅因偶然的采样偏差导致, 这种问题可通过时序差分学习缓解.

#### 5.3.2 时序差分学习

时序差分学习 (Temporal Difference, TD学习) 的核心思想是**从每次经验中学习**, 而非像直接评估那样仅记录总奖励和访问次数. 策略评估中, 我们利用固定策略和贝尔曼方程生成的方程组来确定状态价值 (或通过类似值迭代的更新方式):
$$
V^{\pi}(s)=\sum_{s^{\prime}}T(s,\pi(s),s^{\prime})[R(s,\pi(s),s^{\prime})+\gamma V^{\pi}(s^{\prime})]
$$
TD学习的关键在于**在无权重的情况下计算加权平均**, 通过指数移动平均实现. 初始化 $\forall s,\,V^{\pi}(s)=0$ 后, 每个时间步智能体从状态$s$执行动作 $\pi(s)$, 转移到 $s^{\prime}$ 并获得奖励 $R(s,\pi(s),s^{\prime})$. 样本值计算为:
$$
\text{sample}=R(s,\pi(s),s^{\prime})+\gamma V^{\pi}\left(s^{\prime}\right)
$$
随后通过以下规则将样本融入现有模型:
$$
V^{\pi}(s)\leftarrow(1-\alpha)V^{\pi}(s)+\alpha\cdot{\text{sample}}
$$
其中$\alpha$ (学习率) 控制新旧样本的权重比例. 初始可设$\alpha=1$, 随后逐步衰减至0. 展开更新规则可见:
$$
V_{k}^{\pi}(s)\leftarrow\alpha\cdot[(1-\alpha)^{k-1}\cdot \text{sample}_{1}+...+\text{sample}_{k}]
$$
旧样本因$(1-\alpha)^n$衰减而被指数级降权, 这正是TD学习的优势:

- 每步即时学习, 利用状态转移信息更新 $V^{\pi}(s)$ 而不是放到最后才计算
- 旧样本自动降权
- 比直接评估更快收敛

#### 5.3.3 Q学习

直接评估和TD学习最终能学习策略下的真实状态价值, 但存在根本缺陷: 最优策略需要Q值知识. 传统方法需已知转移函数 $T$ 和奖励函数 $R$ 才能通过贝尔曼方程计算Q值:
$$
Q^{*}(s,a)=\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma V^{*}(s^{\prime})]
$$
Q学习通过**直接学习Q值**跳过该限制, 完全无需模型. 其更新规则 (Q值迭代) 为:
$$
Q_{k+1}(s,a)\leftarrow\sum_{s^{\prime}}T(s,a,s^{\prime})[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q_{k}(s^{\prime},a^{\prime})]
$$
实际中通过采样实现 (动作已确定. $s'$ 为该次动作的样本):
$$
\text{sample}=R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})
$$
并融入指数移动平均:
$$
Q(s,a)\leftarrow(1-\alpha)Q(s,a)+\alpha\cdot\text{sample}
$$
只要我们投入足够的时间进行探索, 并以适当的速度降低学习率 $\alpha$, Q学习就能为每个Q状态找到最优的Q值. 这正是Q学习的革命性所在 — 时序差分学习 (TD学习) 和直接评估法需要先遵循策略来学习策略下状态的价值, 再通过其他技术判断策略的最优性; 而Q学习即使采取次优或随机行动, 也能直接学习到最优策略. 这种方法被称为**离策略学习** (与直接评估法和TD学习这类**同策略学习**形成对比).

Q学习是**离策略学习**的典范 — 即使采取次优动作也能学习最优策略, 这是其革命性所在.

#### 5.3.4 近似Q学习

Q学习是一种卓越的学习技术, 至今仍是强化学习领域发展的核心. 然而, 它仍有改进的空间. 目前, Q学习仅以表格形式存储所有状态的 Q 值, 但考虑到强化学习的大多数应用场景涉及数千甚至数百万个状态, 这种存储方式效率并不高. 这意味着在训练过程中我们无法遍历所有状态, 而且即便能够遍历, 也会因内存限制而无法存储所有 Q 值.

![图3.1](../0_Attachment/Pasted%20image%2020250517162911.png)
![图3.2](../0_Attachment/Pasted%20image%2020250517162915.png)
![图3.3](../0_Attachment/Pasted%20image%2020250517162917.png)

如图, 如果Pacman在运行基础Q学习后发现图3.1的情况不利, 它仍然不会意识到图3.2甚至图3.3的情况同样不利. **近似Q学习**试图通过掌握少量通用场景并推演至众多相似情境来解决这一问题. 实现学习经验泛化的关键在于**基于特征的状态表示法** — 该方法将每个状态表征为一个被称为**特征向量**的多维数组. 例如, Pacman的特征向量可能编码为:

- 最近幽灵距离
- 最近食物距离
- 幽灵数量
- 是否被困 (0/1) 

此时状态价值和Q值表示为**线性函数**:
$$
V(s)=\vec{w}\cdot\vec{f}(s)=\sum_{i}w_{i}f_{i}(s) \\
Q(s,a)=\vec{w}\cdot\vec{f}(s,a)=\sum_{i}w_{i}f_{i}(s,a)
$$
定义**差异值**:
$$
\text{difference}=\left[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\right]-Q(s,a)
$$
权重更新规则为:
$$
w_{i}\leftarrow w_{i}+\alpha\cdot\text{difference}\cdot f_{i}(s,a)
$$
不同于为每个状态单独存储 Q 值, 近似Q学习只需存储一个权重向量, 即可按需实时计算 Q 值. 这不仅使Q学习具备了更强的泛化能力, 还大幅提升了内存使用效率.

关于Q学习的最后一点说明是, 我们可以用**差值**的形式重新表述精确Q学习的更新规则如下:
$$
Q(s,a)\leftarrow Q(s,a)+\alpha\cdot\text{difference}
$$
这第二种表示方式为我们提供了对更新规则略有不同但同样有价值的解读: 它计算采样估计值与当前 $Q(s,a)$ 模型之间的差值, 并按差值大小成比例地调整模型, 使其向估计值方向偏移.

### 5.4 探索与利用

目前我们已经介绍了多种智能体学习最优策略的方法, 并强调了"充分探索"的必要性, 但尚未具体说明"充分"的含义. 接下来我们将讨论两种分配探索与利用时间的方法: $\epsilon$-贪心策略和探索函数.

#### 5.4.1 $\epsilon$-贪心策略

采用$\epsilon$-贪心策略的智能体会设定一个概率值$0\leq\epsilon\leq 1$, 以$\epsilon$的概率随机行动进行探索, 同时以$(1-\epsilon)$的概率遵循当前既定策略进行利用. 这种策略实现简单但存在调控难点: 若$\epsilon$取值过大, 即使在学习到最优策略后, 智能体仍会保持随机行为; 反之若$\epsilon$过小, 则探索频率不足, 导致Q学习 (或其他学习算法) 收敛速度缓慢. 解决方法是通过人工调整并随时间逐步降低$\epsilon$值.

#### 5.4.2 探索函数

探索函数通过改进的Q值迭代更新机制, 自动赋予访问频次较低的状态优先权, 从而避免人工调整$\epsilon$的问题. 改进后的更新公式为:
$$
Q(s,a)\leftarrow(1-\alpha)Q(s,a)+\alpha\cdot[R(s,a,s^{\prime})+\gamma\max_{a^{\prime}}f(s^{\prime},a^{\prime})]
$$
其中$f$表示探索函数. 探索函数的设计具有灵活性, 常见形式为:
$$
f(s,a)=Q(s,a)+\frac{k}{N(s,a)}
$$
这里$k$为预设常数, $N(s,a)$表示Q状态$(s,a)$的访问次数. 智能体在状态$s$时总是选择当前$f(s,a)$值最高的动作, 从而无需概率性地决定探索或利用. 探索行为通过探索函数自动实现 — 因为$\frac{k}{N(s, a)}$项会给访问频次低的动作提供足够"奖励值", 使其可能超越高Q值动作被选中. 随着状态访问频次增加, 该项奖励会逐渐趋近于0, $f(s, a)$最终回归到$Q(s,a)$, 使策略逐渐转向完全利用.

### 5.5 本章总结

必须牢记强化学习建立在马尔可夫决策过程 (MDP) 基础上, 其目标是通过推导最优策略来解决MDP问题. 与价值迭代, 策略迭代等方法不同, 强化学习在未知转移函数$T$和奖励函数$R$的情况下, 要求智能体通过在线试错而非离线计算来学习最优策略. 主要方法包括:

● 基于模型的学习: 通过计算估计转移函数$T$和奖励函数$R$的值, 并基于这些估计值运用价值迭代或策略迭代等MDP求解方法.

● 无模型学习: 避免估计$T$和$R$, 直接通过其他方法估计状态值或Q值:

- 直接评估: 遵循策略$\pi$, 简单统计各状态获得的总奖励及访问次数. 当采样足够时, 虽然收敛速度慢且忽略状态转移信息, 但能逼近$\pi$下的真实状态值.
- 时序差分学习 (TD学习): 遵循策略$\pi$, 采用指数移动平均与采样值相结合的方式, 最终收敛到$\pi$下的真实状态值. TD学习与直接评估都属于同策略学习, 即在评估当前策略价值后再决定是否更新策略.

Q学习: 通过Q值迭代更新直接通过试错学习最优策略, 属于异策略学习的典型案例 — 即使在采取次优动作时仍能学习最优策略.

近似Q学习: 原理与Q学习相同, 但采用基于特征的状态表示来实现泛化学习.

● 为量化不同强化学习算法的性能, 我们使用遗憾值 (Regret) 概念. 遗憾值反映了从初始时刻就采取最优策略获得的总奖励与实际学习算法累计获得总奖励之间的差异.

## 6. 贝叶斯网络

### 6.1 概率论回顾

我们假设您已在CS70课程中学习过概率论基础, 因此本节笔记默认您已掌握概率论的基本概念, 如概率密度函数 (PDF), 条件概率, 独立性和条件独立性. 以下是我们将使用的概率规则摘要.

**随机变量**表示结果未知的事件. **概率分布**是对结果赋予权重的过程, 需满足以下条件:
$$
\begin{array}{l}
0 \leq P(\omega) \leq 1 \\
\sum_{\omega} P(\omega) = 1
\end{array}
$$
例如, 若A是二元变量 (只能取两个值), 则 $P(A=0)=p$ 且 $P(A=1)=1-p$, 其中 $p \in [0,1]$

我们约定: 大写字母表示随机变量, 小写字母表示该变量的具体取值

符号 $P(A, B, C)$ 表示变量A, B, C的**联合分布**. 联合分布中顺序无关, 即 $P(A, B, C) = P(C, B, A)$

可通过**链式法则** (The Chain Rule) (也称乘积法则, The Product Rule) 展开联合分布:
$$
\begin{array}{l}
P(A, B) = P(A \mid B)P(B) = P(B \mid A)P(A) \\
P(A_1, A_2, \ldots, A_k) = P(A_1)P(A_2 \mid A_1) \ldots P(A_k \mid A_1, \ldots, A_{k-1})
\end{array}
$$
通过**边缘化** (或称"求和消元") 可获得A, B的边缘分布:
 $P(A, B) = \sum_c P(A, B, C=c)$
 A的边缘分布也可表示为:
 $P(A) = \sum_b \sum_c P(A, B=b, C=c)$

对概率分布进行操作时, 结果可能未归一化 (总和不为1). 此时需进行**归一化**: 用分布中所有条目之和除以每个条目.

**条件概率**表示在已知某些事实下事件的概率. 例如 $P(A \mid B=b)$ 表示已知B取值为b时A的概率分布, 定义为:
$$
P(A \mid B) = \frac{P(A, B)}{P(B)}
$$
结合条件概率定义与链式法则, 可得**贝叶斯规则**:
$$
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}
$$
若随机变量A与B**相互独立**, 记为 $A \perp\!\!\!\perp B$ (等价于 $B \perp\!\!\!\perp A$). 此时满足:
$$
P(A, B) = P(A)P(B)
$$
例如两次独立的硬币抛掷. 根据上式与链式法则可推导出 $P(A \mid B) = P(A)$ 和 $P(B \mid A) = P(B)$.

若随机变量A与B在给定变量C时**条件独立**, 记为 $A \perp\!\!\!\perp B \mid C$ (等价于 $B \perp\!\!\!\perp A \mid C$). *(举例: 当有蛀牙时, 牙疼与检测出蛀牙是独立的)* 此时满足:
$$
P(A, B \mid C) = P(A \mid C)P(B \mid C)
$$
这意味着若已知C的取值, 则A与B互不影响. 等价关系还包括:
$$
\begin{array}{l}
P(A \mid B, C) = P(A \mid C) \\
P(B \mid A, C) = P(B \mid C)
\end{array}
$$
注意这三个等式与相互独立等式的形式一致, 仅增加了对C的条件依赖!

### 6.2 概率推断

在人工智能中, 我们常需建模非确定性事件间的关系. 例如:

- 若天气预报降雨概率40%, 我该带伞吗?
- 若冰淇淋球数越多越可能掉落, 我该买几球?
- 若15分钟前我的行车路线上发生事故, 该现在出发还是30分钟后?

这些问题 (及更多) 都可通过**概率推断**解决.

此前课程中, 我们将世界建模为始终已知的特定状态. 接下来我们将采用新模型: 每个可能的世界状态都有其概率. 例如构建天气模型, 其状态包含季节, 温度和天气, 可能给出 $P(\text{winter}, 35^\circ, \text{cloudy}) = 0.023$, 表示"冬季, 35度, 多云"这一具体结果的概率.

更准确地说, 我们的模型是**联合分布** — 即描述每个可能**结果** (变量**赋值**) 概率的表格. 例如下表:

| 季节 S | 温度 T | 天气 W | 概率 P |
| :----- | :----- | ------ | ------ |
| summer | hot    | sun    | 0.30   |
| summer | hot    | rain   | 0.05   |
| summer | cold   | sun    | 0.10   |
| summer | cold   | rain   | 0.05   |
| winter | hot    | sun    | 0.10   |
| winter | hot    | rain   | 0.05   |
| winter | cold   | sun    | 0.15   |
| winter | cold   | rain   | 0.20   |

此模型可回答我们感兴趣的问题, 例如:

1. 晴天概率是多少? $P(W=\text{sun})$
2. 已知冬季时的天气概率分布? $P(W \mid S=\text{winter})$
3. 已知寒冷且雨天时是冬季的概率? $P(S=\text{winter} \mid T=\text{cold}, W=\text{rain})$
4. 已知寒冷时的季节与天气联合分布? $P(S, W \mid T=\text{cold})$

#### 枚举推断 

给定联合PDF, 我们可通过**枚举推断** (inference by enumeration, IBE) 计算任意概率分布 $P(Q_1 \ldots Q_m \mid e_1 \ldots e_n)$. 该过程涉及三类变量:

1. **查询变量** $Q_i$: 未知变量, 出现在条件符号左侧
2. **证据变量** $e_i$: 已知观测变量, 出现在条件符号右侧
3. **隐藏变量**: 存在于联合分布但不在目标分布中的变量

枚举推断算法步骤如下:

1. 筛选所有与证据变量一致的观测行
2. 对隐藏变量求和消元 (边缘化)
3. 归一化表格使其成为概率分布 (总和为1)

例如计算 $P(W \mid S=\text{winter})$ 时:	

1. 选择季节为winter的4行
2. 对温度T求和消元并归一化, 得到:

| W    | S      | 未归一化和     | 概率                   |
| ---- | ------ | -------------- | ---------------------- |
| sun  | winter | 0.10+0.15=0.25 | $0.25/(0.25+0.25)=0.5$ |
| rain | winter | 0.05+0.20=0.25 | $0.25/(0.25+0.25)=0.5$ |

因此 $P(W=\text{sun} \mid S=\text{winter}) = 0.5$, $P(W=\text{rain} \mid S=\text{winter}) = 0.5$, 表明冬季晴天和雨天的概率均为50%.

只要拥有联合PDF表, 枚举推断可计算任意概率分布 (包括多查询变量情形).

### 6.3 贝叶斯网络表示法

虽然枚举推理可以计算我们所需的任何查询概率, 但在计算机内存中表示整个联合分布对于实际问题是不现实的 — 如果我们要表示的 $n$ 个变量每个都可以取 $d$ 个可能的值 (其**定义域**大小为 $d$), 那么我们的联合分布表将有 $d^{n}$ 个条目, 这个数量随变量数量呈指数级增长, 存储起来非常不切实际!

贝叶斯网络通过利用条件概率的思想避免了这个问题. 概率信息不是存储在一个巨大的表中, 而是分布在多个较小的条件概率表 (conditional probability table, CPT) 中, 同时用一个**有向无环图** (directed acyclic graph, DAG) 来捕捉变量之间的关系. 局部概率表和 DAG 共同编码了足够的信息, 可以计算我们原本需要整个大型联合分布才能得到的任何概率分布. 我们将在下一节中看到这是如何实现的.

我们正式定义贝叶斯网络由以下部分组成:

1. 一个有向无环图, 其中每个节点对应一个变量 $X$.
2. 每个节点的条件分布 $P(X \mid A_1...A_n),$ 其中 $A_i$ 是 $X$ 的第 $i$ 个父节点, 存储为条件概率表 (CPT). 每个CPT有 $n+2$ 列: 一列对应每个父变量 $A_1...A_n$的值, 一列对应 $X$ 的值, 还有一列对应 $X$ 给定其父节点的条件概率.

贝叶斯网络图的结构编码了不同节点之间的条件独立关系. 这些条件独立性使我们能够存储多个小表而不是一个大表.

重要的是, 贝叶斯网络节点之间的边并不意味着这些节点之间存在特定的**因果**关系, 或者变量必然相互依赖. 它只是表示节点之间可能存在**某种**关系.

作为贝叶斯网络的一个例子, 考虑一个模型, 其中有五个二元随机变量如下:

- B: 发生入室盗窃.
- A: 警报响起.
- E: 发生地震.
- J: 约翰打电话.
- M: 玛丽打电话.

假设警报可能在入室盗窃或地震发生时响起, 而玛丽和约翰如果听到警报会打电话. 我们可以用下图表示这些依赖关系.

![图1: 基础贝叶斯网络样例](../0_Attachment/Pasted%20image%2020250725231717.png)

在这个贝叶斯网络中, 我们将存储概率表 $P(B)$, $P(E)$, $P(A|B,E)$, $P(J|A)$ 和 $P(M|A)$.

给定图的所有CPT, 我们可以使用以下规则计算给定赋值的概率:

$P(X1,X2,...,Xn)=\prod_{i=1}^{n}P(X_{i}|parents(X_{i}))$

对于上面的警报模型, 我们实际上可以如下计算联合概率:

$P(-b,-e,+a,+j,-m)=P(-b)\cdot P(-e)\cdot P(+a|-b,-e)\cdot P(+j|+a)\cdot P(-m|+a)$

我们将在下一节中看到这个关系是如何成立的.

作为现实检验, 重要的是要理解贝叶斯网络只是一种模型. 模型试图捕捉世界运作的方式, 但由于它们总是简化, 所以总是错误的. 然而, 通过良好的建模选择, 它们仍然可以成为足够好的近似, 对解决现实世界中的实际问题有用.

一般来说, 一个好的模型可能不会考虑每个变量, 甚至不会考虑变量之间的每个交互. 但是通过在图的结构中做出建模假设, 我们可以产生极其高效的推理技术, 这些技术通常比枚举推理等简单程序更实用.

### 6.4 贝叶斯网络的结构

在本课程中, 我们将参考两条可以通过查看贝叶斯网络的图形结构推断出的贝叶斯网络独立性规则:

- **每个节点在给定其所有父节点的情况下, 与图中所有其祖先节点 (非后代) 条件独立.** ![图1: 父节点](../0_Attachment/Pasted%20image%2020250727004041.png)
- **每个节点在给定其马尔可夫覆盖的情况下, 与所有其他变量条件独立.** 一个变量的马尔可夫覆盖包括其父节点, 子节点以及子节点的其他父节点. ![图2: 马尔可夫覆盖](../0_Attachment/Pasted%20image%2020250727004151.png)

使用这些工具, 我们可以回到上一节的断言: 我们可以通过连接贝叶斯网络的CPT来获得所有变量的联合分布.

$P(X_{1},X_{2},\ldots,X_{n})=\prod_{i=1}^{n}P(X_{i}\mid\text{parents}(X_{i}))$

联合分布与贝叶斯网络的CPT之间的这种关系之所以成立, 是因为图给出的条件独立关系. 我们将通过一个例子来证明这一点.

让我们重新审视前面的例子. 我们有CPT $P(B), P(E), P(A\mid B, E), P(J\mid A)$ 和 $P(M\mid A)$, 以及下图:

![图3: 基础贝叶斯网络样例](../0_Attachment/Pasted%20image%2020250725231717.png)

对于这个贝叶斯网络, 我们试图证明以下关系:

$P(B,E,A,J,M)=P(B)P(E)P(A\mid B,E)P(J\mid A)P(M\mid A)$

我们可以用另一种方式展开联合分布: 使用链式法则. 如果我们用拓扑排序 (父节点在子节点之前) 展开联合分布, 我们得到以下方程:

$P(B, E, A, J, M)= P(B) P(E\mid B) P(A\mid B, E) P(J\mid B, E, A) P(M\mid B, E, A, J)$

注意, 在第一个方程中, 每个变量都表示在一个CPT $P(var\mid Parents(var))$ 中, 而在第二个方程中, 每个变量都表示在一个CPT $P(\operatorname{var}\mid\operatorname{Parents}(\operatorname{var}),\operatorname{Ancestors}(\operatorname{var}))$ 中.

我们依赖于上面的第一个条件独立关系, 即在给定所有父节点的情况下, 每个节点与图中所有其祖先节点条件独立. (在其他地方, 这个假设可能被定义为"一个节点在给定其父节点的情况下, 与其非后代节点条件独立". 我们总是希望做出尽可能最小的假设并证明我们所需的内容, 因此我们将采用祖先节点的假设.)

因此, 在贝叶斯网络中, $P(\operatorname{var}\mid\operatorname{Parents}(\operatorname{var}),\operatorname{Ancestors}(\operatorname{var}))=P(\operatorname{var}\mid\operatorname{Parents}(\operatorname{var}))$, 所以这两个方程是相等的. 贝叶斯网络中的条件独立性允许多个较小的条件概率表来表示整个联合概率分布.

### 6.5 D-分离

在分析随机变量时, 一个关键问题是判断变量之间是否独立或条件独立. 贝叶斯网络通过其拓扑结构为我们提供了一种快速判断这些关系的方法.

我们已知**在给定父节点的情况下, 任意节点都与其所有祖先节点条件独立**.

接下来我们将展示三种经典的三节点两边的贝叶斯网络结构 (三元组) 及其表达的条件独立关系.

#### 6.5.1 因果链

![图1: 无观测的因果链](../0_Attachment/Pasted%20image%2020250727105637.png)

![图2: 已观测的因果链](../0_Attachment/Pasted%20image%2020250727105642.png)

图1展示了一个**因果链**结构, 其联合概率分布表示为:
$$
P(x,y,z)=P(z|y)P(y|x)P(x)
$$
需要注意的是, $X$ 和 $Z$ 并不保证独立. 例如:
$$
P(y|x) = \begin{cases} 1 & \text{if } x = y \\ 0 & \text{else } \end{cases}
\\
P(z|y) = \begin{cases} 1 & \text{if } z = y \\ 0 & \text{else } \end{cases}
$$

此时 $P(z\mid x)=1$ 当且仅当 $x=z$, 说明 $X$ 和 $Z$ 不独立

但在观测Y的情况下 (图2), 有 $X \perp\!\!\!\perp Z\mid Y$, 即:
$$
P(X|Z,Y)=P(X|Y)
$$
证明如下:
$$
P(X | Z, y) = \frac{P(X, Z, y)}{P(Z, y)} = \frac{P(Z|y) P(y|X) P(X)}{\sum_{x} P(x, y, Z)} = \frac{P(Z|y) P(y|X) P(X)}{P(Z|y) \sum_{x} P(y|x)P(x)} = \frac{P(y|X) P(X)}{\sum_{x} P(y|x)P(x)} = P(X|y)
$$

#### 6.5.2 共同原因

![图3: 无观测的共同原因](../0_Attachment/Pasted%20image%2020250727105731.png)

![图4: 已观测的共同原因](../0_Attachment/Pasted%20image%2020250727105740.png)

**共同原因**结构的联合分布为:
$$
P(x, y, z) = P(x|y)P(z|y)P(y)
$$

就像因果链一样, 我们可以通过以下反例来证明 $X$ 不能保证独立于 $Z$:
$$
P(x|y) = \begin{cases} 1 & \text{if } x = y \\ 0 & \text{else } \end{cases}
\\
P(z|y) = \begin{cases} 1 & \text{if } z = y \\ 0 & \text{else } \end{cases}
$$

但观测到Y时 (图4) 有 $X \perp\!\!\!\perp Z | Y$:
$$
P(X | Z, y) = \frac{P(X, Z, y)}{P(Z, y)} = \frac{P(X|y) P(Z|y) P(y)}{P(Z|y) P(y)} = P(X|y)
$$

#### 6.5.3 共同效应

![图5: 无观测的共同效应](../0_Attachment/Pasted%20image%2020250727105820.png)

![图6: 已观测的共同效应](../0_Attachment/Pasted%20image%2020250727105855.png)

共同效应结构的联合分布为:
$$
P(x,y,z)=P(y|x,z)P(x)P(z)
$$

在未观测Y时 (图5), X和Z独立: $X \perp\!\!\!\perp Z$. 但观测Y或其子代时 (图6), 独立性可能被破坏. 例如当 $X$, $Z$ 为二元变量且以相等的概率为真或假:
$$
P(X=true) = P(X=false) = 0.5
\\
P(Z=true) = P(Z=false) = 0.5
$$
$Y$ 表示两者是否相等:
$$
P(Y | X, Z) = \begin{cases} 1 & \text{if } X = Z \text{ and } Y = true \\ 1 & \text{if } X \ne Z \text{ and } Y = false \\ 0 & \text{else} \end{cases}
$$
如果 $Y$ 未被观测, $X$ 和 $Z$ 是独立的. 但如果 $Y$ 被观测, 那么知道 $X$ 的值就等于知道 $Z$的值, 反之亦然. 因此, 给定 $Y$时, $X$ 和 $Z$ 不是条件独立的.

共同效应可被视为与因果链和共同原因"相反"——当不对 $Y$ 进行条件化时, $X$ 和 $Z$ 必然独立. 但当对 $Y$ 进行条件化时, $X$ 和 $Z$ 是否相关取决于 $P(Y∣X,Z)$ 的具体概率值.

同样的逻辑适用于对图中 $Y$ 的后代节点进行条件化的情况. 如图7所示, 若观测到 $Y$ 的某个后代节点, 则 $X$ 和 $Z$ 不能保证是独立的.

![图7: 后代节点被观测的共同效应](../0_Attachment/Pasted%20image%2020250727105928.png)

#### 6.5.4 一般情况与D-分离算法

我们可以将前三种情况作为基础模块, 用于回答具有超过三个节点和两条边的任意贝叶斯网络中的条件独立性问题. 问题形式化表述如下:

**给定一个贝叶斯网络 $G$、两个节点 $X$ 和 $Y$, 以及表示观测变量的 (可能为空的) 节点集合 $\{Z_1,\ldots,Z_k\}$, 以下陈述是否必然成立: $X \perp\!\!\!\perp Y \mid \{Z_1,\ldots,Z_k\}$ ?**

**D-分离** (有向分离) 是贝叶斯网络图结构的一种性质, 它隐含了这种条件独立关系, 并推广了我们之前讨论的案例. 如果变量集合 $Z_1,\ldots,Z_k$ d-分离了 $X$ 和 $Y$, 则在所有可由该贝叶斯网络编码的概率分布中, 均有 $X \perp\!\!\!\perp Y \mid \{Z_1,\ldots,Z_k\}$.

我们首先介绍一个基于节点 $X$ 到 $Y$ 可达性概念的算法 (**注意: 该算法不完全正确, 稍后将修正**):

1. 在图中标记所有观测节点 $\{Z_1,\ldots,Z_k\}$ (用阴影表示).
2. 如果存在一条从 $X$ 到 $Y$ 的未被阴影节点阻塞的无向路径, 则 $X$ 和 $Y$ 是"连通的".
3. 若 $X$ 和 $Y$ 连通, 则它们在给定 $\{Z_1,\ldots,Z_k\}$ 下不条件独立；否则条件独立.

然而, 此算法仅在贝叶斯网络中不存在**共同效应** (Common Effect) 结构时有效, 因为若存在此类结构, 当共同效应中的节点 $Y$ 被激活 (观测) 时, 两个节点会变得"可达". 为此, 我们修正得到以下**d-分离算法**:

1. 标记所有观测节点 $\{Z_1,\ldots,Z_k\}$.

2. 枚举从 $X$ 到 $Y$ 的所有无向路径.

3. 对每条路径
   1. 将路径分解为三元组 (连续的3个节点段).

   2. 若所有三元组均为活跃的, 则该路径活跃并d-连接 $X$ 和 $Y$.

4. 若不存在d-连接 $X$ 和 $Y$ 的路径, 则 $X \perp\!\!\!\perp Y \| \{Z_1,\ldots,Z_k\}$

图中的任何路径均可分解为一系列由3个连续节点和2条边组成的**三元组**. 三元组的活跃性取决于其中间节点是否被观测. 若一条路径中所有三元组均活跃, 则该路径活跃并d-连接 $X$ 和 $Y$, 意味着在给定观测变量下 $X$ 不保证与 $Y$ 条件独立. 若所有路径均不活跃, 则 $X$ 和 $Y$ 在给定观测变量下条件独立.

活跃三元组: ![图8](../0_Attachment/Pasted%20image%2020250727110250.png)

非活跃三元组: ![图9](../0_Attachment/Pasted%20image%2020250727110308.png)

#### 6.5.5 应用示例

![图10](../0_Attachment/Pasted%20image%2020250727110429.png)

- $R \perp\!\!\!\perp B$ - 成立
- $R \perp\!\!\!\perp B | T$ - 不保证成立
- $R \perp\!\!\!\perp B | T'$ - 不保证成立
- $R \perp\!\!\!\perp T' | T$ - 成立

![图11](../0_Attachment/Pasted%20image%2020250727110434.png)

- $L \perp\!\!\!\perp T' | T$ - 成立
- $L \perp\!\!\!\perp B$ - 成立
- $L \perp\!\!\!\perp B | T$ - 不保证成立
- $L \perp\!\!\!\perp B | T'$ - 不保证成立
- $L \perp\!\!\!\perp B | T,R$ - 成立

![图12](../0_Attachment/Pasted%20image%2020250727110442.png)

- $T \perp\!\!\!\perp D$ - 不保证成立
- $T \perp\!\!\!\perp D | R$ - 成立
- $T \perp\!\!\!\perp D | R,S$ - 不保证成立

### 6.6 贝叶斯网络中的精确推理

推理是指寻找某个概率分布 $P\left(Q_{1}\ldots Q_{k}\mid e_{1}\ldots e_{k}\right)$ 值的问题, 正如笔记开头概率推理部分所述. 给定一个贝叶斯网络, 我们可以通过形成联合 PDF 并使用枚举推理来朴素地解决这个问题. 这需要创建并遍历一个指数级大的表格.

#### 6.6.1 变量消除

另一种方法是逐个消除隐藏变量. 为了**消除**变量 $X$, 我们:

1. 连接 (相乘) 所有涉及 $X$ 的因子.
2. 对X进行求和.

**因子**简单地定义为**未归一化的概率**. 在变量消除的整个过程中, 每个因子将与它所对应的概率成比例, 但每个因子的潜在分布不一定像概率分布那样总和为1 (参考 PPT 的 Factor Zoo). 变量消除的伪代码如下:

![图1: 变量消除](../0_Attachment/Pasted%20image%2020250731111425.png)

让我们通过一个例子来具体说明这些概念. 假设我们有一个如下所示的模型, 其中 $T$, $C$, $S$ 和 $E$ 可以取二进制值. 这里, $T$ 表示冒险者拿走宝藏的概率, $C$ 表示冒险者拿走宝藏时笼子掉落的概率, $S$ 表示冒险者拿走宝藏时蛇被释放的概率, $E$表示根据笼子和蛇的状态信息, 冒险者逃脱的概率.

![图2: 变量消除](../0_Attachment/Pasted%20image%2020250731111445.png)

在这种情况下, 我们有因子 $P(T), P(C\mid T), P(S\mid T)$ 和 $P(E\mid C, S)$. 假设我们想计算 $P(T\mid+e)$. 枚举推理的方法是形成一个16行的联合PDF $P(T, C, S, E)$, 然后只选择与 +e 对应的行, 对 $C$ 和 $S$ 进行求和, 最后归一化.

另一种方法是逐个消除 $C$ 和 $S$. 我们将按以下步骤进行:

1. 连接 (相乘) 所有涉及C的因子, 形成 $f_{1}(C,+e, T, S)=P(C\mid T)\cdot P(+e\mid C, S)$. 有时这被写作 $P(C,+e\mid T, S)$

2. 从这个新因子中对C进行求和, 留下一个新因子 $f_{2}(+e, T, S)$, 有时写作 $P(+e\mid T, S)$
3. 连接所有涉及S的因子, 形成 $F_{3}(+e, S, T)=P(S\mid T)\cdot f_{2}(+e, T, S)$, 有时写作 $P(+e, S\mid T)$
4. 对S进行求和, 得到 $f_{4}(+e, T)$, 有时写作 $P(+e\mid T)$
5. 连接剩余的因子, 得到 $f_{5}(+e, T)=f_{4}(+e, T)\cdot P(T)$

6. 一旦我们有了 $f_{5}(+e, T)$, 我们就可以通过归一化轻松计算 $P(T\mid+e)$


在编写连接得到的因子时, 我们可以使用因子表示法, 如 $f_{1}(C,+e, T, S)$, 它忽略条件栏, 并简单地列出该因子中包含的变量.

或者, 我们可以写作 $P(C,+e\mid T, S)$, 即使这不能保证是一个有效的概率分布 (例如所有行概率和不为1). 为了机械地推导这个表达式, 注意原始因子中条件条左侧的所有变量 (这里是 $P(C\mid T)$ 中的C和 $P(E\mid C, S)$ 中的E) 都保留在条的左侧. 然后, 所有剩余的变量 (这里是T和S) 放在条的右侧.

这种编写因子的方法基于链式法则的重复应用. 在上面的例子中, 我们知道条件栏的两侧不能有相同的变量. 此外, 我们知道:
$$
P(T,C,S,+e)=P(T)P(S|T)P(C|T)P(+e|C,S)=P(S,T)P(C|T)P(+e|C,S)
$$
因此:
$$
P(C|T)P(+e|C,S)=\frac{P(T,C,S,+e)}{P(S,T)}=P(C,+e|T,S)
$$

虽然变量消除过程在概念上更为复杂, 但生成的任何因子的最大大小仅为8行, 而不是16行 (如果我们形成整个联合PDF的话).

另一种看待这个问题的方法是观察到 $P(T\mid+e)$ 的计算可以通过枚举推理进行
$$
\alpha\sum_{s}\sum_{c}P(T)P(s|T)P(c|T)P(+e|c,s)
$$

或者通过变量消除进行:
$$
\alpha P(T)\sum_{s}P(s|T)\sum_{c}P(c|T)P(+e|c,s)
$$

我们可以看到这些方程是等价的, 只是在变量消除中, 我们将与求和无关的项移到了每个求和的外部!

关于变量消除的最后一点需要注意的是, 只有在我们能够将最大因子的大小限制在一个合理的值时, 它才能比枚举推理有所改进.

### 6.7 贝叶斯网络中的近似推理: 采样

概率推理的另一种方法是通过简单地计数样本来隐式计算查询的概率. 这不会像IBE或变量消除那样产生精确的解, 但这种近似推理通常足够好, 尤其是可以大幅节省计算量.

例如, 假设我们想计算 $P(+t\mid+e)$. 如果我们有一台神奇的机器, 可以从我们的分布中生成样本, 我们可以收集所有 $E=+e$ 的样本, 然后计算其中 $T=+t$ 的样本比例. 我们只需查看样本就能轻松计算任何我们想要的推理. 让我们看看一些不同的生成样本的方法.

#### 6.7.1 先验采样

给定一个贝叶斯网络模型, 我们可以轻松编写一个模拟器. 例如, 考虑下面给出的仅包含两个变量 T 和 C 的简化模型的 CPT.

![图1: TC模型](../0_Attachment/Pasted%20image%2020250731173154.png)

一个简单的Python模拟器可以这样编写:

```python
import random

def get_t():
    if random.random() < 0.99:
        return True
    return False

def get_c(t):
    if t and random.random() < 0.95:
        return True
    return False

def get_sample():
    t = get_t()
    c = get_c(t)
    return [t, c]
```

我们称这种简单的方法为**先验采样** (prior sampling). 这种方法的缺点是为了分析不太可能发生的情景, 可能需要生成大量的样本. 如果我们想计算 $P(C\mid-t)$, 我们将不得不丢弃 99% 的样本.

#### 6.7.2 拒绝采样

缓解上述问题的一种方法是修改我们的程序, 提前拒绝任何与我们的证据不一致的样本. 例如, 对于查询 $P(C\mid-t)$, 除非t为假, 否则我们将避免生成C的值. 这仍然意味着我们必须丢弃大部分样本, 但至少我们生成的坏样本创建时间更短. 我们称这种方法为**拒绝采样** (rejection sampling).

![](../0_Attachment/Pasted%20image%2020250731180904.png)

以上两种方法有效的原因是: 任何有效样本的出现概率与联合PDF中指定的相同.

#### 6.7.3 似然加权

一种更奇特的方法是**似然加权** (likelihood weighting), 它确保我们永远不会生成坏样本. 在这种方法中, 我们手动将所有变量设置为查询中的证据. 例如, 如果我们想计算 $P(C\mid-t)$, 我们只需声明 $t$ 为假. 这里的问题是, 这可能会产生与正确分布不一致的样本.

如果我们简单地强制某些变量等于证据, 那么我们的样本出现的概率仅等于非证据变量的 CPT 的乘积. 这意味着联合 PDF 不能保证是正确的 (尽管对于某些情况, 如我们的双变量贝叶斯网络, 可能是正确的). 相反, 如果我们采样了变量 $Z_1$ 到 $Z_p$ 并固定了证据变量 $E_1$ 到 $E_m$, 则样本的概率为 $P(Z_1 \ldots Z_p, E_1 \ldots E_m) = \prod_{i=1}^p P(Z_i \mid \text{Parents}(Z_i))$. 缺失的是样本的概率不包括所有 $P(E_i \mid \text{Parents}(E_i))$ 的概率, 即并非每个CPT都参与.

似然加权通过为每个样本使用一个权重来解决这个问题, 该权重是给定采样变量的证据变量的概率. 也就是说, 我们不是平等地计数所有样本, 而是可以为样本 $j$ 定义一个权重 $w_{j}$, 反映给定采样值时证据变量的观测值的可能性. 通过这种方式, 我们确保每个CPT都参与. 为此, 我们遍历贝叶斯网络中的每个变量, 像正常采样一样, 如果变量不是证据变量, 则采样一个值, 如果变量是证据, 则改变样本的权重.

例如, 假设我们想计算 $P(T \mid +c, +e)$. 对于第 $j$ 个样本, 我们将执行以下算法:

1. 将 $w_j$ 设为1.0, 且 $c = \text{true}$ 和 $e = \text{true}$.
2. 对于 $T$: 这不是一个证据变量, 所以我们从 $P(T)$ 采样 $t_{j}$.
3. 对于 $C$: 这是一个证据变量, 所以我们将样本的权重乘以 $P\left(+c\mid t_{j}\right)$, 即 $w_j = w_j \cdot P(+c \mid t_j)$.
4. 对于 $S$: 从 $P(S\mid t_{j})$ 采样 $s_{j}$.
5. 对于 $E$: 将样本的权重乘以 $P\left(+e\mid+c, s_{j}\right)$, 即 $w_{j}=w_{j}\cdot P\left(+e\mid+c, s_{j}\right)$.

然后, 当我们执行通常的计数过程时, 我们将样本 $j$ 的权重设为 $w_{j}$ 而不是1, 其中 $0\leq w_{j}\leq 1$. 这种方法是有效的, 因为在最终的概率计算中, 权重有效地替代了缺失的CPT. 实际上, 我们确保每个样本的加权概率由以下公式给出:
$$
P(z_1 \ldots z_p, e_1 \ldots e_m) = \left[\prod_{i=1}^p P(z_i \mid \text{Parents}(z_i))\right] \cdot \left[\prod_{i=1}^m P(e_i \mid \text{Parents}(e_i))\right]
$$

似然加权的伪代码如下:

![图2: 似然加权](../0_Attachment/Pasted%20image%2020250731173213.png)

对于我们所有的三种采样方法 (先验采样, 拒绝采样和似然加权), 我们可以通过生成更多的样本来获得更高的准确性. 然而, 在这三种方法中, 似然加权是计算效率最高的, 原因超出了本课程的范围.

#### 6.7.4 吉布斯采样

**吉布斯采样** (Gibbs Sampling)是第四种采样方法. 在这种方法中, 我们首先将所有变量设置为某个完全随机的值 (不考虑任何CPT). 然后, 我们反复选择一个变量, 清除其值, 并根据当前分配给所有其他变量的值重新采样它.

对于上面的T, C, S, E例子, 我们可能会分配 $t = \text{true}$, $c = \text{true}$, $s = \text{false}$ 和 $e = \text{true}$. 我们选择四个变量中的一个来重新采样, 比如 $S$, 并清除它. 然后我们从分布P(S|+t,+c,+e)中选取一个新变量. 这需要我们了解这个条件分布. 事实证明, 我们可以轻松计算任何单个变量给定所有其他变量的分布. 更具体地说, $P(S \mid T, C, E)$ 可以仅使用连接了 $S$ 与它的邻居的 CPT 来计算. 因此, 在一个典型的贝叶斯网络中, 大多数变量只有少量的邻居, 我们可以在线性时间内为每个变量预先计算给定其所有邻居的条件分布.

我们不会证明这一点, 但如果我们重复这个过程足够多次, 尽管我们可能从一个低概率的值分配开始, 但我们的后续样本最终会收敛到正确的分布. 如果你好奇, 可以阅读维基百科吉布斯采样文章中的"失败模式"部分, 了解一些超出本课程范围的注意事项.

吉布斯采样的伪代码如下:

![图3: 吉布斯采样](../0_Attachment/Pasted%20image%2020250731173225.png)

### 6.8 总结

贝叶斯网络是联合概率分布的一种强大表示. 其拓扑结构编码了独立性和条件独立性关系, 我们可以使用它来建模任意分布, 以进行推理和采样.

在本笔记中, 我们介绍了两种概率推理方法: 精确推理和概率推理 (采样). 在精确推理中, 我们保证得到精确正确的概率, 但计算量可能过大.

所涵盖的精确推理算法包括:

- 枚举推理
- 变量消除

我们可以转向采样以使用更少的计算量来近似解.

所涵盖的采样算法包括:

- 先验采样
- 拒绝采样
- 似然加权
- 吉布斯采样

## 7. 决策网络和 VPI

### 7.1 效用函数

在我们对理性智能体的讨论中, 效用的概念反复出现. 例如在游戏中, 效用值通常被硬编码到游戏中, 智能体利用这些效用值来选择行动. 现在我们将讨论如何生成一个可行的效用函数.

理性智能体必须遵循**最大效用原则** — 它们必须始终选择能最大化其期望效用的行动. 然而, 这一原则仅对具有**理性偏好**的智能体有益. 为了构建一个非理性偏好的例子, 假设存在三个对象A、B和C, 我们的智能体当前拥有A. 假设智能体有以下非理性偏好

- 智能体偏好 $B$ 胜过 $A$ 加1美元
- 智能体偏好 $C$ 胜过 $B$加1美元
- 智能体偏好 $A$ 胜过 $C$ 加1美元

此时, 一个恶意智能体若拥有 $B$ 和 $C$, 可以用 $B$ 交换 $A$ 加1美元, 再用 $C$ 交换 $B$ 加1美元, 最后用 $A$ 交换 $C$ 加1美元. 我们的智能体就这样白白损失了3美元! 通过这种方式, 我们的智能体会被迫陷入一个无尽噩梦般的循环, 失去所有金钱.

现在让我们正式定义偏好的数学语言:

- 若智能体偏好奖品 $A$胜过奖品 $B$, 记作 $A \succ B$

- 若智能体对 $A$ 和 $B$ 无偏好, 记作 $A \sim B$

- **彩票**是指以不同概率获得不同奖品的情境. 表示以概率 $p$ 获得 $A$, 以概率 $(1−p)$ 获得 $B$ 的彩票为:
  $$
  L = [p, A; (1-p), B]
  $$

为了使一组偏好具有理性, 必须满足以下五个**理性公理**:

1. **可排序性**:
   $(A \succ B) \vee (B \succ A) \vee (A \sim B)$
   理性智能体必须对 $A$ 和 $B$ 有明确偏好, 或认为两者无差异.
2. **传递性**:
   $(A \succ B) \wedge (B \succ C) \Rightarrow (A \succ C)$
   若理性智能体偏好 $A$ 胜于 $B$, $B$ 胜于 $C$, 则必然偏好 $A$ 胜于 $C$.
3. **连续性**:
   $A \succ B \succ C \Rightarrow \exists p \: [p, A; (1-p), C] \sim B$
   若理性智能体偏好 $A$ 胜于 $B$, $B$ 胜于 $C$, 则存在一个 $A$ 和 $C$ 的彩票 $L$, 使得智能体在适当选择 $p$ 时对 $L$ 和 $B$ 无差异.
4. **可替代性**:
   $A \sim B \Rightarrow [p, A; (1-p), C] \sim [p, B; (1-p), C]$
   若理性智能体对 $A$ 和 $B$ 无差异, 则任何仅将 $A$ 替换为 $B$ (或反之）的彩票组合也视为无差异.
5. **单调性**:
   $A \succ B \Rightarrow (p \geq q) \Leftrightarrow [p, A; (1-p), B] \succeq [q, A; (1-q), B]$
   若理性智能体偏好 $A$ 胜于 $B$, 则在仅涉及 $A$ 和 $B$ 的彩票选择中, 智能体会偏好 $A$ 概率更高的彩票.

若智能体满足所有五个公理, 则可保证其行为可描述为期望效用最大化. 具体而言, 这意味着存在一个实值**效用函数** $U$, 该函数会为更受偏好的奖品分配更高效用值, 且彩票的效用等于该彩票所产生奖品的效用期望值. 这两个陈述可总结为以下数学等价关系:
$$
U(A) \geq U(B) \Leftrightarrow A \succeq B
\\
U([p_1, S_1; ... ;p_n, S_n]) = \sum_i p_i U(S_i)
$$
若满足这些约束并选择适当算法, 则实现该效用函数的智能体将保证行为最优. 让我们通过具体示例详细讨论效用函数. 考虑以下彩票:
$$
L = [0.5, \$0; 0.5, \$1000]
$$

这表示有50%概率获得1000美元, 50%概率获得0美元的彩票. 现在考虑三个智能体 $A_1$, $A_2$ 和 $A_3$, 其效用函数分别为 $U_1(\$x) = x$, $U_2(\$x) = \sqrt{x}$ 和 $U_3(\$x) = x^2$. 若这三个智能体需在参与彩票与固定获得500美元之间选择, 它们会如何决策? 下表列出了各智能体对两种选择的效用值:

| 智能体 | 彩票效用 | 固定支付效用 |
| ------ | -------- | ------------ |
| 1      | 500      | 500          |
| 2      | 15.81    | 22.36        |
| 3      | 500000   | 250000       |

彩票效用的计算过程如下 (使用上述公式2):
$$
U_1(L) = U_1([0.5, \$0; 0.5, \$1000]) = 0.5 \cdot U_1(\$1000) + 0.5 \cdot U_1(\$0) = 0.5 \cdot 1000 + 0.5 \cdot 0 = \boxed{500}
\\
U_2(L) = U_2([0.5, \$0; 0.5, \$1000]) = 0.5 \cdot U_2(\$1000) + 0.5 \cdot U_2(\$0) = 0.5 \cdot \sqrt{1000} + 0.5 \cdot \sqrt{0} = \boxed{15.81}
\\
U_3(L) = U_1([0.5, \$0; 0.5, \$1000]) = 0.5 \cdot U_3(\$1000) + 0.5 \cdot U_3(\$0) = 0.5 \cdot 1000^2 + 0.5 \cdot 0^2 = \boxed{500000}
$$
由此可知, 智能体 $A_1$ 对彩票与固定收益无差异 (称为**风险中性**); $A_2$ 偏好固定收益 (**风险厌恶**）；$A_3$ 偏好彩票 (**风险寻求**).

### 7.2 决策网络

先前我们学习了博弈树及minimax, expectimax 等算法, 用于确定最大化期望效用的最优行动. 在第五章笔记中, 我们讨论了贝叶斯网络及如何利用已知证据进行概率推理预测. 现在我们将结合贝叶斯网络与 expectimax, 介绍一种称为**决策网络**的模型, 用于基于整体概率图模型量化不同行动对效用的影响. 首先解析决策网络的组成:

- **机会节点**: 与贝叶斯网络行为相同. 每个结果都有关联概率, 可通过底层贝叶斯网络推理确定. 用椭圆形表示.
- **行动节点**: 完全由我们控制的节点, 表示可选择的行动. 用矩形表示.
- **效用节点**: 某些行动节点和机会节点的子节点. 根据父节点取值输出效用值. 用菱形表示.

考虑这样一种情况, 在早上去上课时决定是否带雨伞, 并且知道预计有 30% 的几率下雨. 你应该带伞吗? 如果下雨的可能性为 80%,你的答案会改变吗? 这种情况非常适合使用决策网络进行建模, 建模如下:

![图1: 决策网络示例](../0_Attachment/Pasted%20image%2020250811115607.png)

决策网络的目标仍是选择**最大期望效用** (maximum expected utility, MEU) 的行动, 步骤如下:

1. 实例化所有已知证据, 通过推理计算效用节点所有父机会节点的后验概率.

2. 遍历每个可能行动, 计算给定后验概率下采取该行动的期望效用:
   $$
   EU(a \mid e) = \sum_{x_1, ..., x_n}P(x_1, ..., x_n \mid e)U(a, x_1, ..., x_n)
   $$

3. 选择效用最高的行动作为 MEU.

通过天气示例计算最优行动 (是否带伞):

![图2: 带表的决策网络](../0_Attachment/Pasted%20image%2020250811115643.png)

计算两种行动的期望效用:
$$
\begin{aligned} EU(\text{leave} \mid \text{bad}) &= \sum_{w}P(w \mid \text{bad})U(\text{leave}, w) \\ &= 0.34 \cdot 100 + 0.66 \cdot 0 = \boxed{34} \\ EU(\text{take} \mid \text{bad}) &= \sum_{w}P(w \mid \text{bad})U(\text{take}, w) \\ &= 0.34 \cdot 20 + 0.66 \cdot 70 = \boxed{53} \end{aligned}
$$
确定 MEU:
$$
MEU(F = \text{bad}) = \max_aEU(a \mid \text{bad}) = \boxed{53}
$$
因此最优行动是带伞.

#### 7.2.1 结果树

决策网络包含类似 expectimax 的元素, 可将其展开为结果树:

![图3: 结果树](../0_Attachment/Pasted%20image%2020250811115700.png)

顶层是最大化节点 (由我们控制选择行动), 下一层是机会节点 (按后验概率导向不同效用节点). 与expectimax 的区别在于节点标注了当前已知信息 (花括号中内容).

### 7.3 完全信息价值

在我们迄今为止所涵盖的所有内容中, 我们通常总是假设我们的代理拥有特定问题所需的所有信息和/或无法获取新信息. 在实践中, 情况并非如此, 决策中最重要的部分之一是了解是否值得收集更多证据来帮助决定采取哪种行动. 观察新证据几乎总是有一些成本 (时间/金钱等). 在本节中, 我们将讨论一个非常重要的概念 — **完全信息价值 (value of perfect information, VPI)**, 它从数学上量化了智能体在观察到一些新证据时预期增加的 MEU 的量. 我们可以将学习一些新信息的 VPI 与观察该信息相关的成本进行比较, 以决定是否值得观察

#### 7.3.1 通用公式

与其简单地提出计算新证据的完美信息价值的公式, 不如让我们通过一个直观的推导来介绍一下. 我们从上面的定义中知道, VPI 指如果我们决定观察新证据, 我们的 MEU 预计会增加的量. 根据我们目前的证据, 我们知道我们当前的最大效用
$$
MEU(e) = \max_a\sum_sP(s \mid e)U(s, a)
$$
此外, 如果我们在采取行动之前观察到一些新的证据, 那么我们当时行动的最大预期效用将变成:
$$
MEU(e, e') = \max_a\sum_sP(s \mid e, e')U(s, a)
$$
但是, 请注意, 我们不知道会得到什么新证据. 例如, 如果我们事先不知道天气预报并选择观察它, 那么我们观察到的天气预报可能是好是坏. 因为我们不知道 $e'$ 会得到什么新证据, 所以我们必须将其表示为随机变量 $E'$. 如果我们不知道从观察中获得的证据会告诉我们什么, 如果我们选择观察一个新变量, 我们将如何表示即将得到的新 MEU? 答案是计算最大预期效用的期望值:
$$
MEU(e, E') = \sum_{e'}P(e' \mid e)MEU(e, e')
$$
观察一个新的证据变量会产生一个不同的 MEU, 其概率对应于观察证据变量的每个值的概率, 因此通过上述计算 $MEU(e, E')$, 我们计算出如果我们选择观察新证据, 我们期望我们的新 MEU 是多少. 我们现在快完成了 — 回到对 VPI 的定义, 我们想找到如果我们选择观察新证据, 我们的 MEU 预计会增加多少. 如果我们选择观察, 我们知道我们当前的 MEU 和新 MEU 的期望值, 所以预期的 MEU 增加只是这两个项的差! 事实上
$$
\boxed{VPI(E' \mid e) = MEU(e, E') - MEU(e)}
$$
我们可以在其中解读 $VPI(E' \mid e)$ 为 "给出我们当前的证据 e, 观察新证据 E' 的价值". 让我们通过一个例子来研究:

![图1: VPI样例](../0_Attachment/Pasted%20image%2020250811181621.png)

如果我们没有观察到任何证据, 那么 MEU 可以计算如下:
$$
\begin{aligned} MEU(\varnothing) &= \max_aEU(a) \\ &= \max_a\sum_wP(w)U(a, w) \\ &= \max\{0.7 \cdot 100 + 0.3 \cdot 0, 0.7 \cdot 20 + 0.3 \cdot 70\} \\ &= \max\{70, 35\} \\ &= 70 \end{aligned}
$$
请注意, 当我们没有证据时, 约定是写 $MEU(\varnothing)$, 表示我们的证据是空集. 现在假设我们正在决定是否观察天气预报. 我们已经计算了 $MEU(F = \text{bad}) = 53$, $MEU(F = \text{good}) = 95$, 则:
$$
\begin{aligned} MEU(e, E') &= MEU(F) \\ &= \sum_{e'}P(e' \mid e)MEU(e, e') \\ &= \sum_{f}P(F = f)MEU(F = f) \\ &= P(F = \text{good})MEU(F = \text{good}) + P(F = \text{bad})MEU(F = \text{bad}) \\ &= 0.59 \cdot 95 + 0.41 \cdot 53 \\ &= 77.78 \end{aligned}
$$
因此, $VPI(F) = MEU(F) - MEU(\varnothing) = 77.78 - 70 = \boxed{7.78}$

#### 7.3.2 VPI性质

1. **非负性**: $\forall E', e\:\: VPI(E' \mid e) \geq 0$
   观察信息总能带来更明智决策, MEU不会减少.
2. **非累加性**: $VPI(E_j, E_k \mid e) \neq VPI(E_j \mid e) + VPI(E_k \mid e)$
   因为观察 $E_j$ 可能改变对 $E_k$ 的关注程度.
3. **顺序无关性**: $VPI(E_j, E_k \mid e) = VPI(E_j \mid e) + VPI(E_k \mid e, E_j) = VPI(E_k \mid e) + VPI(E_j \mid e, E_k)$
   观察多个证据的顺序不影响最终MEU增益.

## 8. 隐马尔可夫模型

### 8.1 马尔可夫模型

在前面的笔记中, 我们讨论了贝叶斯网络, 以及它们是如何用来紧凑地表示随机变量之间关系的极好结构. 接下来, 我们将介绍一种与之密切相关的结构 — **马尔可夫模型** (Markov Models). 在本课程中, 你可以将它看作是一种链状, 无限长度的贝叶斯网络.

我们在本节将使用的运行示例是天气模式的日常变化. 我们的天气模型是**时间相关**的 (马尔可夫模型通常都是这样), 也就是说, 每一天的天气我们都会用一个单独的随机变量表示. 如果我们将 $W_i$ 定义为第 $i$ 天的天气这一随机变量, 那么天气的马尔可夫模型将如下所示:

![图1: 天气马尔可夫模型](../0_Attachment/Pasted%20image%2020250811221726.png)

在马尔可夫模型中, 我们应该存储哪些关于随机变量的信息呢? 为了跟踪我们关心的量 (这里是天气) 随时间的变化, 我们需要知道两个部分:

1. 在 $t=0$ 时的**初始分布**.
2. 描述从一个时间步到下一个时间步状态转移概率的**转移模型**.

初始分布由概率表 $P(W_0)$ 给出, 从状态 $i$ 转移到 $i+1$ 的转移模型由 $P(W_{i+1} \mid W_i)$ 给出. 注意, 这个转移模型隐含了 $W_{i+1}$ 仅依赖于 $W_i$ 的值. 换句话说, 在 $t = i+1$ 时的天气满足**马尔可夫性质** (**无记忆性质**), 并且与除 $t=i$ 之外的所有其他时间步的天气独立.

使用我们的天气马尔可夫模型, 如果我们想用链式法则重构 $W_0, W_1, W_2$ 的联合分布, 我们有:
$$
P(W_0, W_1, W_2) = P(W_0) \, P(W_1 \mid W_0) \, P(W_2 \mid W_1, W_0)
$$

但在假设马尔可夫性质成立且 $W_0 \perp W_2 \mid W_1$ 的情况下, 联合分布可以简化为:
$$
P(W_0, W_1, W_2) = P(W_0) \, P(W_1 \mid W_0) \, P(W_2 \mid W_1)
$$

此时我们已经具备了从马尔可夫模型计算该联合分布所需的全部信息. 更一般地, 马尔可夫模型在每个时间步都做如下独立性假设: $W_{i+1} \perp \{ W_0, \dots, W_{i-1} \} \mid W_i$, 这使我们可以用链式法则重构前 $n+1$ 个变量的联合分布:
$$
P(W_0, W_1, \dots, W_n) = P(W_0) \, P(W_1 \mid W_0) \, P(W_2 \mid W_1) \dots P(W_n \mid W_{n-1}) = P(W_0) \prod_{i=0}^{n-1} P(W_{i+1} \mid W_i)
$$

马尔可夫模型中通常还会做一个最终假设: 转移模型是**平稳的** (stationary). 也就是说, 对所有 $i$ (所有时间步) 都有相同的 $P(W_{i+1} \mid W_i)$. 这使得我们只需要两个表就能表示一个马尔可夫模型: 一个是 $P(W_0)$, 一个是 $P(W_{i+1} \mid W_i)$.

#### 8.1.1 迷你前向算法

我们已经知道如何计算马尔可夫模型在多个时间步的联合分布. 但这并不能直接帮助我们求某个特定时间 $t$ 的天气分布. 当然, 我们可以先算联合分布再对其他变量求和 (边缘化), 但如果我们有 $j$ 个变量且每个变量有 $d$ 种取值, 那么联合分布的大小是 $O(d^j)$, 这会非常低效.

因此, 我们引入一种更高效的技术 — **迷你前向算法 (mini-forward algorithm) **.

它的核心思路如下. 根据边缘化性质, 我们有:
$$
P(W_{i+1}) = \sum_{w_i} P(w_i, W_{i+1})
$$

根据链式法则, 我们可以将其改写为:
$$
\boxed{P(W_{i+1}) = \sum_{w_i}P(W_{i+1}|w_i)P(w_i)}
$$

这个公式非常直观: 为了计算时间步 $i+1$ 的天气分布, 我们查看时间步 $i$ 的概率分布 $P(W_i)$, 并使用转移模型 $P(W_{i+1} \mid W_i)$ 将模型"推进"一步.

通过这个公式, 我们可以迭代计算任意时间步的天气分布: 先用初始分布 $P(W_0)$ 计算 $P(W_1)$, 再用 $P(W_1)$ 计算 $P(W_2)$, 依此类推.

让我们通过一个例子来演示, 初始分布与转移模型如下:

| $W_0$ | $P(W_0)$ |
| ----- | -------- |
| sun   | 0.8      |
| rain  | 0.2      |

| $W_{i+1}$ | $W_i$ | $P(W_{i+1} \mid W_i)$ |
| --------- | ----- | --------------------- |
| sun       | sun   | 0.6                   |
| rain      | sun   | 0.4                   |
| sun       | rain  | 0.1                   |
| rain      | rain  | 0.9                   |

使用迷你前向算法计算 $P(W_1)$:
$$
P(W_1 = sun) = \sum_{w_0} P(W_1 = sun \mid w_0) P(w_0)
\\
P(W_1 = rain) = \sum_{w_0} P(W_1 = rain \mid w_0) P(w_0)
$$

因此:
| $W_1$ | $P(W_1)$ |
| ----- | -------- |
| sun   | 0.5      |
| rain  | 0.5      |

值得注意的是, 晴天的概率从 $t=0$ 的 80% 降到了 $t=1$ 的 50%. 这是由于转移模型更倾向于从晴天转向雨天. 于是自然会产生一个问题: 某个时间步处于某状态的概率会不会收敛? 我们将在下一节回答这个问题.

#### 8.1.2 平稳分布

为了解决上述问题, 我们必须计算天气的**平稳分布**  (Stationary Distribution). 顾名思义, 平稳分布在时间推移后保持不变, 即:
$$
P(W_{t+1}) = P(W_t)
$$

我们可以将这一等式与迷你前向算法的公式结合, 得到:
$$
P(W_{t+1}) = P(W_t) = \sum_{w_t} P(W_{t+1} \mid w_t) P(w_t)
$$

对于天气例子, 这给出两个方程:
$$
P(W_t = sun) = 0.6 \cdot P(W_t = sun) + 0.1 \cdot P(W_t = rain)
\\
P(W_t = rain) = 0.4 \cdot P(W_t = sun) + 0.9 \cdot P(W_t = rain)
$$

概率和为 1:
$$
P(W_t = sun) + P(W_t = rain) = 1
$$

解的平稳分布为:
| $W$  | $P(W)$ |
| ---- | ------ |
| sun  | 0.2    |
| rain | 0.8    |

由此我们得出, 当时间趋于无穷大时, 雨天的概率收敛到 80%. 这是转移模型偏向于从晴天转向雨天的直接结果.

### 8.2 隐马尔可夫模型

在马尔可夫模型中, 我们已经看到如何通过一串随机变量来引入随时间的变化. 例如, 如果我们想知道使用上文的标准马尔可夫模型第 10 天的天气, 我们可以从初始分布 $P(W_0)$ 开始, 并使用迷你前向算法配合我们的转移模型来计算 $P(W_{10})$. 然而, 在 $t = 0$ 到 $t = 10$ 之间, 我们可能会收集到新的气象证据, 这些证据可能会影响我们在任意给定时间步对天气的概率分布的信念. 简而言之, 如果天气预报第 10 天有 80% 的降雨概率, 但第 9 天夜间晴空万里, 那么那个 80% 的概率可能会大幅下降. 这正是**隐马尔可夫模型** (Hidden Markov Models, HMM) 所帮助解决的问题 — 它允许我们在每个时间步观察到一些证据, 这些证据可以潜在地影响每个状态的信念分布. 我们天气模型的隐马尔可夫模型可以用一个贝叶斯网结构来描述, 如下所示:

![图1: 天气 HMM](../0_Attachment/Pasted%20image%2020250812134028.png)

与普通马尔可夫模型不同, 我们现在有两种不同类型的节点. 为了区分, 我们把每个 $W_i$ 称为**状态变量**, 把每个天气预报 $F_i$ 称为**证据变量**. 由于 $W_i$ 编码了我们对第 $i$ 天天气的概率分布的信念, 因此天气预报 *(用于揭示今天的天气)* 在第 $i$ 天是条件依赖于该信念的. 该模型暗示了与标准马尔可夫模型类似的条件独立性关系, 并对证据变量引入了一组额外的关系:

- $F_1 \perp W_0 \mid W_1$
- $\forall i = 2, \dots, n;\quad W_i \perp \{W_0, \dots, W_{i-2}, F_1, \dots, F_{i-1}\} \mid W_{i-1}$
- $\forall i = 2, \dots, n;\quad F_i \perp \{W_0, \dots, W_{i-1}, F_1, \dots, F_{i-1}\} \mid W_i$

和马尔可夫模型一样, 隐马尔可夫模型假设转移模型 $P(W_{i+1}\mid W_i)$ 是平稳的 (stationary). 隐马尔可夫模型还额外做出简化假设, 即**传感器模型** $P(F_i\mid W_i)$ 也是平稳的. 因此, 任何隐马尔可夫模型都可以用三个概率表紧凑地表示: 初始分布、转移模型与传感器模型.

作为关于符号的最后一点, 我们定义在时间 $i$ 并且已观测到所有证据 $F_1,\dots,F_i$ 时的**信念分布**为:
$$
B(W_i) = P(W_i \mid f_1, \dots, f_i)
$$

同样地, 我们定义在时间 $i$ 且仅观测到证据 $f_1,\dots,f_{i-1}$ 时的信念分布为:
$$
B'(W_i) = P(W_i \mid f_1, \dots, f_{i-1})
$$

将 $e_i$ 定义为在时间步 $i$ 观测到的证据, 你有时也会看到来自时间步 $1 \le i \le t$ 的聚合证据被重新表示为:
$$
e_{1:t} = e_1, \dots, e_t
$$

在此记法下, $P(W_i \mid f_1, \dots, f_{i-1})$ 可以写作 $P(W_i \mid f_{1:(i-1)})$. 这个符号在接下来的各节中将变得相关, 我们将在那里讨论随时间推进的更新, 它会迭代地将新的证据纳入我们的天气模型.

#### 8.2.1 前向算法

利用上面陈述的条件概率假设和条件概率表的边缘化性质, 我们可以推导出 $B(W_i)$ 与 $B'(W_{i+1})$ 之间的关系, 这个关系和迷你前向算法的更新规则形式相同. 我们从边缘化开始:
$$
B'(W_{i+1}) = P(W_{i+1} \mid f_1, \dots, f_i) = \sum_{w_i} P(W_{i+1}, w_i \mid f_1, \dots, f_i)
$$
然后用链式法则可以重新表达为:
$$
B'(W_{i+1}) = P(W_{i+1} \mid f_1, \dots, f_i) = \sum_{w_i} P(W_{i+1} \mid w_i, f_1, \dots, f_i)\, P(w_i \mid f_1, \dots, f_i)
$$
注意到 $P(w_i \mid f_1, \dots, f_i)$ 就是 $B(w_i)$, 并且由于 $W_{i+1} \perp \{f_1, \dots, f_i\} \mid W_i$, 上述式子可以简化为 $B(W_i)$ 与 $B'(W_{i+1})$ 之间的最终关系:
$$
\boxed{B'(W_{i+1}) = \sum_{w_i}P(W_{i+1} \mid w_i)B(w_i)}
$$
现在让我们考虑如何推导 $B'(W_{i+1})$ 与 $B(W_{i+1})$ 之间的关系. 通过条件概率的定义 (带有额外条件）, 我们可以看到
$$
B(W_{i+1}) = P(W_{i+1} \mid f_1, \dots, f_{i+1}) = \frac{P(W_{i+1}, f_{i+1} \mid f_1, \dots, f_i)}{P(f_{i+1} \mid f_1, \dots, f_i)}
$$
在处理条件概率时, 一个常用技巧是在需要归一化之前延迟归一化. 具体地说, 由于上面展开中分母对 $B(W_{i+1})$ 表示的概率表的每一项都是相同的常数, 我们可以省去实际除以 $P(f_{i+1} \mid f_1, \dots, f_i)$ *(分母等于对分子取遍 $W_i$ 的边缘化求和, 是一个常数)*. 相反, 我们只需注意到 $B(W_{i+1})$ 与 $P(W_{i+1}, f_{i+1} \mid f_1, \dots, f_i)$ 成正比: $B(W_{i+1}) \propto P(W_{i+1}, f_{i+1} \mid f_1, \dots, f_i)$, 比例常数等于 $P(f_{i+1} \mid f_1, \dots, f_i)$. 每当我们决定要恢复信念分布 $B(W_{i+1})$ 时, 我们可以将计算出的每一项除以这个比例常数 (归一化).

现在, 使用链式法则我们可以观察到:
$$
B(W_{i+1}) \propto P(W_{i+1}, f_{i+1} \mid f_1, \dots, f_i) = P(f_{i+1} \mid W_{i+1}, f_1, \dots, f_i)\, P(W_{i+1} \mid f_1, \dots, f_i)
$$
根据前面与隐马尔可夫模型相关的条件独立性假设, $P(f_{i+1} \mid W_{i+1}, f_1, \dots, f_i)$ 等价于简单的 $P(f_{i+1} \mid W_{i+1})$, 并且按定义 $P(W_{i+1} \mid f_1, \dots, f_i) = B'(W_{i+1})$. 这允许我们将 $B'(W_{i+1})$ 与 $B(W_{i+1})$ 之间的关系表示为:
$$
\boxed{B(W_{i+1}) \propto P(f_{i+1} | W_{i+1})B'(W_{i+1})}
$$

将我们刚刚推导出的两个关系结合起来, 得到一个迭代算法, 称为**前向算法** (forward algorithm), 它是隐马尔可夫模型中对应于之前的迷你前向算法的版本:
$$
\boxed{B(W_{i+1}) \propto P(f_{i+1} | W_{i+1})\sum_{w_i}P(W_{i+1} | w_i)B(w_i)}
$$

前向算法可以被视为由两个不同的步骤组成: **时间推进更新** (time elapse update), 对应于从 $B(W_i)$ 确定 $B'(W_{i+1})$, 以及**观测更新** (observation update), 对应于从 $B'(W_{i+1})$ 确定 $B(W_{i+1})$. 因此, 为了将我们的信念分布推进一个时间步 (即从 $B(W_i)$ 计算 $B(W_{i+1})$）, 我们必须首先用时间推进更新将模型的状态推进一个时间步, 然后用该时间步的新证据来做观测更新.

考虑以下初始分布、转移模型和传感器模型:

| $W_0$ | $B(W_0)$ |
| ----- | -------- |
| sun   | 0.8      |
| rain  | 0.2      |

| $W_{i+1}$ | $W_i$ | $P(W_{i+1} \mid W_i)$ |
| --------- | ----- | --------------------- |
| sun       | sun   | 0.6                   |
| rain      | sun   | 0.4                   |
| sun       | rain  | 0.1                   |
| rain      | rain  | 0.9                   |

| $F_i$ | $W_i$ | $P(F_i \mid W_i)$ |
| ----- | ----- | ----------------- |
| good  | sun   | 0.8               |
| bad   | sun   | 0.2               |
| good  | rain  | 0.3               |
| bad   | rain  | 0.7               |

为了计算 $B(W_1)$, 我们首先进行时间更新得到 $B'(W_1)$:
$$
\begin{aligned}
B'(W_1=\text{sun}) &= \sum_{w_0} P(W_1=\text{sun}\mid w_0)\, B(w_0) \\
&= P(W_1=\text{sun}\mid W_0=\text{sun})\, B(W_0=\text{sun}) + P(W_1=\text{sun}\mid W_0=\text{rain})\, B(W_0=\text{rain}) \\
&= 0.6 \cdot 0.8 + 0.1 \cdot 0.2 = \boxed{0.5}
\end{aligned}
$$
$$
\begin{aligned}
B'(W_1=\text{rain}) &= \sum_{w_0} P(W_1=\text{rain}\mid w_0)\, B(w_0) \\
&= P(W_1=\text{rain}\mid W_0=\text{sun})\, B(W_0=\text{sun}) + P(W_1=\text{rain}\mid W_0=\text{rain})\, B(W_0=\text{rain}) \\
&= 0.4 \cdot 0.8 + 0.9 \cdot 0.2 = \boxed{0.5}
\end{aligned}
$$

因此:

| $W_1$ | $B'(W_1)$ |
| ----- | --------- |
| sun   | 0.5       |
| rain  | 0.5       |

接下来, 我们假设第 1 天的天气预报为 good (即 $F_1 = \text{good}$), 并进行观测更新来得到 $B(W_1)$:
$$
\begin{aligned}
B(W_1=\text{sun}) &\propto P(F_1=\text{good}\mid W_1=\text{sun})\, B'(W_1=\text{sun}) = 0.8 \cdot 0.5 = \boxed{0.4} \\
B(W_1=\text{rain}) &\propto P(F_1=\text{good}\mid W_1=\text{rain})\, B'(W_1=\text{rain}) = 0.3 \cdot 0.5 = \boxed{0.15}
\end{aligned}
$$

对 $B(W_1)$ 进行归一化, 结果为:

| W1   | B(W1)          |
| ---- | -------------- |
| sun  | $\frac{8}{11}$ |
| rain | $\frac{3}{11}$ |

注意观测天气预报的结果. 因为预报员预测了好天气, 所以我们对晴天的信念在时间更新后从 $\dfrac{1}{2}$ 增加到了观测更新后的 $\dfrac{8}{11}$.

作为结束的说明, 上面讨论的延迟归一化技巧在处理隐马尔可夫模型时实际上可以显著简化计算. 如果我们从某个初始分布开始, 并且有兴趣计算时间 $t$ 的信念分布, 我们可以使用前向算法迭代地计算 $B(W_1), \dots, B(W_t)$, 并且仅在最后通过将 $B(W_t)$ 表中的每一项除以它们的和来做一次归一.

### 8.3 维特比算法

在前向算法 (Forward Algorithm) 中, 我们使用递归来求解 $P(X_N \mid e_{1:N})$, 也就是在已观察到的证据变量到目前为止的条件下, 系统可能处于的状态的概率分布. 另一个与隐马尔可夫模型 (Hidden Markov Models, HMM).关的重要问题是: 给定已观察到的证据变量到目前为止, 系统最有可能经历的隐藏状态序列是什么? 换句话说, 我们想要求解 $\arg\max_{x_{1:N}} P(x_{1:N}\mid e_{1:N}) = \arg\max_{x_{1:N}} P(x_{1:N}, e_{1:N}).$ 这个轨迹也可以使用动态规划通过**维特比算法**来 (Viterbi Algorithm) 求解.

该算法由两次遍历组成: 第一次沿时间向前运行, 计算在已观测证据到目前为止条件下, 到达每个 (状态, 时间).组的最佳路径的概率. 第二次遍历沿时间向后运行: 首先找到位于概率最大的路径上的终止状态, 然后沿着导致该状态的路径向后遍历 (该路径必然是最佳路径).

为了可视化该算法, 考虑下面的**状态网格** (state trellis), 它是随时间展开的状态与转移的图:

![图1: 状态网格](../0_Attachment/Pasted%20image%2020250813001545.png)

在这个具有两个可能隐藏状态 (sun 或 rain). HMM 中, 我们希望计算从 $X_1$ 到 $X_N$ 的最高概率路径 (为每个时间步分配一个状态). 边从 $X_{t-1}$ 到 $X_t$ 的权重等于 $P(X_t\mid X_{t-1})P(E_t\mid X_t)$, 路径的概率通过取其边权的乘积来计算. 权重公式中的第一项表示某个特定转移有多可能, 第二项表示观测证据与该结果状态的拟合程度.

回顾一下: $P(X_{1:N}, e_{1:N}) = P(X_1)P(e_1\mid X_1)\prod_{t=2}^N P(X_t\mid X_{t-1})P(e_t\mid X_t)$. 前向算法 (在未归一化的意义下) 算的是 $P(X_N, e_{1:N}) = \sum_{x_1,\dots,x_{N-1}} P(X_N, x_{1:N-1}, e_{1:N})$. 在维特比算法中, 我们想要计算 $\arg\max_{x_1,\dots,x_N} P(x_{1:N}, e_{1:N})$
以找到给定证据的序列的最大似然估计. 注意乘积中的每一项恰好是网格上从层 $t-1$ 到层 $t$ 的边权的表达式. 因此, 沿网格路径的权重乘积给出了在给定证据下路径的概率.

我们可以为所有可能的隐藏状态求一个联合概率表, 但这会导致指数级的空间开销. 给定这样的表, 我们可以用动态规划以多项式时间计算最佳路径. 然而, 因为我们可以用动态规划直接计算最佳路径, 所以并不需要在任意时刻保存整个表.

定义 $m_t[x_t] = \max_{x_{1:t-1}} P(x_{1:t}, e_{1:t})$, 即到时间 $t$ 为止, 且以给定 $x_t$ 结尾的路径的最大概率. 这等价于从步骤 1 到 $t$ 的网格上具有最高权重的路径. 还注意到
$$
\begin{align}
m_t[x_t] &= \max_{x_{1:t-1}} P(e_t\mid x_t)P(x_t\mid x_{t-1})P(x_{1:t-1}, e_{1:t-1}) \tag{1}\\
&= P(e_t\mid x_t)\max_{x_{t-1}} P(x_t\mid x_{t-1}) \max_{x_{1:t-2}} P(x_{1:t-1}, e_{1:t-1}) \tag{2}\\
&= P(e_t\mid x_t)\max_{x_{t-1}} P(x_t\mid x_{t-1})\, m_{t-1}[x_{t-1}]. \tag{3}
\end{align}
$$
这表明我们可以通过动态规划递归地计算所有时刻的 $m_t$.

这使得确定最终状态 $x_N$ (位于最可能路径上的终态).为可能, 但我们仍然需要一种回溯来重构整条路径. 令 $a_t[x_t] = P(e_t\mid x_t)\,\arg\max_{x_{t-1}} P(x_t\mid x_{t-1})\, m_{t-1}[x_{t-1}] = \arg\max_{x_{t-1}} P(x_t\mid x_{t-1})\, m_{t-1}[x_{t-1}]$
来记录到达 $x_t$ 的最佳路径上最后一次的转移. 现在我们可以概述该算法.

```
Result: Most likely sequence of hidden states x1:N
/* Forward pass
for t=1 to N:
    for xt in X:
        if t=1:
            m_t[x_t] = P(x_t)P(e_0|x_t)
        else:
            a_t[x_t] = argmax_{x_{t-1}} P(x_t|x_{t-1}) m_{t-1}[x_{t-1}];
            m_t[x_t] = P(e_t|x_t)P(x_t|a_t[x_t]) m_{t-1}[a_t[x_t]];
        
    

/* Find the most likely path's ending point
x_N = argmax_{x_N} m_N[x_N];
/* Work backwards through our most likely path and find the hidden states
For t=N to 2:
    x_{t-1} = a_t[x_t];
```

注意我们的 $a$ 数组定义了一组 $N$ 条序列, 其中每一条序列都是到某个特定终态 $x_N$ 的最可能序列. 一旦我们完成了前向遍历, 我们查看这些 $N$ 条序列的可能性, 选择最好的那一条, 并在回溯过程中重建它. 这样我们就在多项式时间和多项式空间内计算出了证据的最可能解释.

*[(视频解释维特比算法)](https://www.bilibili.com/video/BV1vd4y147Pz)*

### 8.4 粒子滤波

回想一下在贝叶斯网络中, 当运行精确推理过于昂贵时, 使用我们讨论过的采样技术 (sampling techniques) 是一种有效的替代方法来近似我们想要的概率分布. 隐马尔可夫模型也有同样的缺点 — 前向算法运行精确推理的时间随随机变量取值域大小的增加而增加. 在我们当前的天气问题中, 天气只取两个值 $W_i\in\{\text{sun},\text{rain}\}$ 这是可接受的, 但假设我们想要运行推理来计算某一天的实际温度, 精确到十分之一度, 这时状态空间将变得很大.

隐马尔可夫模型中对应于贝叶斯网采样的方法称为**粒子滤波** (particle filtering), 它通过在状态图中模拟一组粒子的运动来近似该随机变量的概率 (信念) 分布. 这解决了与前向算法相同的问题: 它给出 $P(X_N\mid e_{1:N})$ 的近似.

我们不再存储将每个状态映射到其信念概率的完整概率表, 而是存储一个由 $n$ 个**粒子**组成的列表, 其中每个粒子处于时间相关随机变量域中的 $d$ 个可能状态之一. 通常 $n$ 显著小于 $d$ (记作 $n \ll d$), 但仍然足够大以产生有意义的近似; 否则, 粒子滤波的性能优势将不显著. 粒子在此算法中的样本.

在任一时间步, 我们对某个粒子处于某个状态的信念完全集中在该粒子列表中处于该状态的粒子数量. 例如, 假设我们想模拟某一天 $i$ 的温度 $T$ 的信念分布, 且为简便起见假设温度只能取整数值区间 $[10,20]$ ($d=11$ 个可能状态). 假设我们有 $n=10$ 个粒子, 在模拟的时间步 $i$ 它们取到以下值:
$$
[15, 12, 12, 10, 18, 14, 12, 11, 11, 10]
$$

通过对粒子列表中出现的每个温度计数并除以粒子总数, 我们可以生成时间步 $i$ 的经验分布:

| $T_i$        | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | 19   | 20   |
| ------------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| **$B(T_i)$** | 0.2  | 0.2  | 0.3  | 0    | 0.1  | 0.1  | 0    | 0    | 0.1  | 0    | 0    |

现在我们已经看到如何从粒子列表恢复信念分布, 剩下要讨论的是如何为我们选择的某一时间步生成这样的粒子列表.

#### 8.4.1 粒子滤波模拟

粒子滤波模拟 (particle filtering simulation) 从粒子初始化开始, 这可以非常灵活地完成 — 我们可以随机采样粒子, 均匀采样, 或从某个初始分布中采样. 一旦采样得到初始粒子列表, 模拟在每个时间步上进行类似于前向算法的操作: 先做时间推移更新 (time elapse update), 然后做观测更新 (observation update):

- **时间推移更新** — 根据转移模型更新每个粒子的值. 对于处于状态 $t_i$ 的粒子, 从由 $P(T_{i+1}\mid t_i)$ 给出的分布中采样更新后的值. 注意时间推移更新与贝叶斯网的先验采样 (prior sampling) 是相似的, 因为粒子在任一状态的频率反映了转移概率.

- **观测更新** — 在粒子滤波的观测更新中, 我们使用传感模型 $P(F_i\mid T_i)$ 根据观测证据和粒子的状态对每个粒子加权. 具体地, 对于处于状态 $t_i$ 且观测为 $f_i$ 的粒子, 赋予权重 $P(f_i\mid t_i)$. 观测更新算法如下:
  1. 如上所述计算所有粒子的权重.
  2. 计算每个状态的总权重.
  3. 如果所有状态的权重之和为 0, 则重新初始化所有粒子.
  4. 否则, 对状态的总权重进行归一化, 并从该分布中重采样粒子列表.

注意观测更新与似然加权 (likelihood weighting) 的相似性, 我们再次根据证据对样本进行下调权重.

让我们通过一个例子来更好地理解这个过程. 为我们的天气情形定义如下的温度转移模型: 对于某个温度状态, 你可以保持在同一状态, 或者转移到相隔一度的状态, 范围限制在 $[10,20]$ 内. 对于可能的结果状态, 最接近 15 的状态的转移概率为 80%, 其余可能状态平均分配剩余的 20% 概率.

我们的温度粒子列表如下:
$$
[15, 12, 12, 10, 18, 14, 12, 11, 11, 10]
$$
对处于该粒子列表中第一个粒子 (状态 $T_i=15$) 执行时间推移更新, 我们需要对应的转移模型:

| $T_{i+1}$             | 14   | 15   | 16   |
| --------------------- | ---- | ---- | ---- |
| $P(T_{i+1}∣T_{i}=15)$ | 0.1  | 0.8  | 0.1  |

在实践中, 我们为 $T_{i+1}$ 的每个取值分配不同的区间, 这些区间共同覆盖 $[0,1)$ 且互不重叠. 对于上面的转移模型, 区间如下:

1. $T_{i+1}=14$ 的区间为 $0 \le r < 0.1$;
2. $T_{i+1}=15$ 的区间为 $0.1 \le r < 0.9$;
3. $T_{i+1}=16$ 的区间为 $0.9 \le r < 1$.

为了对处于 $T_i=15$ 的粒子进行重采样, 我们只需生成一个位于 $[0,1)$ 的随机数并看它落在哪个区间内. 因此若随机数为 $r=0.467$, 则该粒子会保持在 $T_{i+1}=15$, 因为 $0.1 \le r < 0.9$. 现在考虑下面这组 10 个位于 $[0,1)$ 的随机数:
$$
[0.467, 0.452, 0.583, 0.604, 0.748, 0.932, 0.609, 0.372, 0.402, 0.026]
$$
如果我们用这 10 个值分别作为对 10 个粒子进行重采样的随机值, 那么在完成整个时间推移更新后我们的新粒子列表应为:
$$
[15, 13, 13, 11, 17, 15, 13, 12, 12, 10]
$$
更新后的粒子列表对应的更新后信念分布 $B(T_{i+1})$ 如下:

| $T_i$        | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | 19   | 20   |
| ------------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| $B(T_{i+1})$ | 0.1  | 0.1  | 0.2  | 0.3  | 0    | 0.2  | 0    | 0.1  | 0    | 0    | 0    |

将更新后的信念分布 $B(T_{i+1})$ 与初始信念分布 $B(T_i)$ 相比, 我们可以看到总体趋势是粒子趋向于汇聚到温度 $T=15$.

接下来, 假设我们的传感模型 $P(F_i\mid T_i)$ 表示预测正确 ($f_i=t_i$) 的概率为 80%, 而对其余 10 个状态中的任一状态给出预测的概率均为 2%. 假设观测为 $F_{i+1}=13$, 我们对 10 个粒子的权重如下:

| 粒子     | p1   | p2   | p3   | p4   | p5   | p6   | p7   | p8   | p9   | p10  |
| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| **状态** | 15   | 13   | 13   | 11   | 17   | 15   | 13   | 12   | 12   | 10   |
| **权重** | 0.02 | 0.8  | 0.8  | 0.02 | 0.02 | 0.02 | 0.8  | 0.02 | 0.02 | 0.02 |

然后我们按状态汇总权重:

| 状态     | 10   | 11   | 12   | 13   | 15   | 17   |
| -------- | ---- | ---- | ---- | ---- | ---- | ---- |
| **权重** | 0.02 | 0.02 | 0.04 | 2.4  | 0.04 | 0.02 |

将所有权重相加可得总和为 2.54, 然后我们可以通过将每一项除以该和来归一化权重表, 得到概率分布:

| 状态           | 10     | 11     | 12     | 13     | 15     | 17     |
| -------------- | ------ | ------ | ------ | ------ | ------ | ------ |
| **权重**       | 0.02   | 0.02   | 0.04   | 2.4    | 0.04   | 0.02   |
| **归一化权重** | 0.0079 | 0.0079 | 0.0157 | 0.9449 | 0.0157 | 0.0079 |

最后一步是从这个概率分布中重采样, 使用与时间推移更新相同的技术. 假设我们生成 10 个位于 $[0,1)$ 的随机数, 值如下:
$$
[0.315, 0.829, 0.304, 0.368, 0.459, 0.891, 0.282, 0.980, 0.898, 0.341]
$$
这会产生如下重采样后的粒子列表:
$$
[13, 13, 13, 13, 13, 13, 13, 15, 13, 13]
$$
相应的最终新信念分布为:

| $T_i$        | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | 19   | 20   |
| ------------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| $B(T_{i+1})$ | 0    | 0    | 0    | 0.9  | 0    | 0.1  | 0    | 0    | 0    | 0    | 0    |

注意我们的传感模型编码了天气预测以 80% 的概率是非常准确的, 因此我们的新粒子列表与此一致, 大多数粒子被重采样为 $T_{i+1}=13$.

#### 8.4.2 动态贝叶斯网络 *(PPT 补充)*

要点

- 我们希望通过多种证据源, 跟踪随时间变化的多个变量.
- 核心思想: 在每个时间点重复相同的固定贝叶斯网络结构.
- 时间t的变量可以依赖于时间 $t−1$ 的变量.
- 动态贝叶斯网络是隐马尔可夫模型 (HMMs) 的泛化.
- ![图1](../0_Attachment/Pasted%20image%2020250813181218.png)

动态贝叶斯网络的精确推理

- 变量消除法适用于动态贝叶斯网络.
- 推理过程: 将网络"展为 T 个时间步的结构, 然后通过变量消除逐步计算出 $P(X_T∣e_{1:T})$.
- 在线信念更新: 消除前一时间步的所有变量, 仅存储当前时间步的因子.
- ![图2](../0_Attachment/Pasted%20image%2020250813181143.png)

DBN粒子滤波器

- 粒子是某一时间步的完整样本.
- **初始化**: 为t=1时刻的贝叶斯网络生成先验样本.
  - 示例粒子: $G_1^a=(3,3), G_1^b=(5,3)$
- **时间推移**: 为每个粒子采样其后继状态.
  - 示例后继: $G_2^a=(2,3), G_2^b=(6,3)$
- **观测**: 根据证据的条件似然对每个完整样本加权.
  - 似然公式: $P(E_1^a∣G_1^a)⋅P(E_1^b∣G_1^b)$ (权重大小)
- **重采样**: 按似然比例选择先验样本 (即保留高似然样本, 淘汰低似然样本).

### 8.5 总结

我们看到**马尔可夫模型**可以被视为链状, 无限长度的贝叶斯网络. 我们还学到马尔可夫模型服从**马尔可夫性质**, 即我们所建模的量的分布只依赖于上一个时间步该量的值. 我们还学会了可以使用一种称为**迷你前向算法**的技术来计算任一时间步的分布, 并且如果让时间趋于无穷, 该分布最终会收敛到**平稳分布**.

我们还介绍了两种新模型:

- **马尔可夫模型** (Markov models), 它编码了满足马尔可夫性质的时间相关随机变量. 我们可以用迷你前向算法对马尔可夫模型在任一时间步计算信念分布.
- **隐马尔可夫模型** (Hidden Markov Models), 它是带有额外性质的马尔可夫模型: 在每个时间步我们可以观察到会影响信念分布的新证据. 要在隐马尔可夫模型中计算任一时间步的信念分布, 我们使用前向算法.

有时, 对这些模型运行精确推理代价过高, 在这种情况下我们可以使用粒子滤波作为一种近似推理方法.

## 9. 机器学习

### 9.1 机器学习

在本课程之前的几次笔记中, 我们已经学习了各种帮助我们在不确定性下进行推理的模型. 到目前为止, 我们假设所使用的概率模型是理所当然存在的, 并且生成这些模型所依赖的概率表的方法已被抽象掉. 我们将开始打破这种抽象屏障, 深入讨论**机器学习** (machine learning, ML) — 这是计算机科学的一个广泛领域, 涉及在给定数据的情况下, 构建和/或学习指定模型的参数.

机器学习算法种类繁多, 针对的问题类型和数据类型也各不相同, 可根据它们要完成的任务以及处理的数据类型进行分类. 机器学习算法的两个主要子类是**监督学习** (supervised learning) 和**无监督学习** (unsupervised learning). 监督学习算法通过推断输入数据与相应输出数据之间的关系, 以便在新的, 之前未见过的输入数据上预测输出. 而无监督学习算法则处理没有对应输出数据的输入数据, 主要任务是识别数据点之间或内部的固有结构, 并相应地进行分组和/或处理. 在本课程中, 我们讨论的算法将仅限于监督学习任务.

![图1](../0_Attachment/Pasted%20image%2020250813231440.png)

一旦你有了可用于学习的数据集, 机器学习过程通常会将数据集拆分为三个不同的子集:

- **训练集** (training data): 用于实际生成将输入映射到输出的模型.
- **验证集** (validation data, 也称 hold-out data 或 development data): 用于通过在输入上进行预测并生成准确率来衡量模型性能. 如果模型表现不理想, 可以重新训练 — 要么调整称为**超参数** (hyperparameters) 的特殊模型特定值, 要么直接使用不同的学习算法, 直到得到满意的结果.
- **测试集** (test set): 用于在开发结束时评估模型的最终表现. 这部分数据在开发过程中模型从未见过, 相当于"期末考试", 用来衡量模型在真实数据上的表现.

接下来, 我们将介绍一些基础的机器学习算法, 如朴素贝叶斯 (Naive Bayes), 线性回归 (Linear Regression), 逻辑回归 (Logistic Regression) 以及感知机 (Perceptron) 算法.

### 9.2 朴素贝叶斯

我们将通过一个具体的机器学习算法示例来激发对机器学习的讨论. 让我们考虑构建电子邮件垃圾邮件过滤器的常见问题, 它将邮件分类为垃圾邮件 (不想要的邮件) 或正常邮件 (想要的邮件). 这样的问题称为**分类问题** — 给定各种数据点 (在本例中, 每封邮件是一个数据点), 我们的目标是将它们分到两类或多类中的一**类**. 对于分类问题, 我们给定一个带有相应**标签**的训练集, 这些标签通常是若干离散值之一.

我们的目标将是使用这个训练数据 (邮件以及每封邮件的垃圾/正常标签) 来学习某种关系, 以便对以前未见过的邮件进行预测. 在本节中, 我们将描述如何构建一种用于解决分类问题的模型, 称为**朴素贝叶斯分类器** (Naive Bayes Classifier).

![图1](../0_Attachment/Pasted%20image%2020250814170722.png)

为了训练一个将邮件分类为垃圾或正常的模型, 我们需要一些已预分类的邮件作为训练数据来学习. 不过, 邮件只是文本字符串, 为了学到有用的东西, 我们需要从每封邮件中提取某些属性, 称为**特征** (feature). 特征可以是任何东西, 从特定的单词计数到文本模式 (例如, 单词是否全部大写) 或你能想象到的任何其他数据属性.

为训练提取的具体特征通常取决于你试图解决的具体问题, 而且你决定选择哪些特征会显著影响模型的性能. 决定使用哪些特征称为**特征工程** (feature engineering), 对机器学习至关重要, 但在本课程中, 你可以假设任何给定数据集都会给出已提取的特征. 在本笔记中, $f(x)$ 指的是应用于所有输入 $x$ 的特征函数, 然后再将它们放入模型.

现在假设你有一个包含 $n$ 个单词的词典, 从每封电子邮件中你提取出一个特征向量 $F\in\mathbb{R}^n$, 其中第 $i$ 个分量 $F_i$ 是一个随机变量, 取值为 0 或 1, 取决于词典中的第 $i$ 个单词是否出现在待考虑的邮件中. 例如, 如果 $F_{200}$ 是单词 "free" 的特征, 那么当 "free" 出现在该邮件中时我们有 $F_{200}=1$, 否则为 $0$.

用这些定义, 我们可以更具体地定义如何预测邮件是垃圾邮件还是正常邮件 — 如果我们能得到每个 $F_i$ 与标签 $Y$ 之间的联合概率表, 我们就可以计算任何待考虑邮件在其特征向量下属于垃圾或正常的概率. 具体地, 我们可以计算:
$$
P\bigl(Y=\text{spam}\mid F_1=f_1,\ldots,F_n=f_n\bigr)
\\
P\bigl(Y=\text{ham}\mid F_1=f_1,\ldots,F_n=f_n\bigr)
$$
然后简单地根据两个概率中较大的那个为邮件贴标签.

不幸的是, 由于我们有 $n$ 个特征和 $1$ 个标签, 每个都可以取两个不同的值, 相应的联合概率表的大小关于 $n$ 以指数级增长, 其条目数为 $2^{\,n+1}$ — 非常不切实际! 这个问题可以通过用贝叶斯网来建模联合概率表来解决, 做出关键的简化假设: 在给定类别标签的条件下, 每个特征 $F_i$ 相互独立.

这是一个非常强的建模假设 (这也是朴素贝叶斯被称为"朴素"的原因), 但它简化了推理并且在实践中通常工作良好. 它导致如下的贝叶斯网来表示我们想要的联合概率分布.

![图2](../0_Attachment/Pasted%20image%2020250814170745.png)

按照 d-分离的规则可以清楚地看出, 在这个贝叶斯网中, 每个 $F_i$ 在给定 $Y$ 的条件下与所有其他 $F_j$ 条件独立. 现在我们有一个 $P(Y)$ 的表, 具有 2 个条目, 以及 $n$ 个每个 $P(F_i\mid Y)$ 的表, 每个有 $2^2=4$ 个条目, 因此总共为 $4n+2$ 个条目 — 关于 $n$ 是线性的! 这个简化假设突出了统计效率的概念所带来的权衡; 有时我们需要牺牲模型的复杂性以维持在计算限制之内.

确实, 在特征数量足够少的情况下, 通常会对特征之间的关系做出更多假设以生成更好的模型 (对应于在你的贝叶斯网中添加边). 使用这个模型, 对未知数据点进行预测就相当于在我们的贝叶斯网上运行推理. 我们观察到了 $F_1,\ldots,F_n$ 的值, 并希望选择具有最大条件概率的 $Y$ 的值:
$$
\text{prediction}(f_1, \cdots, f_n) = \underset{y}{\text{argmax}}~P(Y=y \mid F_1=f_1, \ldots, F_N = f_n) \\ = \underset{y}{\text{argmax}}~P(Y=y, F_1=f_1, \ldots, F_N = f_n) \\ = \underset{y}{\text{argmax}}~P(Y=y) \prod_{i=1}^n P(F_i = f_i \mid Y=y)
$$

第一步是因为在归一化分布或未归一化分布中最大概率类别是相同的, 第二步直接来自朴素贝叶斯独立性假设: 在给定类别标签的条件下各特征相互独立 (如图形模型结构所示).

从垃圾邮件过滤器推广开来, 假设现在有 $k$ 个类别标签 ($Y$ 的可能取值). 另外, 在注意到我们想要的概率 — 给定我们的特征 $P\bigl(Y=y_i\mid F_1=f_1,\ldots,F_n=f_n\bigr)$ — 与联合概率 $P\bigl(Y=y_i,F_1=f_1,\ldots,F_n=f_n\bigr)$ 成正比之后, 我们可以计算:

$$
P(Y, F_1 = f_1, \dots, F_n = f_n) = \begin{bmatrix} P(Y = y_1, F_1 = f_1, \dots, F_n = f_n) \\ P(Y = y_2, F_1 = f_1, \dots, F_n = f_n) \\ \vdots \\ P(Y = y_k, F_1 = f_1, \dots, F_n = f_n) \end{bmatrix}= \begin{bmatrix} P(Y = y_1)\prod_i P(F_i = f_i | Y = y_1) \\ P(Y = y_2)\prod_i P(F_i = f_i | Y = y_2) \\ \vdots \\ P(Y = y_k)\prod_i P(F_i = f_i | Y = y_k) \end{bmatrix}
$$

对应于特征向量 $F$ 的类别预测就是上面计算向量中最大值对应的标签:

$$
\operatorname{prediction}(F) = \arg\max_{y_i} P(Y=y_i)\prod_j P(F_j=f_j\mid Y=y_i).
$$

我们现在已经了解了朴素贝叶斯分类器的建模假设以及如何用它来进行预测, 但还没有触及如何从输入数据中精确地学习贝叶斯网中使用的**条件概率表**. 这个问题将留到下一个主题 — **参数估计**中讨论.

#### 9.2.1 参数估计

假设你有一组**样本点**或**观测值** $x_1,\ldots,x_N$, 并且你相信这些数据是从由未知参数 $\theta$ **参数化**的分布中抽取的. 换句话说, 你相信每个观测的概率 $P_\theta(x_i)$ 都是 $\theta$ 的函数. 例如, 我们可以在掷硬币, 硬币有 $\theta$ 的概率朝上为正面 (heads).

在看到样本后, 你如何"学习"最可能的 $\theta$ 值? 例如, 如果我们有 10 次掷硬币, 并观察到 7 次为正面, 我们应该选择哪个 $\theta$ 值? 一个答案是推断 $\theta$ 等于使我们样本 $x_1,\ldots,x_N$ 从我们假定的分布中被抽取出来的概率最大的值. 机器学习中一个常用且基础的方法称为**极大似然估计** (Maximum Likelihood Estimation, MLE), 它正是这么做的.

极大似然估计通常做如下简化假设:

- 每个样本都从相同的分布中抽取. 换句话说, 每个 $x_i$ **服从相同分布**. 在我们的掷硬币示例中, 每次掷硬币朝上为正面的概率都是 $\theta$.
- 在给定分布参数的条件下, 每个样本 $x_i$ 条件**独立**于其他样本. 这是一个强假设, 但正如我们将看到的, 它极大地简化了极大似然估计的问题并且在实践中通常工作良好. 在掷硬币示例中, 一次掷硬币的结果不会影响其他掷硬币的结果.
- 在看到任何数据之前, 所有可能的 $\theta$ 值在先验上同等可能 (这称为**均匀先验**).

上述前两条假设常称为**独立同分布** (i.i.d.). 第三条假设使得 MLE 方法成为最大后验估计 (Maximum a posteriori, MAP) 方法的一个特殊情况, MAP 允许使用非均匀先验.

现在让我们定义样本的似然函数 $\mathcal{L}(\theta)$, 它表示从参数为 $\theta$ 的分布中抽到我们的样本的概率. 对于固定样本 $x_1,\ldots,x_N$, 似然只是 $\theta$ 的一个函数:
$$
\mathcal{L}(\theta) = P_{\theta}(x_1, \ldots, x_N)
$$

使用样本 $x_i$ 为 i.i.d. 的简化假设, 似然函数可以重写为:
$$
\mathcal{L}(\theta) = \prod_{i=1}^N P_{\theta}(x_i)
$$

我们如何找到使 $\mathcal{L}(\theta)$ 达到最大值的 $\theta$? 这将是最能解释我们观察到的数据的 $\theta$ 值. 回忆微积分, 在函数实现极大值或极小值的点处, 它对每个输入的偏导数 (即梯度) 必须等于零. 因此, 极大似然估计 $\theta$ 是满足下列方程的值:
$$
\frac{\partial}{\partial\theta} \mathcal{L}(\theta) = 0
$$

让我们通过一个例子使这个概念更具体. 设想你有一个装有红球和蓝球的袋子, 不知道每种球的数量. 你通过抽取一个球, 记录颜色, 然后把球放回去 (有放回抽样) 来抽样. 从这个袋子中抽取 3 个球的样本是: 红, 红, 蓝. 这似乎暗示我们应该推断袋子中有三分之二的球是红色, 三分之一是蓝色. 我们假设每次抽球为红的概率为 $\theta$, 为蓝的概率为 $1-\theta$ (这称为伯努利分布 Bernoulli distribution) :
$$
P_{\theta}(x_i) = \begin{cases} \theta & \text{if } x_i = \text{red} \\ 1 - \theta & \text{if } x_i = \text{blue} \end{cases}
$$

样本的似然为:
$$
\mathcal{L}(\theta) = \prod_{i=1}^3 P_{\theta}(x_i) = P_{\theta}(x_1 = \text{red}) \cdot P_{\theta}(x_2 = \text{red}) \cdot P_{\theta}(x_3 = \text{blue}) = \theta^2 \cdot (1 - \theta)
$$

下一步是把似然的导数设为 0 并求解 $\theta$:
$$
\frac{\partial}{\partial\theta}L(\theta)=\frac{\partial}{\partial\theta}\bigl(\theta^2(1-\theta)\bigr)=\theta(2-3\theta)=0.
$$

解这个方程得到 $\theta=\tfrac{2}{3}$, 直观上这很有道理！ (还有另一个解 $\theta=0$  —  但它对应于似然函数的最小值, 因为 $L(0)=0 < L(\tfrac{2}{3})=\tfrac{4}{27}$. ) 

#### 9.2.2 在朴素贝叶斯中的极大似然

现在让我们回到为垃圾邮件分类器推断条件概率表的问题, 先回顾我们已知的变量:

- $n$  —  词典中的单词数.
- $N$  —  你用于训练的观测样本数. 对于接下来的讨论, 我们还定义 $N_h$ 为标记为 ham 的训练样本数, $N_s$ 为标记为 spam 的训练样本数. 注意 $N_h+N_s=N$.
- $F_i$  —  随机变量, 如果词典的第 $i$ 个单词出现在待考虑的邮件中则为 1, 否则为 0.
- $Y$  —  随机变量, 根据对应邮件的标签取值为 spam 或 ham.
- $f_i^{(j)}$  —  指训练集中第 $j$ 项中随机变量 $F_i$ 的解析值. 换句话说, 每个 $f_i^{(j)}$ 若字典中第 $i$ 个词出现在第 $j$ 封邮件中则为 1, 否则为 0. 这是我们第一次看到该记法, 但在接下来的推导中会很有用.

**免责声明**: 如果你愿意可以跳过以下数学推导. 对于 CS 188, 你只需要知道本节末尾总结的推导结果.

现在在每个条件概率表 $P(F_i\mid Y)$ 中, 注意我们有两个不同的伯努利分布: $P(F_i\mid Y=\text{ham})$ 和 $P(F_i\mid Y=\text{spam})$. 为简单起见, 让我们具体考虑 $P(F_i\mid Y=\text{ham})$ 并尝试找到参数 $\theta=P(F_i=1\mid Y=\text{ham})$ 的极大似然估计, 即给定邮件为 ham 的情况下第 $i$ 个词出现的概率. 由于我们在训练集中有 $N_h$ 个 ham 邮件, 我们有 $N_h$ 个关于词 $i$ 在 ham 邮件中是否出现的观测. 因为我们的模型假定在给定类别的条件下每个单词的出现服从伯努利分布, 我们可以把似然函数表述如下:

$$
\mathcal{L}(\theta) = \prod_{j=1}^{N_h}P(F_i = f_i^{(j)}| Y = ham) = \prod_{j=1}^{N_h}\theta^{f_i^{(j)}}(1 - \theta)^{1 - f_i^{(j)}}
$$

第二步来自一个小技巧: 如果 $f_i^{(j)}=1$, 那么 $P(F_i=f_i^{(j)}\mid Y=\text{ham})=\theta^{1}(1-\theta)^{0}=\theta$; 类似地, 如果 $f_i^{(j)}=0$, 那么 $P(F_i=f_i^{(j)}\mid Y=\text{ham})=\theta^{0}(1-\theta)^{1}=1-\theta$.

为了计算 $\theta$ 的极大似然估计, 回想下一步是对 $L(\theta)$ 求导并将其设为 0. 直接对 $L(\theta)$ 求导并求解 $\theta$ 会比较困难, 因此我们使用一个在极大似然推导中非常常见的技巧: 改为求对数似然函数的最大值. 由于 $\log(x)$ 是严格递增函数 (有时称为**单调变换**), 找到能最大化 $\log L(\theta)$ 的 $\theta$ 也能最大化 $L(\theta)$. 对数似然的展开如下:
$$
\begin{align*}
\log \mathcal{L}(\theta) &= \log\left(\prod_{j=1}^{N_h} \theta^{f_i^{(j)}}(1-\theta)^{1-f_i^{(j)}}\right) \\
&= \sum_{j=1}^{N_h} \log\bigl(\theta^{f_i^{(j)}}(1-\theta)^{1-f_i^{(j)}}\bigr) \\
&= \sum_{j=1}^{N_h} \log\bigl(\theta^{f_i^{(j)}}\bigr) + \sum_{j=1}^{N_h} \log\bigl((1-\theta)^{1-f_i^{(j)}}\bigr) \\
&= \log(\theta)\sum_{j=1}^{N_h} f_i^{(j)} + \log(1-\theta)\sum_{j=1}^{N_h} (1-f_i^{(j)}).
\end{align*}
$$
在上面的推导中, 我们使用了对数函数的性质 $\log(a^c)=c\log(a)$ 和 $\log(ab)=\log(a)+\log(b)$. 现在我们对对数似然求导并设为 0, 然后解出 $\theta$:
$$
\begin{align*}
\frac{\partial}{\partial\theta}\Bigl(\log(\theta)\sum_{j=1}^{N_h} f_i^{(j)} + \log(1-\theta)\sum_{j=1}^{N_h} (1-f_i^{(j)})\Bigr) &= 0,\\[6pt]
\frac{1}{\theta}\sum_{j=1}^{N_h} f_i^{(j)} - \frac{1}{1-\theta}\sum_{j=1}^{N_h} (1-f_i^{(j)}) &= 0.
\end{align*}
$$
移项并化简得到:
$$
\begin{align*}
\frac{1}{\theta}\sum_{j=1}^{N_h} f_i^{(j)} &= \frac{1}{1-\theta}\sum_{j=1}^{N_h} (1-f_i^{(j)}),\\[6pt]
(1-\theta)\sum_{j=1}^{N_h} f_i^{(j)} &= \theta\sum_{j=1}^{N_h} (1-f_i^{(j)}),\\[6pt]
\sum_{j=1}^{N_h} f_i^{(j)} - \theta\sum_{j=1}^{N_h} f_i^{(j)} &= \theta\sum_{j=1}^{N_h} 1 - \theta\sum_{j=1}^{N_h} f_i^{(j)},\\[6pt]
\sum_{j=1}^{N_h} f_i^{(j)} &= \theta\cdot N_h,\\[6pt]
\theta &= \frac{1}{N_h}\sum_{j=1}^{N_h} f_i^{(j)}.
\end{align*}
$$
我们得到了一个非常简单的最终结果！根据上式, 参数 $\theta$ (记得 $\theta$ 是 $P(F_i=1\mid Y=\text{ham})$) 的极大似然估计对应于在 ham 邮件中第 $i$ 个单词出现的次数除以 ham 邮件的总数. 你可能会觉得这是直观的结果 — 确实如此, 但推导和技巧在比简单伯努利分布更复杂的情形下会很有用.

综上所述, 在使用伯努利特征分布的朴素贝叶斯模型中, 对于任一给定类别, 任一给定结果的极大似然估计等于该结果的计数除以该类别的样本总数. 上述推导可以推广到我们有超过两个类别或每个特征有超过两个结果的情况, 但这里不再给出该推广的详细推导.

#### 9.2.3 平滑

尽管极大似然估计是一个非常强大的参数估计方法, 但不良的训练数据往往会带来不好的后果. 例如, 如果在我们训练集中每次单词 "minute" 出现时该邮件都被标记为 spam, 那么训练得到的模型会学习到  
$$
P(F_{\text{minute}}=1\mid Y=\text{ham})=0.
$$

因此在一封未见过的邮件中, 如果单词 "minute" 出现时,   
$$
P(Y=\text{ham})\prod_i P(F_i\mid Y=\text{ham}) = 0,
$$

于是你的模型永远不会将任何包含单词 "minute" 的邮件判定为 ham. 这是**过拟合** (overfitting) 的经典例子, 即构建了不能很好泛化到以前未见过数据的模型. 仅仅因为某个特定单词在训练数据中没有出现, 并不意味着它在测试数据或真实世界中不会出现.

可以通过**拉普拉斯平滑** (Laplace smoothing) 来缓解朴素贝叶斯分类器的过拟合问题. 概念上, 强度为 $k$ 的拉普拉斯平滑假设每个结果都额外出现了 $k$ 次. 因此, 如果对于某个样本你对结果 $x$ 的极大似然估计在样本量为 $N$ 时为
$$
P_{\text{MLE}}(x) = \frac{\text{count}(x)}{N},
$$

那么强度为 $k$ 的拉普拉斯估计为
$$
P_{\text{LAP},k}(x) = \frac{\text{count}(x)+k}{N + k|X|},
$$

其中 $|X|$ 是结果的可能取值数量. 这个式子的含义是: 我们假设每个结果额外出现了 $k$ 次, 因此把计数当作 $\text{count}(x)+k$, 同样如果我们对 $|X|$ 个类每个都额外假设出现了 $k$ 次, 那么总体样本数应当加上 $k|X|$, 从而得到上式. 对于条件概率的拉普拉斯估计 (在不同类别下计算结果的拉普拉斯估计) 也有类似的结果:
$$
P_{\text{LAP},k}(x\mid y) = \frac{\text{count}(x,y)+k}{\text{count}(y) + k|X|}.
$$

有两个值得注意的拉普拉斯平滑特例. 第一个是当 $k=0$ 时,
$$
P_{\text{LAP},0}(x) = P_{\text{MLE}}(x).
$$

第二个是当 $k\to\infty$ 时. 观察到每个结果的无限个额外实例会使得实际样本的影响微不足道, 因此拉普拉斯估计暗示每个结果同等可能. 确实:
$$
P_{\text{LAP},\infty}(x) = \frac{1}{|X|}.
$$

在模型中适当的 $k$ 值通常通过试验-验证来确定. $k$ 是模型的一个超参数, 这意味着你可以设定它为任意值并查看哪个值在验证数据上产生最佳的预测准确率/性能.

### 9.3 感知器

#### 9.3.1 线性分类器

朴素贝叶斯的核心思想是从训练数据中提取某些属性, 称为特征, 然后估计给定这些特征时标签的概率: $P(y \mid f_1, f_2, \dots, f_n)$. 因此, 给定一个新的数据点, 我们可以提取相应的特征, 并以在这些特征下概率最大的标签来对新数据点进行分类. 然而, 这要求我们去估计分布, 我们使用极大似然估计 (MLE) 来做到这一点. 那如果我们决定不去估计概率分布会怎样? 让我们先看一个简单的线性分类器, 我们可以用它来做**二分类**, 其中标签有两种可能: 正类或负类.

**线性分类器** (linear classifier) 的基本思想是使用特征的线性组合来做分类 — 我们称这个值为**激活** (activation). 具体地, 激活函数接受一个数据点, 将数据点的每个特征 $f_i(\mathbf{x})$ 乘以对应的权重 $w_i$, 然后输出所有结果值的和. 在向量形式中, 我们也可以把它写成权重向量 $\mathbf{w}$ 与特征向量 $\mathbf{f}(\mathbf{x})$ 的点积:
$$
\text{activation}_w(\mathbf{x}) = h_{\mathbf{w}}(\mathbf{x}) = \sum_i w_i f_i(\mathbf{x}) = \mathbf{w}^T \mathbf{f}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{f}(\mathbf{x})
$$

如何用激活值进行分类? 对于二分类, 当数据点的激活为正时, 我们将该数据点分类为正类; 若为负, 则分类为负类:
$$
\text{classify}(\mathbf{x}) = \begin{cases} + & \text{if } h_{\mathbf{w}}(\mathbf{x}) > 0 \\ - & \text{if } h_{\mathbf{w}}(\mathbf{x}) < 0 \end{cases}
$$

为了从几何上理解这一点, 让我们重新审视向量化的激活函数. 我们可以将点积重写为如下形式, 其中 $\|\cdot\|$ 是模长运算符, $\theta$ 是 $\mathbf{x}$ 与 $\mathbf{f}(\mathbf{x})$ 之间的夹角:
$$
h_{\mathbf{w}}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{f}(\mathbf{x}) = \|\mathbf{w}\| \|\mathbf{f}(\mathbf{x})\| \cos(\theta)
$$

![图1](../0_Attachment/Pasted%20image%2020250814214952.png)

由于模长总是非负的, 并且我们的分类规则只看激活的符号, 所以决定类别的唯一期项是 $\cos(\theta)$:
$$
\text{classify}(\mathbf{x}) = \begin{cases} + & \text{if } \cos(\theta) > 0 \\ - & \text{if } \cos(\theta) < 0 \end{cases}
$$

因此, 我们关心的是 $\cos(\theta)$ 何时为正或负. 很容易看出当 $\theta < \frac{\pi}{2}$ 时, $\cos(\theta)$ 将落在区间 $(0,1]$, 为正; 当 $\theta > \frac{\pi}{2}$ 时, $\cos(\theta)$ 将落在区间 $[-1,0)$, 为负. 你可以通过看单位圆来确认这一点. 本质上, 我们的简单线性分类器在检查新的数据点的特征向量是否大致"指向"预定义的权重向量的相同方向, 如果是则应用正类:
$$
\text{classify}(\mathbf{x}) = \begin{cases} + & \text{if } \theta < \frac{\pi}{2} \text{ (i.e., when } \theta \text{ is less than 90°, or acute)} \\ - & \text{if } \theta > \frac{\pi}{2} \text{ (i.e., when } \theta \text{ is greater than 90°, or obtuse)} \end{cases}
$$

到目前为止, 我们还没有考虑那些使得 $\text{activation}_w(\mathbf{x}) = \mathbf{w}^T \mathbf{f}(\mathbf{x}) = 0$ 的点. 遵循所有相同的逻辑, 我们会看到对于这些点 $\cos(\theta)=0$. 此外, 对于这些点 $\theta=\frac{\pi}{2}$ (即 $\theta$ 为 $90^\circ$）. 换句话说, 这些是与 $\mathbf{w}$ 正交的特征向量的数据点. 我们可以添加一条与 $\mathbf{w}$ 正交的虚线, 在这条线上任何位于其上的特征向量的激活都等于 0:

![图2: 决策边界](../0_Attachment/Pasted%20image%2020250814215001.png)

我们称这条蓝线为**决策边界** (decision boundary), 因为它将我们分类为正类的区域与分类为负类的区域分隔开来. 在更高维度中, 线性决策边界通常称为**超平面** (hyperplane). 超平面是一个维度比潜在空间低一维的线性表面, 从而将空间分成两部分. 对于一般的分类器 (非线性的), 决策边界可能不是线性的, 但简单定义为在特征向量空间中分隔类别的表面. 对于落在决策边界上的点, 我们可以任意应用任一标签, 因为两类在此处同样有效 (在下面的算法中, 我们会把位于该线上点分类为正类).

![图3: x 被分为正类](../0_Attachment/Pasted%20image%2020250814215010.png)
![图4: x 被分为负类](../0_Attachment/Pasted%20image%2020250814215019.png)

### 9.4 二元感知器

太好了, 现在你知道线性分类器是如何工作的, 但我们如何构建一个好的分类器呢? 在构建分类器时, 你从带有正确类别标签的数据开始; 我们称之为**训练集** (training set). 你通过在训练数据上评估分类器, 将其与训练标签进行比较, 然后调整分类器的参数, 直到达到目标, 从而构建分类器.

让我们探讨一种特定的简单线性分类器实现: 二元感知器 (binary perceptron). 感知器是一个二分类器 — 尽管它可以扩展到处理多于两个类. 二元感知器的目标是找到一个能够完美分隔训练数据的决策边界. 换句话说, 我们在寻找最优的权重 — 最好的 $w$ — 使得任何特征化的训练点与权重相乘后都能被完美分类.

**算法**

感知器算法按如下步骤工作:

1. 将所有权重初始化为 0: $w = 0$

2. 对于每个训练样本, 其特征为 $f(x)$ 且真实类别标签 $y^* \in \{-1, +1\}$, 执行:

   1. 使用当前权重对样本进行分类, 令 $y$ 为当前 $w$ 预测的类别:
      $$
      y = \text{classify}(x) = \begin{cases} +1 & \text{if } h_{\mathbf{w}}(\mathbf{x}) = \mathbf{w}^T \mathbf{f}(\mathbf{x}) > 0 \\ -1 & \text{if } h_{\mathbf{w}}(\mathbf{x}) = \mathbf{w}^T \mathbf{f}(\mathbf{x}) < 0 \end{cases}
      $$

   2. 比较预测标签 $y$ 与真实标签 $y^*$:

      - 如果 $y = y^*$, 不做任何事
      - 否则, 如果 $y \neq y^*$, 则更新权重: $w \leftarrow w + y^* f(x)$

3. 如果你在遍历了**每个**训练样本后都没有更新权重 (所有样本都被正确预测), 则终止. 否则, 重复步骤 2.

**更新权重**

让我们检查并证明更新权重的过程. 回顾步骤 2b, 上述当分类器正确时, 什么都不改变. 但当分类器错误时, 权重向量按下面的规则更新:
$$
\mathbf{w} \leftarrow \mathbf{w} + y^* \mathbf{f}(\mathbf{x})
$$
其中 $y^*$ 是真实标签, 取值为 $1$ 或 $-1$, $x$ 是我们误分类的训练样本. 你可以这样解释这个更新规则:

1. 把正类误分类为负类: $\mathbf{w} \leftarrow \mathbf{w} + \mathbf{f}(\mathbf{x})$
2. 把负类误分类为正类: $\mathbf{w} \leftarrow \mathbf{w} - \mathbf{f}(\mathbf{x})$

为什么这有效? 一种看法是把它看成一种平衡行为. 误分类要么是因为某个训练样本的激活值比应有的值小得多 (导致情况 1 的误分类), 要么是因为激活值比应有的值大得多 (导致情况 2 的误分类).

考虑情况 1, 其中激活值为负而应为正. 换句话说, 激活值太小. 我们要调整 $w$ 来修复这一点, 使得该训练样本的激活值变大. 为了让你相信更新规则 $w \leftarrow w + f(x)$ 确实能做到这一点, 让我们更新 $w$ 并查看激活值如何变化.
$$
h_{w+f(x)}(x) = (w + f(x))^T f(x) = w^T f(x) + f(x)^T f(x) = h_w(x) + f(x)^T f(x)
$$

使用我们的更新规则, 我们看到新的激活值增加了 $f(x)^T f(x)$, 这是一个正数, 因此表明我们的更新是有道理的. 激活值正在变大 — 更接近于变为正值. 你可以对当分类器因为激活值过大 (激活为正但应为负) 而误分类的情况重复同样的逻辑. 你会看到更新会使新的激活值减少 $f(x)^T f(x)$, 从而变小并更接近正确分类.

虽然这清楚地解释了为什么我们要加和减某些东西, 但为什么要加减我们的样本点的特征呢? 一个很好的思考方式是, 权重并不是决定此分数的唯一因素. 分数是由权重与相应样本相乘决定的. 这意味着样本的某些部分比其他部分贡献更大. 考虑下面的情形, 设 $x$ 是给定的, 真实标签为 $y^* = -1$ 的训练样本:
$$
\mathbf{w}^T = \begin{bmatrix}2 & 2 & 2\end{bmatrix}, \quad \mathbf{f}(\mathbf{x}) = \begin{bmatrix}4 \\ 0 \\ 1\end{bmatrix} \quad h_{\mathbf{w}}(\mathbf{x}) = (2 \times 4) + (2 \times 0) + (2 \times 1) = 10
$$

我们知道权重需要变小, 因为激活需要为负才能正确分类. 不过我们不想把它们都改动相同的量. 你会注意到样本的第一个元素 4 对我们的得分 10 的贡献要比第三个元素大得多, 而第二个元素根本没有贡献. 因此一个合适的权重更新应当大量改变第一个权重, 少量改变第三个权重, 而第二个权重不应改变. 毕竟, 第二和第三个权重可能都没有坏, 我们不想去修复没有坏的东西！

当思考一种好的方式去改变我们的权重向量以满足上述愿望时, 事实证明仅仅使用样本本身确实能做到我们想要的; 它会大量改变第一个权重, 少量改变第三个权重, 并且完全不改变第二个权重！

一个可视化也会有帮助. 在下图中, $f(x)$ 是一个当前被误分类 (属于正类 $y^* = +1$) 的数据点的特征向量——它位于由"旧 $w$"定义的决策边界的错误一侧. 把它加到权重向量上会生成一个新的权重向量, 该向量与 $f(x)$ 的夹角更小. 它也会移动决策边界. 在这个例子中, 它移动了决策边界, 使得 $x$ 现在会被正确分类 (注意错误并不总是会被修正 — 这取决于权重向量的大小以及 $f(x)$ 当前离边界有多远).

![图1: 将 x 与旧 w 错误分类](../0_Attachment/Pasted%20image%2020250814223511.png)
![图2: 更新 w](../0_Attachment/Pasted%20image%2020250814223517.png)
![图3: 更新 x 的分类](../0_Attachment/Pasted%20image%2020250814223524.png)

**偏置** (Bias) 

如果你尝试基于到目前为止提到的内容实现感知器, 你会注意到一个特别不友好的怪癖. 你得到的任何决策边界都会穿过原点. 基本上, 你的感知器只能产生可以用函数 $w^T f(x) = 0$ 表示的决策边界, 其中 $w, f(x) \in \mathbb{R}^d$. 问题是, 即使在存在线性决策边界可以分隔数据正负类的问题中, 该边界也可能不经过原点, 而我们希望能够画出那些不经过原点的直线.

为此, 我们将修改特征和权重以加入偏置项: 在样本的特征向量中添加一个总为 1 的特征, 并在权重向量中为该特征添加一个额外的权重. 这样做本质上允许我们产生可以表示为 $w^T f(x) + b = 0$ 的决策边界, 其中 $b$ 是加权偏置项 (即特征为 1 的那一项乘以权重向量中的最后一个权重).

从几何上, 我们可以通过思考激活函数在 $w^T f(x)$ 与包含偏置 $w^T f(x) + b$ 时的外观来可视化这一点. 为此, 我们需要比特征化数据的空间多一维 (在下面的图中称为标记数据空间). 在上面所有章节中, 我们只看的是数据空间的平面视图.

![图4: 无偏置](../0_Attachment/Pasted%20image%2020250814224625.png)
![图5: 有偏置](../0_Attachment/Pasted%20image%2020250814224633.png)

**示例**

让我们看一个逐步运行感知器算法的例子.

让我们用感知器算法对数据进行一次遍历, 按顺序取每个数据点. 我们从权重向量 $[w_0, w_1, w_2] = [-1, 0, 0]$ 开始 (其中 $w_0$ 是我们偏置特征的权重, 记住该偏置特征总为 1).

**训练集**

| #    | **f**1 | **f**2 | **y∗** |
| ---- | ------ | ------ | ------ |
| 1    | 1      | 1      | -      |
| 2    | 3      | 2      | +      |
| 3    | 2      | 4      | +      |
| 4    | 3      | 4      | +      |
| 5    | 2      | 3      | -      |

**单次感知器更新过程**

| step | Weights       | Score                         | Correct? | Update       |
| ---: | ------------- | ----------------------------- | -------- | ------------ |
|    1 | $[-1, 0, 0]$  | $-1 * 1 + 0 * 1 + 0 * 1 = -1$ | yes      | none         |
|    2 | $[-1, 0, 0]$  | $-1 * 1 + 0 * 3 + 0 * 2 = -1$ | no       | $+[1, 3, 2]$ |
|    3 | $[0, 3, 2]$   | $0 * 1 + 3 * 2 + 2 * 4 = 14$  | yes      | none         |
|    4 | $[0, 3, 2]$   | $0 * 1 + 3 * 3 + 2 * 4 = 17$  | yes      | none         |
|    5 | $[0, 3, 2]$   | $0 * 1 + 3 * 2 + 2 * 3 = 12$  | no       | $-[1, 2, 3]$ |
|    6 | $[-1, 1, -1]$ |                               |          |              |

我们在这里停止, 但实际上该算法会对数据进行更多轮的遍历, 直到所有数据点在一次遍历中都被正确分类为止.

#### 9.4.1 多类感知器 (Multiclass Perceptron) 

上面呈现的感知器是二分类器, 但我们可以相当容易地将其扩展以处理多类. 主要的区别在于我们如何设置权重以及如何更新这些权重. 对于二分类情况, 我们有一个权重向量, 维度等于特征数 (加上偏置特征). 对于多类情况, 我们将为每个类有一个权重向量. 因此, 在 3 类的情况下, 我们有 3 个权重向量. 为了对样本进行分类, 我们通过对每个类将特征向量与该类的权重向量做点积来计算该类的分数. 得分最高的类就是我们的预测类别.

例如, 考虑 3 类的情况. 设我们的样本有特征 $f(x) = [-2, 3, 1]$, 类 0, 1, 2 的权重为:
$$
\mathbf{w}_0 = \begin{bmatrix}-2 & 2 & 1\end{bmatrix} \\
\mathbf{w}_1 = \begin{bmatrix}0 & 3 & 4\end{bmatrix} \\
\mathbf{w}_2 = \begin{bmatrix}1 & 4 & -2\end{bmatrix}
$$
对每个类取点积得到分数 $s_0 = 11,\, s_1 = 13,\, s_2 = 8$. 因此我们会预测 $x$ 属于类 1.

需要注意的是, 在实际实现中, 我们不会把权重作为分开的结构保存; 我们通常把它们堆叠在一起以创建一个权重矩阵. 这样, 我们就不用做与类数相同次数的点积, 而是可以做一次矩阵-向量乘法. 这在实践中通常更高效 (因为矩阵-向量乘法通常有高度优化的实现).

在上面的例子中, 那将是:
$$
\mathbf{W} = \begin{bmatrix} -2 & 2 & 1 \\ 0 & 3 & 4 \\ 1 & 4 & -2 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix}-2 \\ 3 \\ 1\end{bmatrix}
$$
我们的标签是:
$$
\arg\max (\mathbf{Wx}) = \arg\max \begin{bmatrix}11 \\ 13 \\ 8\end{bmatrix} = 1
$$

随着权重结构的变化, 权重更新在多类情况下也有所不同. 如果我们正确分类数据点, 则像二分类那样什么也不做. 如果我们选择错误, 例如我们选择了类 $y \neq y^*$, 那么我们将把特征向量加到真实类 $y^*$ 对应的权重向量上, 并从预测的错误类 $y$ 对应的权重向量中减去特征向量. 在上面的例子中, 假设正确类是类 2, 但我们预测为类 1. 我们现在将从类 1 的权重向量中减去 $x$:
$$
\mathbf{w}_1 = \begin{bmatrix}0 & 3 & 4\end{bmatrix} - \begin{bmatrix}-2 & 3 & 1\end{bmatrix} = \begin{bmatrix}2 & 0 & 3\end{bmatrix}
$$
接着, 我们把特征向量加到正确类 (在本例中为类 2) 的权重向量上:
$$
\mathbf{w}_2 = \begin{bmatrix}1 & 4 & -2\end{bmatrix} + \begin{bmatrix}-2 & 3 & 1\end{bmatrix} = \begin{bmatrix}-1 & 7 & -1\end{bmatrix}
$$

这相当于"奖励"正确的权重向量, "惩罚"误导的, 错误的权重向量, 而对其它权重向量保持不变. 考虑到权重和权重更新的差异, 其余算法本质上相同: 循环遍历每个样本点, 在出错时更新权重, 直到不再犯错为止.

为了合入偏置项, 做法与二分类感知器相同 — 在每个特征向量中添加一个值为 1 的额外特征, 并在每个类的权重向量中为该特征添加一个额外的权重 (这相当于在矩阵形式中添加一列).

### 9.5 线性回归

现在, 我们将从之前对朴素贝叶斯的讨论转向**线性回归** (Linear Regression). 这种方法也称为**最小二乘法** (least squares), 可追溯到卡尔·弗里德里希·高斯 (Carl Friedrich Gauss), 是机器学习和计量经济学中研究最广的工具之一.

**回归问题**是一种输出为连续变量 (用 y 表示）的机器学习问题. 特征可以是连续的或类别的. 我们将一组特征表示为 $x \in \mathbb{R}^n$, 对 $n$ 个特征即 $x = (x_1, \dots, x_n)$.

我们使用以下线性模型来预测输出:
$$
h_{\mathbf{w}}(\mathbf{x}) = w_0 + w_1 x^1 + \cdots + w_n x^n
$$
其中线性模型的权重 $w_i$ 是我们要估计的. 权重 $w_0$ 对应模型的截距. 有时文献中会在特征向量 $\mathbf{x}$ 上加一个 1, 以便我们可以将线性模型写成 $\mathbf{w}^T \mathbf{x}$, 此时 $\mathbf{x} \in \mathbb{R}^{n+1}$. 要训练模型, 我们需要一个衡量模型预测输出好坏的度量. 为此, 我们将使用 $L_2$ 损失函数, 它使用 $L_2$ 范数惩罚预测值与真实输出之间的差异. 如果我们的训练数据集有 $N$ 个数据点, 则损失函数定义如下:

$$
Loss(h_{\mathbf{w}}) = \frac{1}{2} \sum_{j=1}^N L_2(y^j, h_{\mathbf{w}}(\mathbf{x}^j)) = \frac{1}{2} \sum_{j=1}^N (y^j - h_{\mathbf{w}}(\mathbf{x}^j))^2 = \frac{1}{2} \left\|\mathbf{y} - \mathbf{X} \mathbf{w}\right\|_2^2
$$

注意 $\mathbf{x}^j$ 对应第 $j$ 个数据点 $\mathbf{x}^j \in \mathbb{R}^n$. 项 $\tfrac{1}{2}$ 只是为了简化闭式解的表达. 最后一个表达式是损失函数的等价表示, 使得最小二乘的推导更容易. 量 $\mathbf{y}, \mathbf{X}, \mathbf{w}$ 定义如下:

$$
\mathbf{y} = \begin{bmatrix} y^1 \\ y^2 \\ \vdots \\ y^N \end{bmatrix}, \quad \mathbf{X} = \begin{bmatrix} 1 & x_1^1 & \cdots & x_1^n \\ 1 & x^1_2 & \cdots & x^n_2 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x^1_N & \cdots & x^n_N \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} w_0 \\ w_1 \\ \vdots \\ w_n \end{bmatrix}
$$

其中 $\mathbf{y}$ 是堆叠的输出向量, $\mathbf{X}$ 是特征向量的矩阵, $x_i^{(j)}$ 表示第 $j$ 个数据点的第 $i$ 个分量. 最小二乘解记作 $\hat {\mathbf{w}}$, 现在可以用基本线性代数规则来推导. 更具体地, 我们将找到使损失函数最小的 $\hat {\mathbf{w}}$, 方法是对损失函数求导并令导数为零.

$$
\nabla_{\mathbf{w}} \frac{1}{2} \left\|\mathbf{y} - \mathbf{X} \mathbf{w}\right\|_2^2 = \nabla_{\mathbf{w}} \frac{1}{2} \left(\mathbf{y} - \mathbf{X} \mathbf{w}\right)^T \left(\mathbf{y} - \mathbf{X} \mathbf{w}\right) \\
= \nabla_{\mathbf{w}} \frac{1}{2} \left(\mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \mathbf{w} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}\right) \\
= \nabla_{\mathbf{w}} \frac{1}{2} \left(\mathbf{y}^T \mathbf{y} - 2 \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}\right) = -\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \mathbf{w}
$$
将梯度设为零我们得到:
$$
-\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \mathbf{w} = 0 \Rightarrow \hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$

得到估计的权重向量后, 我们现在可以对新的未见测试数据点进行预测:
$$
h_{\hat{\mathbf{w}}}(\mathbf{x}) = \hat{\mathbf{w}}^T \mathbf{x}
$$

### 9.6 优化

线性回归方法允许我们通过对损失函数求导并将梯度设为零来得到权重的闭式解. 一般而言, 对于给定的目标函数, 可能不存在闭式解. 在这种情况下, 我们使用**基于梯度的方法**来寻找最优权重. 思想是梯度指向目标函数增长最快的方向. 我们通过朝着最陡**上升**方向移动来最大化函数, 通过朝着最陡**下降**方向移动来最小化函数.

当目标是要最大化的函数时使用梯度上升.

**算法1**: 梯度上升

1. 随机初始化 $\mathbf{w}$.
2. 当 $\mathbf{w}$ 未收敛时重复: $\mathbf{w} \leftarrow \mathbf{w} + \alpha \nabla_{\mathbf{w}} f(\mathbf{w})$

当目标是我们要最小化的损失函数时使用梯度下降. 注意这仅与梯度上升在方向上相反.

**算法2**: 梯度下载

1. 随机初始化 $\mathbf{w}$.
2. 当 $\mathbf{w}$ 未收敛时重复: $\mathbf{w} \leftarrow \mathbf{w} - \alpha \nabla_{\mathbf{w}} f(\mathbf{w})$

一开始, 我们随机初始化权重. 我们用 $\alpha$ 表示学习率, 它控制我们在梯度方向上移动步长的大小. 对于机器学习中的大多数函数, 很难想出学习率的最优值. 实际上, 我们希望学习率足够大以便快速朝正确方向移动, 但同时又足够小以免方法发散. 机器学习文献中的典型方法是以相对较大的学习率开始梯度下降, 并随着迭代次数的增加减小学习率 (学习率衰减).

如果我们的数据集包含大量的 $n$ 个数据点, 那么在梯度下降的每次迭代中计算如上所述的梯度可能会过于计算密集. 因此, 提出了像随机和批量梯度下降这样的替代方法. 在**随机梯度下降** (stochastic gradient descent) 中, 在算法的每次迭代中我们仅使用一个数据点来计算梯度. 该数据点每次从数据集中随机采样. 由于我们仅用一个数据点来估计梯度, 随机梯度下降会产生噪声梯度, 从而使收敛变得更困难. **小批量梯度下降** (mini-batch gradient descent)是随机和普通梯度下降 (批量梯度下降, batch gradient descent) 之间的折中方法, 它每次使用大小为 $m$ 的一批数据点来计算梯度 *(利用计算机的并行性)*. 批量大小 $m$ 是用户指定的参数.

让我们看一个我们已经见过的模型 — 线性回归上的梯度下降示例. 回想在线性回归中, 我们定义损失函数为
$$
\text{Loss}(h_{\mathbf{w}}) = \frac{1}{2}\left\|\mathbf{y} - \mathbf{X}\mathbf{w}\right\|_2^2
$$

线性回归有一个著名的闭式解 $\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$, 我们在上一节看到了它. 然而, 我们也可以选择通过运行梯度下降来求解最优权重. 我们会计算损失函数的梯度为
$$
\nabla_{\mathbf{w}} \text{Loss}(h_{\mathbf{w}}) = -\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \mathbf{w}
$$

然后, 我们使用该梯度写出线性回归的梯度下降算法:

**算法3**: 最小二乘法梯度下降

1. 随机初始化 $\mathbf{w}$.
2. 当 $\mathbf{w}$ 未收敛时重复: $\mathbf{w} \leftarrow \mathbf{w} - \alpha (-\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \mathbf{w})$

一个很好的练习: 创建一个线性回归问题并确认闭式解与从梯度下降收敛得到的解是相同的.

### 9.7 逻辑回归

让我们再次回想之前的线性回归方法. 在那里, 我们假设输出是一个数值的实数.

如果我们想要预测一个类别变量怎么办? 逻辑回归允许我们将输入特征的线性组合通过逻辑函数转换为一个**概率**:
$$
h_{\mathbf{w}}(\mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}
$$
需要注意的是, 尽管逻辑回归被称为回归, 这其实是一个用词不当 (misnomer). 逻辑回归用于解决**分类问题**, 而不是回归问题.

逻辑函数 $g(z)=\frac{1}{1+e^{-z}}$ 常被用来对二分类输出建模. 注意该函数的输出总是在 0 与 1 之间, 如下图所示:

![图1](../0_Attachment/Pasted%20image%2020250815000947.png)

直观上, 逻辑函数模拟数据点属于标签为 1 的类别的概率. 原因在于逻辑函数的输出被限制在 0 到 1 之间, 而我们希望模型能够捕捉特征属于某一特定标签的概率. 例如, 在训练好逻辑回归之后, 我们对一个新的数据点进行预测并得到逻辑函数的输出. 如果输出值大于 0.5, 我们将其分类为标签 1；否则, 我们将其分类为标签 0. 更具体地, 我们将概率建模为:
$$
P(y = +1 \mid \mathbf{f}(\mathbf{x}); \mathbf{w}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{f}(\mathbf{x})}} \\
P(y = -1 \mid \mathbf{f}(\mathbf{x}); \mathbf{w}) = 1 - \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{f}(\mathbf{x})}}
$$
这里我们用 $\mathbf{f}(\mathbf{x})$ 表示特征向量 $\mathbf{x}$ 的函数 (通常是恒等映射）, 分号 ";" 表示该概率是参数权重 $\mathbf{w}$ 的函数.

一个有用的性质是逻辑函数的导数为:
$$
g'(z)=g(z)(1-g(z))
$$
我们如何训练逻辑回归模型? 逻辑回归的损失函数是 $L2$ 损失
$$
\text{Loss}(\mathbf{w}) = \frac{1}{2} (\mathbf{y} - h_{\mathbf{w}}(\mathbf{x}))^2
$$
由于逻辑回归不存在闭式解, 我们通过梯度下降估计未知的权重 $\mathbf{w}$. 为此, 我们需要利用求导的**链式法则**计算损失函数的梯度.  损失函数关于坐标 $i$ 对应权重的偏导数为:
$$
\frac{\partial}{\partial w_i} \frac{1}{2} (y - h_{\mathbf{w}}(\mathbf{x}))^2 = -(y - h_{\mathbf{w}}(\mathbf{x})) h_{\mathbf{w}}(\mathbf{x}) (1 - h_{\mathbf{w}}(\mathbf{x})) x_i
$$
其中我们使用了逻辑函数 $g(z)=\frac{1}{1+e^{-z}}$ 的导数满足 $g'(z)=g(z)(1-g(z))$ 这一事实. 然后我们可以使用梯度下降来估计权重, 并按上文所述进行预测.

### 9.8 多类逻辑回归

在多类逻辑回归中, 我们希望将数据点分类到 $K$ 个不同的类别, 而不仅仅是两个. 因此, 我们希望构建一个模型, 对新数据点属于每个 $K$ 个可能类别的概率进行估计. 为此, 我们用 **softmax 函数**替代逻辑函数, 该函数将具有特征 $x$ 的新数据点属于标签 $i$ 的概率建模为:
$$
P(y=i|\mathbf{f}(\mathbf{x});\mathbf{w}) = \frac{e^{\mathbf{w}_i^T \mathbf{f}(\mathbf{x})}}{\sum_{k=1}^K e^{\mathbf{w}_k^T \mathbf{f}(\mathbf{x})}}
$$
注意这些概率估计之和为 1, 因此它们构成了一个有效的概率分布. 我们估计参数 $\mathbf{w}$ 以最大化我们观测到数据的似然. 假设我们已经观测到 $n$ 个带标签的数据点 $(x_i,y_i)$. 似然 (likelihood) 被定义为样本的联合概率分布, 记为 $\ell(\mathbf{w}_1, \ldots, \mathbf{w}_K)$, 其表示为:
$$
\ell(\mathbf{w}_1, \ldots, \mathbf{w}_K) = \prod_{i=1}^{n} P(y_i | \mathbf{f}(\mathbf{x}_i); \mathbf{w})
$$
为了计算使参数 $w_i$ 最大化似然的值, 我们对似然函数关于每个参数求梯度, 令其等于零, 并解出未知参数. 如果不存在闭式解, 我们将计算似然的梯度并使用梯度上升来获得最优值.

简化这些计算的一个常用技巧是先对似然函数取对数, 这会把乘积变成求和, 从而简化梯度计算. 我们可以这样做, 因为对数是严格递增函数, 这一变换不会影响函数的最大化点.

对于似然函数, 我们需要一种方法来表示 $y\in\{1,\ldots,K\}$ 时的概率 $P(y_i\mid f(x_i); w)$. 因此, 对于每个数据点 $i$, 我们定义 $K$ 个参数 $t_{i,k},\ k=1,\ldots,K$, 使得当 $y_i=k$ 时 $t_{i,k}=1$, 否则 $t_{i,k}=0$. 因此, 我们现在可以把似然表示为:
$$
\ell(\mathbf{w}_1, \ldots, \mathbf{w}_K) = \prod_{i=1}^{n} \prod_{k=1}^K \left( \frac{e^{\mathbf{w}_k^T \mathbf{f}(\mathbf{x}_i)}}{\sum_{\ell=1}^K e^{\mathbf{w}_\ell^T \mathbf{f}(\mathbf{x}_i)}} \right)^{t_{i,k}}
$$

同时我们也得到对数似然:
$$
\log \ell(\mathbf{w}_1, \ldots, \mathbf{w}_K) = \sum_{i=1}^{n} \sum_{k=1}^K t_{i,k} \log \left( \frac{e^{\mathbf{w}_k^T \mathbf{f}(\mathbf{x}_i)}}{\sum_{\ell=1}^K e^{\mathbf{w}_\ell^T \mathbf{f}(\mathbf{x}_i)}} \right)
$$

现在我们有了目标的表达式, 必须估计使表达式最大的那些 $\mathbf{w}_i$.

在多类逻辑回归的例子中, 相对于 $\mathbf{w}_j$ 的梯度为:
$$
\nabla_{\mathbf{w}_j} \log \ell(\mathbf{w}) = \sum_{i=1}^{n} \nabla_{\mathbf{w}_j} \sum_{k=1}^K t_{i,k} \log \left( \frac{e^{\mathbf{w}_k^T \mathbf{f}(\mathbf{x}_i)}}{\sum_{\ell=1}^K e^{\mathbf{w}_\ell^T \mathbf{f}(\mathbf{x}_i)}} \right) = \sum_{i=1}^{n} \left( t_{i,j} - \frac{e^{\mathbf{w}_j^T \mathbf{f}(\mathbf{x}_i)}}{\sum_{\ell=1}^K e^{\mathbf{w}_\ell^T \mathbf{f}(\mathbf{x}_i)}} \right) \mathbf{f}(\mathbf{x}_i)
$$
其中我们使用了 $\sum_k t_{i,k}=1$ 这一事实.

### 9.9 神经网络: 动机

在下文中我们将介绍神经网络的概念. 在这样做的过程中, 我们将使用一些为二元逻辑回归和多类逻辑回归所开发的建模技术.

#### 9.9.1 非线性分隔器

我们知道如何构建一个可以学习线性边界的二元分类模型. 这是一个强大的技术, 当潜在的最优决策边界本身是线性时, 它工作良好. 然而, 许多实际问题需要非线性的决策边界, 而我们的线性感知机模型不足以捕捉这种关系.

考虑以下一组数据
![图1: 非线性分隔器](../0_Attachment/Pasted%20image%2020250816151257.png)

我们希望分离这两种颜色, 显然在一维中无法做到这一点 (单维决策边界将是一个点, 把轴分成两个区域).

为了解决这个问题, 我们可以添加额外的 (可能是非线性的) 特征来构造决策边界. 考虑在数据中加入 $x^2$ 作为特征后的同一数据集:
![图2: 二维线性分隔器](../0_Attachment/Pasted%20image%2020250816151359.png)

有了这额外的信息, 我们现在能够在包含这些点的二维空间中构造一个线性分隔器. 在这个例子中, 我们通过把数据映射到一个更高维空间并手动为数据点添加有用的特征来解决问题. 然而, 在许多高维问题中, 例如图像分类, 手动选择有用的特征是繁琐的. 这需要领域特定的专业知识, 并且与跨任务泛化的目标相违背. 一个自然的愿望是**学习这些特征变换本身**, 使用一种能表示更广泛函数的非线性函数类.

#### 9.9.2 多层感知机

让我们来研究如何从原始感知机结构推导出更复杂的函数. 考虑以下设置, 二层感知机, 也就是以另一感知机的输出作为输入的感知机.

![图3: 二层感知机](../0_Attachment/Pasted%20image%2020250816151444.png)

事实上, 我们可以将其推广为 N 层感知机:
![图4: N 层感知机](../0_Attachment/Pasted%20image%2020250816151500.png)

通过这种附加的结构和权重, 我们可以表达更广泛的函数集.

通过增加模型的复杂度, 我们大大增加了其表达能力. 多层感知机为表示更广泛的函数提供了一种通用方式. 事实上, 多层感知机是一个**通用函数近似器** (Universal Function Approximators), 能够表示任意实函数, 这样我们只剩下选择一组最优权重以参数化网络的问题. 下面正式陈述此点:

**定理. (通用函数近似器)**

- 具有足够神经元的两层神经网络可以将任意连续函数近似到任意所需的精度.


#### 9.9.3 衡量准确性

二元感知机在做出 n 次预测后的准确性可以表示为:
$$
l^{acc}(\mathbf{w}) = \frac{1}{n}\sum_{i=1}^n(\textrm{sgn}(\mathbf{w} \cdot \mathbf{f}(\mathbf{x}_{i}))== y_{i})
$$

其中 $\mathbf{x}_{i}$ 是数据点 $i$, $\mathbf{w}$ 是我们的权重向量, $\mathbf{f}$ 是从原始数据点导出特征向量的函数, $y_i$ 是 $\mathbf{x}_{i}$ 的实际类别标签. 在此上下文中, $\operatorname{sgn}(x)$ 表示**指示函数**, 当 $x$ 为负时取值 $-1$, 当 $x$ 为正时取值 $1$. 我们的准确率函数等价于将正确预测的总数除以预测总数.

有时, 我们希望输出比二元标签更有表现力的结果. 此时, 输出每个 ($N$) 类的概率会很有用, 反映我们认为数据点属于每个类别的置信程度. 和多类逻辑回归一样, 我们为每个类别 ($j$) 存储一个权重向量, 并用 softmax 函数估计概率:
$$
\sigma (\mathbf{x}_{i})_j = \frac{e^{\mathbf{f}(\mathbf{x}_{i})^T \mathbf{w}_j}}{\sum_{\ell=1}^Ne^{\mathbf{f}(\mathbf{x}_{i})^T \mathbf{w}_\ell}} = P(y_{i} = j | \mathbf{f}(\mathbf{x}_{i});\mathbf{w}).
$$

给定由 $f$ 输出的向量, softmax 将其归一化以输出一个概率分布. 为了为我们的模型推导一般的损失函数, 我们可以使用这个概率分布来生成权重集合的似然表达式:
$$
\ell(\mathbf{w}) = \prod_{i=1}^nP(y_{i} | \mathbf{f}(\mathbf{x}_{i}); \mathbf{w}).
$$
该表达式表示某一组权重解释观测到的标签和数据点的似然. 我们寻求使该量最大化的权重集合. 这等价于寻找对数似然表达式的最大值:
$$
\log\ell(\mathbf{w}) = \log \prod_{i=1}^n P(y_{i} | x_{i}; \mathbf{w}) = \sum_{i=1}^n \log P(y_{i} | \mathbf{f}(\mathbf{x}_{i}); \mathbf{w}).
$$

#### 9.9.4 多层前馈神经网络

现在我们引入人工神经网络的想法. 像多层感知机一样, 我们选择在每个感知机节点之后应用一个非线性函数. 这些额外的非线性使整个网络成为非线性的并且更具表达性. 否则, 多层感知机将只是线性函数的复合, 因此仍然是线性的.

在多层感知机的情况下, 我们选择了阶跃函数:
$$
f(x) = \begin{cases} 1 & \text{if } x \ge 0 \\ -1 & \text{otherwise} \end{cases}
$$

图形上, 它看起来像这样:
![图5: 阶跃函数](../0_Attachment/Pasted%20image%2020250816152035.png)

这很难优化, 因为它不是连续的并且在所有点的导数为零. 一个更好的解决方案是选择一个连续函数, 例如 sigmoid 函数或整流线性单元 (ReLU).

- Sigmoid 函数: $\sigma(x) = \frac{1}{1 + e^{-x}}$
  ![图6: Sigmoid 函数](../0_Attachment/Pasted%20image%2020250816152154.png)
- ReLU: $f(x) = \begin{cases} 0 & \text{if } x < 0 \\ x & \text{if } x \ge 0 \end{cases}$
  ![图7: ReLU](../0_Attachment/Pasted%20image%2020250816152206.png)

在多层感知机中, 我们在每层的输出处应用其中一种非线性. 非线性的选择是一个设计决策, 通常需要通过实验来确定.

#### 9.9.5 损失函数与多变量优化

现在我们理解了前馈神经网络是如何构建的, 我们需要一种训练它的方法. 回到我们的对数似然函数, 我们可以推导出一个直观的算法来优化我们的权重.

为了最大化我们的对数似然函数, 我们对其求导以得到**梯度向量**:
$$
\nabla_w \ell(w) = \left[ \frac{\partial \ell(w)}{\partial w_1}, \ldots, \frac{\partial \ell(w)}{\partial w_n} \right].
$$

我们使用梯度上升法找到参数的最优值. 鉴于数据集通常很大, 批量梯度上升是神经网络优化中最流行的梯度上升变体.

#### 9.9.6 自动微分 *(PPT 补充)*

*[图解: 5分钟深度学习系列](https://www.bilibili.com/video/BV1oY411N7Xz)*

自动微分是一种通过程序自动计算函数导数的技术, 无需手动推导. 它仅需定义函数 $g(x,y,w)$ (其中 *w*是待求导的参数), 即可自动计算所有参数的导数 $\frac{\partial g}{\partial w}$.

原理:

1. **正向传播** (Forward Pass): 在计算函数值 $g(x,y,w)$ 的过程中, 缓存中间变量的值 (如每层的输出 $h(x)$).
2. **反向传播** (Backward Pass): 从输出端开始, 利用链式法则反向计算每个参数的导数, 复用正向传播缓存的中间结果, 避免重复计算.

优势:

- **高效性**: 计算成本与前向传播相当, 远低于手动数值微分 (如有限差分法).
- **准确性**: 避免数值微分的舍入误差, 结果精确.
- **通用性**: 适用于任意可微函数 (包括复杂的神经网络结构).

深度学习框架如 Theano, TensorFlow, PyTorch, Chainer 等均内置自动微分功能, 用户只需定义网络结构, 框架会自动处理梯度计.

#### 9.9.7 迁移学习 *(PPT 补充)*

**核心问题**: 如何高效地训练机器学习模型?

- **难点**: 数据标注 (Data Labeling) 非常耗时, 需要大量人力标注样本 (例如图像分类中手动标记每张图片的类别)
- **思路**: 能否复用已有的, 类似的模型, 避免从零开始训练?

**迁移学习** (Transfer Learning): 是指使用一个已有相似任务的预训练模型的权重作为起点, 在此基础上训练新模型的方法.

- **价值**: 避免重复造轮子, 利用已有模型的"知识" (如特征提取能力), 大幅减少训练时间和数据需求.
- **领域适应** (Domain Adaptation) 是迁移学习的一种简单形式, 指用一个现有的预训练模型, 在新的, 更小的数据集上进一步训练. 其依赖"源域" (Source Domain, 如通用数据) 和"目标域" (Target Domain, 如细分数据) 的关联性, 通过少量目标域数据实现适配.
  - 假设已有模型在"通用物体分类"上训练好, 现在需要让它识别"宠物细分品种", 但宠物数据很少, 此时用少量宠物数据微调模型即可.
- 常用技巧: **冻结特征检测部分**
  - 在训练新模型时, **固定 (冻结) 神经网络中负责"特征检测"的底层部分** (如CNN的卷积层), 仅训练"分类/判别"的上层部分 (如全连接层).
  - 原理: 底层网络 (如卷积层) 通常学习通用特征 (如边缘, 纹理), 对多种任务都有效；高层网络 (如分类层) 则依赖具体任务的细节, 需要针对性训练.

### 9.10 归纳学习与决策树 *(PPT 补充)*

#### 9.10.1 形式化学习: 归纳学习

**归纳学习** (Inductive Learning) 是一种**从示例中学习函数**的机器学习范式. 其核心要素包括:

- **目标函数 (True Function) **: 记为 $g$, 是现实中存在的"真实规律" (例如: 判断邮件是否为垃圾邮件的规则, 预测房价的数学关系).
- **训练示例**: 输入 $x$ 与目标函数输出 $g(x)$ 的成对数据 (例如: 一封具体的邮件 $x$ 和它是否是垃圾邮件 $g(x)$；一套房子的特征 $x$ 和它的实际售价 $g(x)$).

核心问题

- 给定一个**假设空间** (Hypothesis Space) $H$ (即模型可能的"候选函数集合", 例如所有可能的线性分类器, 决策树等), 以及一组训练示例 $\{x_i,g(x_i)\}$, 归纳学习的目标是从 $H$ 中找到一个假设 $h$ (即最终训练出的模型), 使得 $h$ 尽可能接近真实的目标函数 $g$ (即 $h≈g$).

归纳学习主要分为两类:

- **分类** (Classification): 输出是离散的类别标签 (例如: 垃圾邮件/正常邮件).
- **回归** (Regression): 输出是连续的实数 (例如: 房价, 温度预测).

回归任务 (函数逼近/曲线拟合): 归纳学习的典型场景

![图1](../0_Attachment/Pasted%20image%2020250817140916.png)

核心权衡: 一致性 vs 简单性

- **一致性** (Consistency): 模型对训练数据的拟合能力 (即是否能"记住"训练数据).
- **简单性** (Simplicity): 模型本身的复杂度 (如曲线的波动幅度, 参数数量) 
- **奥卡姆剃刀** (Ockham’s Razor): 科学哲学中的经典原则, 主张"如无必要, 勿增实体". 在机器学习中, 它被引申为"优先选择更简单的模型" — 因为复杂模型可能过度依赖训练数据的噪声, 泛化能力 (对新数据的预测能力) 更差.

一致性 vs 简单性

- 基本权衡: 偏差 vs 方差
  - **偏差**: 模型因"过于简单"而无法捕捉数据真实规律的误差 (例如: 用直线拟合非线性关系的数据, 会系统性偏离真实值).
  - **方差**: 模型因"过于复杂"而对训练数据波动敏感的误差 (例如: 用高次多项式拟合少量数据, 结果会因微小数据变化而剧烈波动).
- 通常算法默认偏向一致性. 因为直接最小化训练误差 (或寻找与训练数据一致的假设) 是直观且容易实现的策略, 但可能导致过拟合.
- 实现简单性
  - **缩小假设空间 (Hypothesis Space)**
    - **添加先验假设**: 例如朴素贝叶斯 (Naïve Bayes) 假设特征之间"条件独立", 大幅减少模型需要考虑的可能关系.
    - **特征选择**: 从原始特征中筛选最关键的少量特征 (例如: 预测房价时, 只保留面积, 房龄等核心特征, 忽略无关的"窗户颜色").
    - **结构限制**: 例如用决策列表 (Decision List) 代替决策树, 强制模型按固定顺序做判断, 避免树结构无限生长.
  - **正则化 (Regularization)**
    - **平滑处理**: 例如在概率模型中对稀有事件的计数添加"平滑项" (如拉普拉斯平滑), 避免模型因数据稀疏而过度拟合.
    - **泛化参数**: 例如决策树剪枝 (Pruning) 时设置"剪枝截止阈值", 提前停止树的生长；或支持向量机 (SVM) 中调整正则化参数 C, 控制模型对训练误差的容忍度.
  - 这些方法的共同特点是: 保持假设空间的"大容量" (仍能覆盖可能的真实规律), 但通过约束让模型更难"跑到"假设空间的边缘 (即避免过度复杂的模型)

#### 9.10.2 信息熵

信息用于回答问题, 消除不确定性.

信息的价值与不确定性成正比: 对答案的初始不确定性越高, 得到答案后获得的信息量越大.

- 例如: 如果问题是"明天会下雨吗? " (先验概率: 下雨的概率是1/2), 回答"下雨"或"不下雨"的信息量很大 (因为你原本不确定) ；但如果问题是"太阳会从东边升起吗? " (先验概率: 几乎100%), 回答"会"的信息量很小 (因为你早已确定).

信息的度量单位: 比特 (Bit) 

- 布尔问题: 若先验概率为 p=1/2 (如抛硬币正面朝上), 回答问题的信息量是 1 比特 (因为需要 1 位二进制数即可编码).
- 四路问题: 若每种可能的概率相等 (p=1/4), 回答问题的信息量是 2 比特 (因为需要2位二进制数编码4种情况, 如00, 01, 10, 11).
- 极端确定的四路问题: 若其中一种可能的概率为1 (如"今天是星期一"的概率是1, 其他三天概率为0), 回答问题的信息量是0比特 (因为结果已确定, 无需额外信息).
- 非均匀三路问题: 若三种可能的概率为 <1/2, 1/4, 1/4>, 回答问题的信息量需加权计算.

**熵** (Entropy): 若先验概率分布为 $⟨p_1, ..., p_n⟩$, 则熵定义为期望编码长度:
$$
H(⟨p_1,...,p_n⟩) = E_p \log_2{\frac{1}{p_i}} = \sum_{i=1}^{n} -p_i \log_2{p_i}
$$
- $p_i$ 是随机变量取第 $i$ 个值的概率.
- $E_p$ 表示对概率分布 $p$ 取期望 (加权平均).
- $\log_2{\frac{1}{p_i}}$ 是"单个事件的信息量":, 熵是所有可能事件的信息量的期望

#### 9.10.3 决策树 

决策树 (Decision Trees) 是函数的紧凑表示: 真值表, 条件概率表, 回归值

真正的函数: 若目标函数在假设空间 $H$ 中, 则称为可实现 (realizable)

决策树可以表示任意关于特征的函数 (比如在布尔属性上能够表示所有布尔函数), 然而我们希望树是紧凑的 (简洁的), 过复杂的树 (分支极多）会过度贴合训练数据中的噪声 (如随机误差),导致对新数据的预测效果差. 而且树越复杂, 分支条件越多, 人类越难理解模型的决策逻辑.

决策树 vs 感知器

- 感知器的核心是**线性加权求和**, 其对特征的影响是**全局且线性的**. 若想让感知器捕捉**特征间的交互作用** (例如"当顾客很多且等待时间超过60分钟时, 更可能愿意等待"), 必须**手动添加组合特征**. 否则, 感知器无法直接建模特征间的依赖关系.
- 决策树自动对特征做合取, 特征在树的不同分支中可以有不同的效应
- 朴素贝叶斯是相对证据加权 (Relative Evidence Weighting), 而决策树是复杂证据交互 (Complex Evidence Interaction)
  - 朴素贝叶斯假设特征之间条件独立, 因此它通过统计每个特征对类别的独立贡献来建模. 这种方式计算简单, 但无法捕捉特征间的交互.
  - 决策树允许特征间复杂交互 (不同分支下特征的作用不同), 因此能建模更真实的规律. 但如果交互过于复杂, 贪心算法可能无法找到最优树.

决策树的假设空间

- 有 $n$ 个布尔属性时, 有 $2^{2^n}$ 个不同的决策树. 例如 n = 6 时, 有约 1.8447e19 棵树
- **决策树桩** (Decision Stumps): 深度为1的决策树, 数量为 4n
  - 每个输入值 (0和1) 可独立映射到输出值 (0或1), 共有 $2^2=4$ 种可能的映射方式
- 更具表现力的假设空间
  - 优点: 能覆盖几乎所有可能的规律 (高表现力), 因此降低偏差 (Bias) — 模型不会因"太简单"而无法捕捉数据中的真实关系 (例如非线性、多特征交互的规律).
  - 缺点: 与训练数据一致的假设数量剧增, 导致模型容易过拟合 — 记住训练数据中的噪声 (如随机误差), 而非真实规律, 最终增加方差 (对新数据的预测效果变差).

决策树学习

- 目标: 在训练样本上找到一棵"较小"的树并尽可能与训练样本一致
- 基本思想: 递归地选择"最重要"的属性作为 (子) 树的根结点
- ![图2: 决策树学习伪代码](../0_Attachment/Pasted%20image%2020250817143208.png)

如何选择属性

- 理想情况: 属性能完美分割数据. "好的属性"应将当前数据集分割成**尽可能纯的子集**（即子集中所有样本的类别一致, 全正或全负)
- 现实情况：需**量化**分割质量. 对每个划分 (按某属性), 比较划分前后的**熵差**, 差值即为**信息增益** (information gain). 划分后可能有多个子集, 每个子集有自己的熵. 此时需计算**加权期望熵**（即按子集大小加权的平均熵）, 再用父节点的熵减去这个加权期望熵, 得到信息增益.

剪枝: 解决决策树过拟合

1. 构建完整决策树
2. 从底部 (叶子节点) 向上修剪: 若该节点的机会节点概率 $p_{CHANCE}$ 大于预设阈值 $\text{Max} P_{CHANCE}$, 则删除该节点及其子树
3. $\text{Max} P_{CHANCE}$ 是一个正则化参数, 用于控制剪枝的严格程度, 通常通过保留数据设置

控制过拟合的策略

- 限制假设空间: 通过人工规则缩小模型可能的复杂度范围 (即限制假设空间的大小), 降低模型"记住噪声"的能力, 例如限制决策树的最大深度.
- 正则化假设选择: 在模型学习过程中 (如决策树构建, 参数训练), 通过"惩罚复杂模型"或"优先选择简单模型"的规则，引导模型主动避免过拟合, 例如决策树剪枝.

### 9.11 大语言模型

#### 9.11.1 特征工程

**文本分词** (Text Tokenization): 将文本映射为离散的 **token** (类比编译器)

**词嵌入** (Word Embeddings)

- 输入: 一组 token
- 输出: 词向量表示（embedding）
- 再由模型预测下一个词 (如 "ball") 并 **反嵌入 (un-embed)**
- ![图1: 词嵌入](../0_Attachment/Pasted%20image%2020250818132751.png)
- 特性
  - 相似词聚类
  - 学习出的语义特征
  - 向量空间中呈现代数结构: ![图2](../0_Attachment/Pasted%20image%2020250818132807.png)

#### 9.11.2 深度神经网络

**自回归模型** (Autoregressive Models)

- 特点: 逐步预测输出
  - 每次预测一个 token
  - 将 **输入 + 生成的输出** 拼接, 再作为新输入
  - 持续迭代
  - ![图3: 自回归模型](../0_Attachment/Pasted%20image%2020250818132837.png)
- 形式化表示: $x_1, x_2, x_3, \dots, x_n$

**自注意力**机制 (Self-Attention)

- 传统模型: 对所有输入 token 给予同等权重
- 自注意力: 动态分配关注度, 更加关注相关的 token
- 实现步骤:
  - 每个输入计算 **query, key, value**
  - 计算相似度分数
  - 归一化 + softmax 得到注意力权重
  - 加权求和形成新的表示
  - ![图4](../0_Attachment/Pasted%20image%2020250818133124.png)
- 公式:
  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
  $$

**多头注意力** (Multi-Headed Attention)

- 并行计算多个注意力头, 捕捉不同的关系模式
- 单头 vs 多头对比:
  - 单头: 一个注意力分布
  - 多头: 多个注意力分布并拼接
  - ![图5](../0_Attachment/Pasted%20image%2020250818133234.png)

**Transformer 架构**

- 组成单元: ![图6](../0_Attachment/Pasted%20image%2020250818133254.png)
- 流程: ![图7](../0_Attachment/Pasted%20image%2020250818133313.png)

#### 9.11.3 监督学习与强化学习

**自监督学习** (Self-Supervised Learning)

- 关键假设: 如果神经网络能解决一些**预测任务**, 那么它学到的表示可以迁移到其他任务

**预训练与微调**

1. 预训练 (Pre-train): 使用大量数据在自监督任务上训练一个大模型
2. 微调 (Fine-tune): 在特定任务的小数据集上训练从而适配到下游任务

**指令微调** (Instruction Tuning)

- 普通语言模型: 仅预测下一个词，可能生成"人类可能会作出"的闲聊回答
- 指令微调: 在数据集上训练，使模型学会生成**有帮助**的回答
- 例子
  - 问题: What is population of Berkeley?
  - 原始LM输出: This question always fascinated me!
  - 微调后输出: It is 117,145 as of 2021 census.

**基于人类反馈的强化学习** (RLHF)

- 将文本生成建模为 **MDP**
  - 状态: 已生成的词序列
  - 动作: 下一个词 (动作空间可达10万)
  - 转移: 连接上一个状态与新词
  - 奖励: 人类反馈 (+1 对应正确回答, -1 对应错误/有害回答)
- 学习过程:
  - 用人类评分训练奖励模型
  - 使用 **策略优化** (Proximal Policy Optimization, PPO) 优化语言模型

#### 9.11.4 搜索

**策略搜索与策略梯度** (Policy Search & Policy Gradient)

- 策略性能
  $$
  J(\theta) = V^{\pi_\theta}(s_0)
  $$

- 策略梯度更新
  $$
  \theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k)
  $$

- 优势函数 (Advantage Function)
  $$
  A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
  $$

- 策略梯度定理
  $$
  \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\tau} A^\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right]
  $$

**集束搜索** (Beam Search)

- 在生成时同时保留 **k 条最优候选路径**
- 相比贪心搜索更稳定, 避免陷入局部最优
- 并行性能
- ![图8](../0_Attachment/Pasted%20image%2020250818133345.png)

## 10. 逻辑

### 10.1 知识型智能体

想象一个危险的世界, 到处都是熔岩, 唯一的避难所是远处的绿洲. 我们希望智能体能够安全地从当前位置导航到绿洲.

在强化学习中, 我们假设能够给出的唯一引导是奖励函数, 它会尽力把智能体推向正确方向, 像玩"冷暖"游戏一样. 随着智能体探索并收集更多关于世界的观测, 它会逐渐学会将某些动作与未来的正回报联系起来, 而将其他动作与不希望发生的、被灼伤的死亡联系起来. 通过这种方式, 智能体可能学会识别来自世界的某些线索并相应地采取行动. 例如, 如果它感到空气变热, 它应该掉头.

然而, 我们可以考虑一种替代策略. 相反, 让我们把一些关于世界的事实告诉智能体, 并允许它基于手头的信息进行推理以决定该做什么. 如果我们告诉智能体空气在熔岩坑周围会变得又热又朦胧, 或者在水体周围会变得清爽而干净, 那么它就可以基于对大气的读数合理地推断出景观的哪些区域是危险或安全的. 这种替代类型的智能体被称为**知识型智能体** (knowledge-based agent). 这样的智能体维护一个**知识库** (knowledge base), 知识库是编码我们告诉智能体的内容以及它所观察到内容的逻辑句子的集合. 智能体还能够执行**逻辑推理** (logical inference) 以得出新的结论.

### 10.2 逻辑的语言

就像任何其他语言一样, 逻辑句子用一种特殊的**语法**来书写. 每个逻辑句子都是关于一个可能为真也可能为假的世界的**命题**的编码. 例如, 句子"地板是熔岩"在我们智能体的世界中可能为真, 但在我们的世界中很可能为假. 我们可以通过用**逻辑连接词**把更简单的句子串起来来构造复杂句子, 创建诸如"从 Big C 可以看到整个校园, **并且**远足是从学习中获得的健康休息"之类的句子. 语言中有五个逻辑连接词: 

- $\neg$, **非**: $\neg P$ 当且仅当 $P$ 为假时为真. 原子句 $P$ 与 $\neg P$ 称为**文字** (literals).
- $\wedge$, **与**: $A \wedge B$ 当且仅当 $A$ 为真且 $B$ 为真时为真. 一个"与"句称为**合取** (conjunction), 它的组成命题称为**合取项** (conjuncts).
- $\vee$, **或**: $A \vee B$ 当且仅当 $A$ 为真或 $B$ 为真时为真. 一个"或"句称为**析取** (disjunction), 它的组成命题称为**析取项** (disjuncts).
- $\Rightarrow$, **蕴含**: $A \Rightarrow B$ 为真, 除非 $A$ 为真且 $B$ 为假.
- $\Leftrightarrow$, **双条件**: $A \Leftrightarrow B$ 当且仅当 $A$ 与 $B$ 要么同时为真要么同时为假时为真.
- ![图1: 真值表](../0_Attachment/Pasted%20image%2020250818140647.png)

### 10.3 命题逻辑

像其他语言一样, 逻辑有多个变体. 我们将介绍两种: 命题逻辑和一阶逻辑. **命题逻辑** (propositional logic) 由**命题符号** (proposition symbols) 组成的句子构成, 命题符号可能由逻辑连接词连接. 命题符号通常表示为单个大写字母. 每个命题符号代表关于世界的一个原子命题. **模型** (model) 是对所有命题符号赋真或假的一个赋值, 我们可以把它视为一个"可能的世界". 例如, 如果我们有命题 A = "今天下雨" 和 B = "我忘了带伞", 那么可能的模型 (或"世界") 是: 

1. {A=true, B=true} ("今天下雨并且我忘了带伞. ") 
2. {A=true, B=false} ("今天下雨并且我没有忘带伞. ") 
3. {A=false, B=true} ("今天没有下雨并且我忘了带伞. ") 
4. {A=false, B=false} ("今天没有下雨并且我没有忘带伞. ") 

一般来说, 对于 $N$ 个符号, 有 $2^N$ 个可能的模型. 我们说一个句子是**有效的** (valid) 如果它在所有这些模型中都为真 (例如句子 True), **可满足的** (satisfiable) 如果存在至少一个模型使其为真, 而**不可满足的** (unsatisfiable) 如果在任何模型中都不为真. 例如, 句子 $A \wedge B$ 是可满足的, 因为在模型 1 中它为真, 但不是有效的, 因为在模型 2、3、4 中为假. 另一方面, $\neg A \wedge A$ 是不可满足的, 因为无论 $A$ 取何值都无法使其为真.

下面列出一些有用的逻辑等价式, 可以用来将句子简化为更易于处理和推理的形式.

![图1: 逻辑等价式](../0_Attachment/Pasted%20image%2020250818140659.png)

一种在命题逻辑中特别有用的语法是**合取范式** (conjunctive normal form, CNF), 它是子句的合取, 每个子句都是文字的析取. 它的一般形式为 $(P_1 \vee \cdots \vee P_i) \wedge \cdots \wedge (P_j \vee \cdots \vee P_n)$, 即"与"的"或"的形式. 正如我们将看到的, 这种形式对某些分析很有用. 重要的是, 每个逻辑句子都有一个逻辑等价的合取范式. 这意味着我们可以将知识库中包含的所有信息 (知识库只是不同句子的合取) 表示为一个大型的 CNF 语句, 通过将这些 CNF 语句"与"在一起.

CNF 表示在命题逻辑中尤为重要. 下面我们将展示一个将句子转换为 CNF 的例子. 假设我们有句子 $A\Leftrightarrow (B\vee C)$, 我们希望将其转换为 CNF. 推导基于图中的规则.

1. **消去 $\Leftrightarrow$**: 表达式变为 $(A\Rightarrow (B\vee C))\wedge ((B\vee C)\Rightarrow A)$, 使用**双条件消去**.
2. **消去 $\Rightarrow$**: 表达式变为 $(\neg A\vee B\vee C)\wedge (\neg(B\vee C )\vee A)$, 使用**蕴含消去**.
3. 对于 CNF 表示, 非 ($\neg$) 必须仅出现在文字上. 使用**德摩根**规则我们得到 $(\neg A\vee B\vee C)\wedge ((\neg B\wedge \neg C )\vee A)$.
4. 作为最后一步, 我们应用**分配律**并得到 $(\neg A\vee B\vee C)\wedge (\neg B\vee A )\wedge (\neg C\vee A)$.

最终表达式是三个"或"子句的合取, 因此它是 CNF 形式.

### 10.4 命题逻辑推理

逻辑之所以有用且强大, 是因为它赋予了从已有知识中得出新结论的能力. 要定义推理问题, 我们首先需要定义一些术语.

如果在所有使句子 $A$ 为真的模型中, 句子 $B$ 也为真, 则我们称句子 A **蕴涵**句子 $B$, 并将这种关系表示为 $A \models B$. 注意, 如果 $A \models B$, 则 $A$ 的模型是 $B$ 的模型的一个子集, $(M(A)\subseteq M(B))$. 推理问题可以表述为判断 $KB \models q$ 是否成立, 其中 $KB$ 是我们的知识库 (由逻辑句子组成), $q$ 是某个查询. 例如, 如果 Elicia 发誓永远不再踏入 Crossroads, 我们可以推断当寻找晚餐同桌朋友时不会找到她.

我们借助两个有用的定理来证明蕴涵:

1. $A \models B$ 当且仅当 $A \Rightarrow B$ 是有效的.
   通过展示 $A \Rightarrow B$ 是有效的来证明蕴涵称为**直接证明**.
2. $A \models B$ 当且仅当 $A \land \lnot B$ 是不可满足的.
   通过展示 $A \land \lnot B$ 是不可满足的来证明蕴涵称为**反证法证明**.

#### 10.4.1 模型检验

检查是否 $KB \models q$ 的一个简单算法是枚举所有可能的模型, 并检查在所有使 $KB$ 为真的模型中 $q$ 是否也为真. 这种方法称为**模型检验**. 在符号数量可行的句子中, 枚举可以通过**画真值表**来完成.

对于命题逻辑系统, 如果有 $N$ 个符号, 则有 $2^N$ 个模型需要检查, 因此该算法的时间复杂度为 $O(2^N)$, 而在一阶逻辑中, 模型数量是无限的. 事实上, 命题蕴涵问题已知是 co-NP 完全的. 尽管最坏情况运行时间不可避免地是问题规模的指数函数, 但确实存在在实践中可以更快终止的算法. 我们将讨论两种命题逻辑的模型检验算法.

第一种由 Davis, Putnam, Logemann 和 Loveland 提出 (我们将其称为 **DPLL 算法**), 本质上是对可能模型进行深度优先回溯搜索, 并采用三种技巧以减少过多的回溯. 该算法旨在解决可满足性问题, 即给定一个句子, 找到对所有符号的一个使之成立的赋值. 如前所述, 蕴涵问题可以归约为可满足性问题 (证明 $A \land \lnot B$ 不可满足), 并且 DPLL 接受以 CNF 形式的输入. 可满足性可以被表述为一个约束满足问题 (CSP), 如下: 令变量 (节点) 为符号, 约束为 CNF 所施加的逻辑约束. 然后 DPLL 将继续为符号赋真值, 直到找到一个可满足的模型, 或某个符号在不违反逻辑约束的情况下无法赋值, 此时算法将回溯到最近的可行赋值. 然而, DPLL 在简单回溯搜索上做了三项改进:

1. **提前终止** (Early Termination) : 若任一符号为真, 则一个子句为真. 因此, 即使不是所有符号都已赋值, 也可能已知整个句子为真. 同样地, 如果任一子句为假, 则句子为假. 在所有变量都被赋值之前, 提前检查整个句子是否可判定为真或假, 可以防止不必要地在子树中徘徊.
2. **纯符号启发式** (Pure Symbol Heuristic) : 纯符号是指在整个句子中仅以正形式出现 (或仅以负形式出现) 的符号. 纯符号可以立即被赋真或赋假. 例如, 在句子 $(A \lor B) \land (\lnot B \lor C) \land (\lnot C \lor A)$ 中, 我们可以识别出 A 是唯一的纯符号, 并可以立即将 $A$ 赋为真, 从而将满足性问题简化为仅在 $(\lnot B \lor C)$ 上寻找满足赋值.
3. **单子句启发式** (Unit Clause Heuristic) : 单子句是仅含一个文字的子句, 或含一个文字和多个假值的析取. 在单子句中, 我们可以立即给该文字赋值, 因为只有一个有效赋值. 例如, 对于单子句 $(B \lor \text{false} \lor \cdots \lor \text{false})$, B 必须为真才能使该子句为真.

![图1: DPLL 算法](../0_Attachment/Pasted%20image%2020250818145823.png)

#### 10.4.2 DPLL: 例子

假设我们有下面的句子处于合取范式 (CNF):
$$
(\neg N \vee \neg S) \land (M \vee Q \vee N) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P \vee N) \land (\neg R \vee \neg L) \land (S)
$$
我们想用 DPLL 算法来确定它是否可满足. 假设我们使用固定的变量顺序 (按字母顺序) 和固定的取值顺序 (先真后假).  

在对 DPLL 函数的每次递归调用中, 我们跟踪三件事:

- **model** 是我们到目前为止已赋值符号及其值的列表.
- **symbols** 是仍需赋值的未赋值符号列表.
- **clauses** 是在此调用或将来 DPLL 调用中仍需考虑的子句 (析取) 列表.

我们从用一个空模型 (尚未分配任何符号), symbols 包含原始句子的所有符号, clauses 包含原始句子的所有子句来调用 DPLL.

我们的初始 DPLL 调用如下:

- **model**: {}
- **symbols**: $[L, M, N, P, Q, R, S]$
- **clauses**: $(\lnot N \lor \lnot S) \land (M \lor Q \lor N) \land (L \lor \lnot M) \land (L \lor \lnot Q) \land (\lnot L \lor \lnot P) \land (R \lor P \lor N) \land (\lnot R \lor \lnot L) \land (S)$

首先, 我们应用**提前终止**: 检查在当前模型给定下, 是否每个子句都为真, 或至少有一个子句为假. 由于模型尚未赋任何符号, 我们还不能确定哪些子句为真或为假.

接下来, 我们检查纯文字 (纯符号). 没有在整个句子中仅以非否定形式出现或仅以否定形式出现的符号, 因此没有可简化的纯文字. 例如, N 不是纯文字, 因为第一子句使用了否定的 $\lnot N$, 而第二子句使用了非否定的 $N$.

接下来, 我们检查单子句 (只含一个文字的子句). 有一个单子句 $S$. 要使整体句子为真, 我们知道 $S$ 必须为真 (没有其它方式满足该子句). 因此, 我们可以进行另一次 DPLL 调用, 将 $S$ 赋为真加入模型, 并从仍需赋值的符号列表中移除 $S$.

我们的第二次 DPLL 调用如下:

- **model**: $\{ {{\color{red} S}: T} \}$
- **symbols**: $[L, M, N, P, Q, R]$
- **clauses**: $(\neg N \vee \neg S) \land (M \vee Q \vee N) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P \vee N) \land (\neg R \vee \neg L) \land (S)$

首先, 我们通过将模型中的新赋值代入 ($S$ 为真, $\lnot S$ 为假) 来简化子句:
$$
(\neg N \vee {\color{red} F}) \land (M \vee Q \vee N) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P \vee N) \land (\neg R \vee \neg L) \land ({\color{red} T})
$$
简化后:
$$
(\neg N) \land (M \vee Q \vee N) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P \vee N) \land (\neg R \vee \neg L)
$$
使用我们新的简化子句, 可以检查提前终止. 我们仍然没有足够信息来得出所有子句为真或至少一个子句为假的结论.

接下来, 我们检查纯文字. 和之前一样, 没有在整个句子中仅以非否定形式或仅以否定形式出现的符号.

接下来, 我们检查单子句. 有一个单子句 $(\lnot N)$. 要使整体句子为真, $(\lnot N)$ 必须为真, 因此 $N$ 必须为假.

因此, 我们可以进行另一次 DPLL 调用, 将 $N$ 赋为假加入模型, 并从仍需赋值的符号列表中移除 $N$. 我们还可以使用此调用中计算得到的简化子句 (即在该调用中消去了 $S$ 的简化版本).

我们的第三次 DPLL 调用如下:

- **model**: $\{S:T, {\color{red} N:F}\}$
- **symbols**: $[L, M, P, Q, R]$
- **clauses**: $(\neg N) \land (M \vee Q \vee N) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P \vee N) \land (\neg R \vee \neg L)$

在此调用中, 我们首先通过将新的赋值代入 ($N$ 为假, $\lnot N$ 为真) 来简化子句
$$
({\color{red} T}) \land (M \vee Q \vee {\color{red} F}) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P \vee {\color{red} F}) \land (\neg R \vee \neg L)
$$

进一步简化为:
$$
(M \vee Q) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

在我们的新简化子句下, 检查提前终止, 然后检查纯文字. 和之前一样, 都未发现.

接下来, 我们检查单子句. 我们未发现只含一个符号的子句.

此时, 我们需要尝试给某个变量赋值. 按照我们的固定变量顺序, 我们将先给 $M$ 赋值, 并按照固定取值顺序先尝试将 $M$ 赋为真. 如果将 M 赋为真导致不可满足, 则我们需要回溯并尝试将 $M$ 赋为假. 如果将 $M$ 赋为假也导致不可满足, 那么我们就知道整个句子不可满足. 换言之, 我们现在将进行两个递归 DPLL 调用, 一个在分支上令 $M$ 为真, 另一个令 $M$ 为假, 并检查是否有任一分支产生可满足的赋值.

在 $M$ 为真的分支上的第一个 DPLL 调用中, 我们将 $M$ 真加入模型, 并使用上一次调用的简化子句:

- **model**: $\{S:T, N:F, {\color{red} M:T}\}$
- **symbols**: $[L, P, Q, R]$
- **clauses**: $(M \vee Q) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)$

首先, 我们通过将新赋值代入 ($M$ 为真) 来简化子句:
$$
({\color{red} T} \vee Q) \land (L \vee {\color{red} F}) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

简化为:
$$
(L) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

在我们的新简化子句下, 检查提前终止；和之前一样, 我们未能提前终止. 但是, 我们确实发现了一个纯文字 $\lnot Q$ (回想由于没有 $Q$ 的正例, 仅有 $\lnot Q$ 的出现, 这算作纯文字). 我们将 $Q$ 设为假以使 $\lnot Q$ 为真并继续.

在 $M$ 为真的分支上的第二次 DPLL 调用:

- **model**: $\{S:T, N:F, M:T, {\color{red} Q:F}\}$
- **symbols**: $[L, P, R]$
- **clauses**: $(L) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)$

我们据此简化子句:
$$
(L) \land (L \vee {\color{red} T}) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

进一步简化为:
$$
(L) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

检查提前终止与纯文字, 均未发现. 我们确实发现了单子句 $(L)$, 于是将其设为真.

在同一分支中下一次调用 ($M$ 为真) 我们现在有:

- **model**: $\{S:T, N:F, M:T, Q:F, {\color{red} L:T}\}$
- **symbols**: $[P, R]$
- **clauses**: $(L) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)$

简化子句得到:
$$
({\color{red} T}) \land ({\color{red} F} \vee \neg P) \land (R \vee P) \land (\neg R \vee {\color{red} F})
$$

即:
$$
(\neg P) \land (R \vee P) \land (\neg R)
$$

检查提前终止和纯文字, 仍未发现. 检查单子句时, 我们发现 $(\lnot P)$. 我们将该表达式设为真, 即将 $P$ 设为假, 进行下一次 DPLL 调用.

下一次调用如下:

- **model**: $\{S:T, N:F, M:T, Q:F, L:T, {\color{red} P:F}\}$
- **symbols**: $[R]$
- **clauses**: $(\neg P) \land (R \vee P) \land (\neg R)$

将 $P$ 设为假简化得: $({\color{red} T}) \land (R \vee {\color{red} F}) \land (\neg R)$, 即: $(R) \land (\neg R)$

我们检查提前终止. 注意该句子同时包含 $R$ 和 $\lnot R$, 两者无法同时满足. 此时我们可以判定该句子不可满足.

因为 $M$ 真分支以不可满足结束, 我们回溯至在赋 $M$ 真之前的点, 并进行一个令 $M$ 假的 DPLL 调用. $M$ 假分支上的第一次 DPLL 调用:

- **model**: $\{S:T, N:F, {\color{red} M:F}\}$
- **symbols**: $[L, P, Q, R]$
- **clauses**: $(M \vee Q) \land (L \vee \neg M) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)$

我们通过将新赋值代入 ($M$ 假) 来简化子句:
$$
({\color{red} F} \vee Q) \land (L \vee {\color{red} T}) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$
简化为:
$$
(Q) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

我们无法提前终止, 未发现纯文字. 我们发现一个单子句 $Q$, 于是对 DPLL 进行另一次调用, 将 $Q$ 设为真 (并从符号列表中移除).

在 M 假分支上的第二次 DPLL 调用:

- **model**: $\{S:T, N:F, M:F, {\color{red} Q:T}\}$
- **symbols**: $[L, P, R]$
- **clauses**: $(Q) \land (L \vee \neg Q) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)$

将 $Q$ 设为真代入子句:
$$
({\color{red} T}) \land (L \vee {\color{red} F}) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

简化为:
$$
(L) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)
$$

我们无法提前终止, 未发现纯文字. 我们发现单子句 $L$, 于是再对 DPLL 调用, 将 $L$ 设为真 (并从符号列表中移除).

在 $M$ 假分支上的第三次 DPLL 调用:

- **model**: $\{S:T, N:F, M:F, Q:T, {\color{red} L:T}\}$
- **symbols**: $[P, R]$
- **clauses**: $(L) \land (\neg L \vee \neg P) \land (R \vee P) \land (\neg R \vee \neg L)$

将 $L$ 设为真代入:
$$
({\color{red} T}) \land ({\color{red} F} \vee \neg P) \land (R \vee P) \land (\neg R \vee {\color{red} F})
$$

简化为:
$$
(\neg P) \land (R \vee P) \land (\neg R)
$$

我们无法提前终止, 未发现纯文字. 我们发现了两个单子句 $(\lnot P)$ 和 $(\lnot R)$. 依照我们的变量顺序, 我们先选择 $P$, 于是进行另一次 DPLL 调用, 把 P 设为假 (并从符号列表中移除).

在 $M$ 假分支上的第四次 DPLL 调用:

- **model**: $\{S:T, N:F, M:F, Q:T, L:T, {\color{red} P:F}\}$
- **symbols**: $[R]$
- **clauses**: $(\neg P) \land (R \vee P) \land (\neg R)$

将 $P$ 设为假代入: $({\color{red} T}) \land (R \vee {\color{red} F}) \land (\neg R)$, 即: $(R) \land (\neg R)$

我们检查提前终止. 注意该句子同时包含 $R$ 和 $\lnot R$, 两者无法同时满足. 此时我们可以判定该句子不可满足.

因为 $M$ 真赋值导致不可满足, 且 $M$ 假赋值也导致不可满足, 我们可以得出整个句子**不可满足**, 至此结束.

### 10.5 定理证明

另一种方法是对知识库 (KB) 应用推理规则来证明 KB |= q. 举例来说, 如果我们的知识库包含 A 和 $A \Rightarrow B$, 则我们可以推断 B (该规则称为 **Modus Ponens**). 前面提到的两种算法使用方法 ii.), 通过将 $A \land \lnot B$ 写成合取范式 (CNF) 并证明它是否可满足来进行推理.

我们也可以使用三条推理规则来证明蕴涵:

1. 如果我们的知识库包含 $A$ 和 $A \Rightarrow B$, 我们可以推断 $B$ (**Modus Ponens**).
2. 如果我们的知识库包含 $A \land B$, 我们可以推断 $A$. 我们也可以推断 $B$.  (**与消解**, And-Elimination).
3. 如果我们的知识库包含 $A$ 和 $B$, 我们可以推断 $A \land B$ (**合取**, Resolution).

最后一条规则构成了**分辨算法** (resolution algorithm) 的基础, 该算法迭代地将其应用于知识库和新推断出的句子, 直到要么推断出 q, 此时我们证明了 $KB \models q$, 要么没有更多可推断的句子, 此时 $KB \not\models q$.

然而, 在知识库只包含文字 (单独符号) 和蕴含式的特殊情况下: $(P_1 \land \cdots \land P_n \Rightarrow Q) \equiv (\lnot P_1 \lor \cdots \lor \lnot P_n \lor Q)$,
我们可以在与知识库大小成线性关系的时间内证明蕴涵. 一种算法, **前向链** (forward chaining), 迭代地检查每个蕴含语句, 其**前提** (左侧) 已知为真时, 将**结论** (右侧) 加入已知事实列表. 这个过程重复, 直到 q 被加入已知事实列表, 或没有更多能被推断.

![图1: 前向链算法](../0_Attachment/Pasted%20image%2020250818152150.png)

### 10.6 前向链

一种算法, 前向链, 迭代地检查每个蕴含语句, 在其前提 (左侧) 已知为真时, 将结论 (右侧) 添加到已知事实列表中.

#### 10.6.1 前向链: 示例

假设我们有如下知识库:

1. $A \rightarrow B$ 
2. $A \rightarrow C$ 
3. $B \land C \rightarrow D$ 
4. $D \land E \rightarrow Q$ 
5. $A \land D \rightarrow Q$ 
6. $A$

我们想用前向链来确定 $Q$ 是否为真或假.

为了初始化算法, 我们将初始化一个计数列表 $\textit{count}$. 列表中的第 $i$ 个数字告诉我们第 $i$ 条子句前提中有多少个符号. 例如, 第三条子句 $B \land C \rightarrow D$ 的前提中有两个符号 (B 和 C), 因此列表中第三个数应为 2. 注意第六条 A 的前提中有 0 个符号, 因为它等价于 $True \rightarrow A$.

然后, 我们将初始化 $\textit{inferred}$, 这是一个将每个符号映射到真/假的映射表. 它告诉我们哪些符号已被证明为真. 最初, 所有符号都为假, 因为我们还没有证明任何符号为真.

最后, 我们将初始化一个符号列表 $\textit{agenda}$, 它是一个可以被证明为真的符号列表, 但其影响尚未被传播. 例如, 如果 $D$ 在议程中, 这表示我们已准备好证明 $D$ 为真, 但仍需检查这将如何影响其他子句. 最初, $\textit{agenda}$ 只包含我们直接知道为真的符号, 在这里就是 $A$.  (换言之, $\textit{agenda}$从任何前提中有 0 个符号的子句开始. ) 

我们的起始状态如下:  

- **count**: $[1, 1, 2, 2, 2, 0] $
- **inferred**: $\{A:F, B:F, C:F, D:F, E:F, Q:F\}$
- **agenda**: $[A]$

在每次迭代中, 我们将从 $\textit{agenda}$ 中弹出一个元素. 这里, 只有一个元素可以弹出: $A$. 我们弹出的符号不是我们想要分析的符号 ($Q$), 所以算法尚未结束.

根据 $\textit{inferred}$ 表, $A$ 为假. 然而, 由于我们刚从 $\textit{agenda}$ 中弹出了 $A$, 我们可以将其设为真.

接下来, 我们需要传播 $A$ 为真的后果. 对于每个在前提中包含 $A$ 的子句, 我们将减少其对应的 $\textit{count}$, 以表明前提中需要检查的符号数减少了一个. 在此示例中, 第 1, 2 和 5 条子句在前提中包含 A, 所以我们将减少 $\textit{count}$ 的第 1, 2 和 5 个元素.

最后, 我们检查是否有任何子句已达到 $\textit{count}$ 为 0. 我们注意到第 1 和第 2 条子句达到 $\textit{count}$ 为 0. 这表示第 1 和第 2 条子句的所有前提都已满足, 因此第 1, 2 条的结论已准备好被推断. 例如, 在第 1 条子句中, 所有前提 (此处仅 A) 已被满足, 因此结论 B 已准备好被推断. 我们将第 1, 2 条的结论加入议程.

迭代 0 后, 我们的算法如下:

- count: $[{\color{red} 0}, {\color{red} 0}, 2, 2, {\color{red} 1}, 0]$
- inferred: $\{A:T, B:F, C:F, D:F, E:F, Q:F\}$
- agenda: $[{\color{red} B}, {\color{red} C}]$

在下一次迭代中, 我们将从议程中弹出一个元素. 这里我们选择弹出 $B$. 我们弹出的符号不是我们想要分析的符号 ($Q$), 所以算法尚未结束.

根据 $\textit{inferred}$ 表, B 为假. 然而, 由于我们刚弹出了 $B$, 我们可以将其设为真.

接下来, 我们需要传播 $B$ 为真的后果. 唯一在前提中包含 $B$ 的子句是第 3 条. 我们必须减少其对应的 $\textit{count}$.

最后, 我们检查是否有任何子句已达到 $\textit{count}$ 为 0. 没有子句新达到 $\textit{count}$ 为 0, 因此我们无法得出新的结论, 也不能向议程添加任何新符号.

迭代 1 后, 我们的算法如下:

- count: $[0, 0, {\color{red} 1}, 2, 1, 0]$
- inferred: $\{A:T, {\color{red} B:T}, C:F, D:F, E:F, Q:F\}$
- agenda: $[C]$

接下来, 我们将从议程中弹出 $C$ (这不是 $Q$, 所以算法还没结束). 我们可以将 $C$ 在 $\textit{inferred}$ 列表中设为真.

为了传播 $C$ 为真的后果, 我们减少第 3 条子句的 $\textit{count}$ (这是唯一一个在前提中包含 $C$ 的子句).

第 3 条子句新达到 $\textit{count}$ 为 0, 因此我们可以将其结论 $D$ 添加到议程.

迭代 2 后, 我们的算法如下:

- count: $[0, 0, {\color{red} 0}, 2, 1, 0]$
- inferred: $\{A:T, B:T, {\color{red} C:T}, D:F, E:F, Q:F\}$
- agenda: $[{\color{red} D}]$

接下来, 我们将从议程中弹出 $D$ (不是 $Q$, 算法未结束). 我们可以在 $\textit{inferred}$ 列表中将 $D$ 设为真.

为了传播 $D$ 为真的后果, 我们减少包含 $D$ 的子句的 $\textit{count}$ (第 4 和第 5 条).

第 5 条子句新达到 $\textit{count}$ 为 0, 因此我们将其结论 $Q$ 添加到议程.

迭代 3 后, 我们的算法如下:

- count: $[0, 0, 0, {\color{red} 1}, {\color{red} 0}, 0]$
- inferred: $\{A:T, B:T, C:T, {\color{red} D:T}, E:F, Q:F\}$
- agenda: $[{\color{red} Q}]$

接下来, 我们将从议程中弹出 $Q$. 这是我们想要评估的符号, 从议程中弹出它表明它已被证明为真. 我们得出 $Q$ 为真, 并结束算法.

### 10.7 一阶逻辑

逻辑的第二种变体, **一阶逻辑** (first-order logic, FOL), 比命题逻辑更具表达力, 并以对象作为其基本组成部分. 使用一阶逻辑我们可以描述对象之间的关系并对它们应用函数. 每个对象由一个常量符号表示, 每个关系由一个谓词符号表示, 每个函数由一个函数符号表示.

下表总结了一阶逻辑的语法.

![图1: 一阶逻辑语法](../0_Attachment/Pasted%20image%2020250818153617.png)

一阶逻辑中的**项** (term)是指代对象的逻辑表达式. 项的最简单形式是常量符号. 然而, 我们不希望为每一个可能的对象都定义不同的常量符号. 例如, 如果我们想指代 John 的左腿和 Richard 的左腿, 我们可以使用**函数符号**如 `Leftleg(John)` 和 `Leftleg(Richard)`. 函数符号只是命名对象的另一种方式, 并不是实际执行计算的函数.

一阶逻辑的**原子句** (atomic sentences) 是对对象之间关系的描述, 当该关系成立时原子句为真. 原子句的一个例子是 `Brother(John, Richard)`, 它由谓词符号后接括号内的一列项构成. 一阶逻辑的**复杂句** (complex sentences) 与命题逻辑类同, 由原子句通过逻辑联结词连接而成.

自然地, 我们希望有方法来描述整个对象集合. 为此我们使用**量词** (quantifiers). **全称量词** $\forall$ 的含义是"对所有", **存在量词** $\exists$ 的含义是"存在".

例如, 如果我们世界中的对象集合是所有辩论的集合, 句子 $\forall a, ~ TwoSides(a)$ 可以被翻译为"每场辩论都有两方". 如果我们世界中的对象集合是人, 句子 $\forall x, \exists y, SoulMate(x, y)$ 将意味着"对于所有人, 都存在某人是他们的灵魂伴侣. " **匿名变量** a, x, y 是对象的占位符, 可以**替换**为实际对象, 例如, 将 Laura 替换为 x 得到的句子将表示"存在某人是 Laura 的灵魂伴侣".

全称量词和存在量词分别是对所有对象做析取或合取的便捷方式. 因此它们也满足德摩根定律 (注意与合取和析取之间的类似关系):

![图2: 量词](../0_Attachment/Pasted%20image%2020250818153625.png)

最后, 我们使用等号符号来表示两个符号指代同一对象. 例如, 令人惊讶的句子 $\left(Wife(Einstein) = First Cousin(Einstein) \wedge Wife(Einstein) = Second Cousin(Einstein)\right)$ 是真实的!

与命题逻辑中模型是对所有命题符号的真/假赋值不同, 一阶逻辑中的模型是将所有常量符号映射到对象, 谓词符号映射到对象之间的关系, 函数符号映射到对象的函数的映射. 如果在某个模型下句子所描述的关系在映射下为真, 则该句子在该模型下为真. 尽管命题逻辑系统的模型数量总是有限的, 但如果对象数量不受限, 一阶逻辑系统可能有无限多个模型.

这两种逻辑变体允许我们以不同方式描述和思考世界. 使用命题逻辑, 我们将世界建模为一组真或假的符号. 在此假设下, 我们可以将可能世界表示为一个向量, 对每个符号用 1 或 0 表示其真假. 这种二元视角称为**分解表示** (factored representation). 使用一阶逻辑, 我们的世界由相互关联的对象组成. 这第二种面向对象的世界观称为**结构化表示** (structured representation), 在许多方面更具表达力, 并且更贴近我们自然语言描述世界的方式.

### 10.8 一阶逻辑推理

使用一阶逻辑我们以完全相同的方式来形式化推理. 我们希望判断是否 $KB \models q$, 即在所有使 $KB$ 为真的模型中 $q$ 是否也为真. 一种解决方法是**命题化** (propositionalization) 或将问题翻译为命题逻辑, 以便用我们已经介绍的技术来解决. 每个全称 (存在) 量词句子都可以被转换为一个合取 (析取), 对每个可能可以替换变量的对象都有一个子句. 然后我们可以使用一个 SAT 求解器, 如 DPLL 或 Walk-SAT, 去检查 $(KB \land \lnot q)$ 的 (不) 可满足性.

这种方法的一个问题是可能的替换有无限多种, 因为函数可以被无限次地应用到符号上. 例如, 我们可以无限嵌套函数 `Classmate(... Classmate(Classmate(Austen))...)`, 直到引用整个学校. 幸运的是, Jacques Herbrand (1930) 证明的一个定理告诉我们: 如果某个句子由知识库蕴涵, 则存在一个仅涉及命题化知识库的有限子集的证明. 因此, 我们可以尝试遍历有限子集, 具体地通过对嵌套函数应用进行迭代加深搜索, 即先搜索仅含常量符号的替换, 然后含 `Classmate(Austen)` 的替换, 然后含 `Classmate(Classmate(Austen))` 的替换, 依此类推.

另一种方法是直接对一阶逻辑进行推理, 也称为**提升推理** (lifted inference). 例如, 给定
$(\forall x ~ HasAbsolutePower(x) \wedge Person(x) \Rightarrow Corrupt(x)) \wedge Person(John) \wedge HasAbsolutePower(John)$ ("绝对的权力导致绝对腐败"). 我们可以通过将 x 替换为 John 推断出 $Corrupt(John)$. 该规则称为**广义假言三段论** (Generalized Modus Ponens). 一阶逻辑的前向链算法反复应用广义假言三段论和替换以推断 $q$ 或表明 $q$ 无法被推断.

### 10.9 逻辑智能体

现在我们理解了如何形式化我们的知识以及如何用它进行推理, 我们来讨论如何将演绎的能力纳入智能体. 智能体应具备的一项显而易见的能力是基于观察历史和关于世界的已知信息来确定自己所处的状态 (**状态估计**). 例如, 如果我们告诉智能体熔岩池附近空气会开始闪烁, 而它观测到前方的空气正在闪烁, 它就可以推断出危险就在附近.

为了将过去的观测纳入当前状态估计, 智能体需要对时间和状态之间的转移有一个概念. 我们称随时间变化的状态属性为 **fluent**, 并用带时间索引的 fluent 来书写, 例如 $Hot_t$ = 在时间 $t$ 时空气是热的. 如果某件事在该时间点导致空气变热, 或者空气在先前时间点已是热的并且没有发生改变它的动作, 那么在该时间点空气应当是热的. 为表示这一事实我们可以使用下面的**继承状态公理** (successor-state axiom) 的一般形式:
$$
F^{t+1} \Leftrightarrow ActionCausesF^t \vee (F^t \wedge \neg ActionCausesNotF^t)
$$

在我们的世界中, 转移可以表述为
$$
Hot^{t+1} \Leftrightarrow StepCloseToLava^t \vee (Hot^t \wedge \neg StepAwayFromLava^t)
$$

写出世界的规则后, 我们现在实际上可以通过检查某个逻辑公式的可满足性来做规划！为此, 我们构造一个包含初始状态, 转移 (继承状态公理) 和目标的句子 (例如 $InOasis_T \land Alive_T$ 编码了在时间 $T$ 达到绿洲并存活的目标). 如果世界的规则被正确地形式化, 那么找到对所有变量的一个满足赋值将允许我们提取出一系列动作, 使智能体达到目标.

### 10.10 小结

我们介绍了逻辑的概念, 知识型智能体可以利用逻辑来推理关于世界并做出决策. 我们介绍了逻辑的语言, 其语法以及标准的逻辑等价变换. 命题逻辑是基于命题符号和逻辑联结词的简单语言. 一阶逻辑是一种比命题逻辑更强大的表示语言. 一阶逻辑的语法建立在命题逻辑之上, 使用项来表示对象, 并使用全称与存在量词来做断言.

我们进一步描述了用于检查命题逻辑可满足性 (SAT 问题) 的 DPLL 算法. 它是对可能模型的深度优先枚举, 使用提前终止, 纯符号启发与单子句启发来改进性能. 当我们的知识库仅由文字和蕴含式构成时, 前向链算法可用于推理.

一阶逻辑的推理可以直接使用诸如广义假言三段论的规则来完成, 或通过命题化将问题转化为命题逻辑并使用 SAT 求解器来得出结论.
